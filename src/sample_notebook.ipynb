{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Degradation Detection Demo\n",
    "\n",
    "This notebook demonstrates how to use Pynomaly's performance degradation detection capabilities to monitor model performance over time and trigger automated retraining when performance drops below acceptable thresholds.\n",
    "\n",
    "## Features Demonstrated\n",
    "\n",
    "- Real-time performance monitoring\n",
    "- Performance baseline management\n",
    "- Degradation detection and alerting\n",
    "- Automated retraining integration\n",
    "- Performance trend analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Pynomaly imports\n",
    "from pynomaly.application.services.performance_monitoring_service import PerformanceMonitoringService\n",
    "from pynomaly.infrastructure.monitoring.performance_monitor import PerformanceMonitor\n",
    "from pynomaly.domain.entities import Dataset\n",
    "from pynomaly.infrastructure.adapters.sklearn_adapter import SklearnAdapter\n",
    "from pynomaly.domain.value_objects import ContaminationRate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Performance Monitoring\n",
    "\n",
    "Set up the performance monitoring system with configurable thresholds and monitoring intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure performance monitoring\n",
    "performance_monitor = PerformanceMonitor(\n",
    "    max_history=1000,\n",
    "    alert_thresholds={\n",
    "        \"execution_time\": 5.0,      # seconds\n",
    "        \"memory_usage\": 500.0,      # MB\n",
    "        \"cpu_usage\": 70.0,          # percentage\n",
    "        \"samples_per_second\": 50.0  # minimum throughput\n",
    "    },\n",
    "    monitoring_interval=1.0\n",
    ")\n",
    "\n",
    "# Initialize monitoring service\n",
    "monitoring_service = PerformanceMonitoringService(\n",
    "    performance_monitor=performance_monitor,\n",
    "    auto_start_monitoring=True\n",
    ")\n",
    "\n",
    "print(\"Performance monitoring system initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample Data and Detector\n",
    "\n",
    "Generate synthetic data and create an anomaly detector for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with anomalies\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_sample_data(n_samples=1000, n_features=5, contamination=0.1):\n",
    "    # Generate normal data\n",
    "    normal_data = np.random.normal(0, 1, (int(n_samples * (1 - contamination)), n_features))\n",
    "    \n",
    "    # Generate anomalies\n",
    "    anomalies = np.random.uniform(-4, 4, (int(n_samples * contamination), n_features))\n",
    "    \n",
    "    # Combine data\n",
    "    data = np.vstack([normal_data, anomalies])\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.random.permutation(len(data))\n",
    "    data = data[indices]\n",
    "    \n",
    "    return pd.DataFrame(data, columns=[f'feature_{i}' for i in range(n_features)])\n",
    "\n",
    "# Create sample dataset\n",
    "sample_data = generate_sample_data()\n",
    "dataset = Dataset(name=\"Sample Production Data\", data=sample_data)\n",
    "\n",
    "# Create detector\n",
    "detector = SklearnAdapter(\n",
    "    algorithm_name=\"IsolationForest\",\n",
    "    name=\"Production Detector\",\n",
    "    contamination_rate=ContaminationRate(0.1),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Created dataset with {len(sample_data)} samples\")\n",
    "print(f\"Dataset shape: {sample_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monitor Detection Operations\n",
    "\n",
    "Execute detection operations while monitoring performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection function\n",
    "def run_detection(detector, dataset):\n",
    "    \"\"\"Run anomaly detection\"\"\"\n",
    "    detector.fit(dataset)\n",
    "    result = detector.detect(dataset)\n",
    "    return result\n",
    "\n",
    "# Monitor detection operation\n",
    "print(\"Running monitored detection operation...\")\n",
    "result, metrics = monitoring_service.monitor_detection_operation(\n",
    "    detector=detector,\n",
    "    dataset=dataset,\n",
    "    operation_func=run_detection\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(f\"\\nDetection Results:\")\n",
    "print(f\"  Anomalies detected: {len(result.anomalies)}\")\n",
    "print(f\"  Execution time: {metrics.execution_time:.3f}s\")\n",
    "print(f\"  Memory usage: {metrics.memory_usage:.1f}MB\")\n",
    "print(f\"  CPU usage: {metrics.cpu_usage:.1f}%\")\n",
    "print(f\"  Throughput: {metrics.samples_per_second:.1f} samples/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Set Performance Baselines\n",
    "\n",
    "Establish baseline performance metrics for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set baseline performance expectations\n",
    "monitoring_service.set_performance_baseline(\n",
    "    operation_name=\"detection_IsolationForest\",\n",
    "    baseline_metrics={\n",
    "        \"execution_time\": 1.0,    # seconds\n",
    "        \"memory_usage\": 50.0,     # MB\n",
    "        \"cpu_usage\": 30.0         # percentage\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Performance baselines established successfully!\")\n",
    "\n",
    "# Display baselines\n",
    "dashboard_data = monitoring_service.get_monitoring_dashboard_data()\n",
    "print(\"\\nCurrent Performance Baselines:\")\n",
    "for operation, metrics in dashboard_data['performance_baselines'].items():\n",
    "    print(f\"  {operation}:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"    {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simulate Performance Degradation\n",
    "\n",
    "Simulate a scenario where model performance degrades over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate performance degradation by running multiple operations\n",
    "print(\"Simulating performance degradation over time...\")\n",
    "\n",
    "performance_history = []\n",
    "\n",
    "# Run multiple detection operations with gradually degrading performance\n",
    "for i in range(5):\n",
    "    # Simulate degradation by increasing data size\n",
    "    degraded_data = generate_sample_data(n_samples=1000 + i * 500, n_features=5)\n",
    "    degraded_dataset = Dataset(name=f\"Degraded Data {i+1}\", data=degraded_data)\n",
    "    \n",
    "    # Monitor operation\n",
    "    result, metrics = monitoring_service.monitor_detection_operation(\n",
    "        detector=detector,\n",
    "        dataset=degraded_dataset,\n",
    "        operation_func=run_detection\n",
    "    )\n",
    "    \n",
    "    performance_history.append({\n",
    "        'iteration': i + 1,\n",
    "        'data_size': len(degraded_data),\n",
    "        'execution_time': metrics.execution_time,\n",
    "        'memory_usage': metrics.memory_usage,\n",
    "        'throughput': metrics.samples_per_second\n",
    "    })\n",
    "    \n",
    "    print(f\"  Iteration {i+1}: {metrics.execution_time:.3f}s, {metrics.memory_usage:.1f}MB\")\n",
    "    \n",
    "    # Small delay to simulate real-world timing\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(\"\\nPerformance degradation simulation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Check for Performance Regression\n",
    "\n",
    "Analyze the recent performance data to detect regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for performance regression\n",
    "regression_result = monitoring_service.check_performance_regression(\n",
    "    operation_name=\"detection_IsolationForest\",\n",
    "    recent_window=timedelta(minutes=5)\n",
    ")\n",
    "\n",
    "print(\"Performance Regression Analysis:\")\n",
    "print(f\"  Operations analyzed: {regression_result.get('recent_operations', 0)}\")\n",
    "print(f\"  Regressions detected: {regression_result.get('regressions_detected', 0)}\")\n",
    "\n",
    "if regression_result.get('regressions_detected', 0) > 0:\n",
    "    print(\"\\nâš ï¸  Performance regressions detected!\")\n",
    "    for metric, regression in regression_result['regressions'].items():\n",
    "        print(f\"    {metric}:\")\n",
    "        print(f\"      Baseline: {regression['baseline']:.3f}\")\n",
    "        print(f\"      Current: {regression['current']:.3f}\")\n",
    "        print(f\"      Degradation: {regression['degradation_percent']:.1f}%\")\n",
    "else:\n",
    "    print(\"\\nâœ… No significant performance regressions detected.\")\n",
    "\n",
    "# Display current vs baseline performance\n",
    "print(\"\\nCurrent vs Baseline Performance:\")\n",
    "if 'current_performance' in regression_result:\n",
    "    current = regression_result['current_performance']\n",
    "    baseline = regression_result['baseline_performance']\n",
    "    \n",
    "    for metric in current.keys():\n",
    "        if metric in baseline:\n",
    "            change = ((current[metric] - baseline[metric]) / baseline[metric]) * 100\n",
    "            print(f\"  {metric}: {current[metric]:.3f} vs {baseline[metric]:.3f} ({change:+.1f}%)\")\n",
    "        else:\n",
    "            print(f\"  {metric}: {current[metric]:.3f} (no baseline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Performance Trends\n",
    "\n",
    "Create visualizations to show performance trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance trend visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Performance Degradation Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "df = pd.DataFrame(performance_history)\n",
    "\n",
    "# Plot execution time\n",
    "axes[0, 0].plot(df['iteration'], df['execution_time'], 'b-o', linewidth=2)\n",
    "axes[0, 0].set_title('Execution Time Over Time')\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Execution Time (seconds)')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot memory usage\n",
    "axes[0, 1].plot(df['iteration'], df['memory_usage'], 'r-o', linewidth=2)\n",
    "axes[0, 1].set_title('Memory Usage Over Time')\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Memory Usage (MB)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot throughput\n",
    "axes[1, 0].plot(df['iteration'], df['throughput'], 'g-o', linewidth=2)\n",
    "axes[1, 0].set_title('Throughput Over Time')\n",
    "axes[1, 0].set_xlabel('Iteration')\n",
    "axes[1, 0].set_ylabel('Samples/Second')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot data size\n",
    "axes[1, 1].plot(df['iteration'], df['data_size'], 'purple', marker='o', linewidth=2)\n",
    "axes[1, 1].set_title('Data Size Over Time')\n",
    "axes[1, 1].set_xlabel('Iteration')\n",
    "axes[1, 1].set_ylabel('Number of Samples')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nPerformance Summary:\")\n",
    "print(f\"  Execution time increased by {((df['execution_time'].iloc[-1] - df['execution_time'].iloc[0]) / df['execution_time'].iloc[0] * 100):.1f}%\")\n",
    "print(f\"  Memory usage increased by {((df['memory_usage'].iloc[-1] - df['memory_usage'].iloc[0]) / df['memory_usage'].iloc[0] * 100):.1f}%\")\n",
    "print(f\"  Throughput decreased by {((df['throughput'].iloc[0] - df['throughput'].iloc[-1]) / df['throughput'].iloc[0] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Set Up Automated Alert Handling\n",
    "\n",
    "Configure automated responses to performance alerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define alert handler\n",
    "def handle_performance_alert(alert):\n",
    "    \"\"\"Handle performance alerts with automated responses\"\"\"\n",
    "    print(f\"\\nðŸš¨ Performance Alert: {alert.severity.upper()}\")\n",
    "    print(f\"   Metric: {alert.metric_name}\")\n",
    "    print(f\"   Current Value: {alert.current_value:.3f}\")\n",
    "    print(f\"   Threshold: {alert.threshold_value:.3f}\")\n",
    "    print(f\"   Operation: {alert.operation_name}\")\n",
    "    print(f\"   Timestamp: {alert.timestamp}\")\n",
    "    \n",
    "    # Automated response based on severity\n",
    "    if alert.severity == \"critical\":\n",
    "        print(\"   ðŸ”¥ CRITICAL: Triggering emergency response...\")\n",
    "        # In a real scenario, this would trigger immediate retraining\n",
    "        # trigger_emergency_retraining(alert.operation_name)\n",
    "    elif alert.severity == \"high\":\n",
    "        print(\"   âš ï¸  HIGH: Scheduling retraining...\")\n",
    "        # schedule_retraining(alert.operation_name)\n",
    "    elif alert.severity == \"medium\":\n",
    "        print(\"   âš¡ MEDIUM: Monitoring closely...\")\n",
    "        # increase_monitoring_frequency(alert.operation_name)\n",
    "    \n",
    "    # Log the alert\n",
    "    print(f\"   ðŸ“ Alert logged for further analysis\")\n",
    "\n",
    "# Add alert handler to monitoring service\n",
    "monitoring_service.add_alert_handler(handle_performance_alert)\n",
    "\n",
    "print(\"Performance alert handling configured successfully!\")\n",
    "\n",
    "# Display current system status\n",
    "dashboard_data = monitoring_service.get_monitoring_dashboard_data()\n",
    "print(f\"\\nCurrent System Status:\")\n",
    "print(f\"  Monitoring enabled: {dashboard_data['system_status']['monitoring_enabled']}\")\n",
    "print(f\"  Total operations monitored: {dashboard_data['system_status']['total_operations_monitored']}\")\n",
    "print(f\"  Active alerts: {dashboard_data['system_status']['alert_count']}\")\n",
    "print(f\"  Failed operations: {dashboard_data['system_status']['failed_operations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Trends Analysis\n",
    "\n",
    "Analyze performance trends over different time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get performance trends\n",
    "trends = monitoring_service.get_performance_trends(\n",
    "    operation_name=\"detection_IsolationForest\",\n",
    "    time_window=timedelta(minutes=10),\n",
    "    bucket_size=timedelta(minutes=1)\n",
    ")\n",
    "\n",
    "print(\"Performance Trends Analysis:\")\n",
    "print(f\"  Analysis period: {trends.get('time_window_hours', 0):.1f} hours\")\n",
    "print(f\"  Total operations analyzed: {trends.get('total_operations', 0)}\")\n",
    "print(f\"  Time buckets: {len(trends.get('time_buckets', []))}\")\n",
    "\n",
    "if 'trends' in trends:\n",
    "    print(\"\\nTrend Analysis:\")\n",
    "    for metric, trend in trends['trends'].items():\n",
    "        trend_icon = \"ðŸ“ˆ\" if trend == \"increasing\" else \"ðŸ“‰\"\n",
    "        print(f\"  {trend_icon} {metric}: {trend}\")\n",
    "\n",
    "# Algorithm performance comparison\n",
    "print(\"\\nAlgorithm Performance Comparison:\")\n",
    "comparison = monitoring_service.get_algorithm_performance_comparison(\n",
    "    time_window=timedelta(minutes=10),\n",
    "    min_operations=1\n",
    ")\n",
    "\n",
    "if comparison.get('algorithms'):\n",
    "    for algorithm, stats in comparison['algorithms'].items():\n",
    "        print(f\"\\n  {algorithm}:\")\n",
    "        print(f\"    Operations: {stats['operation_count']}\")\n",
    "        print(f\"    Avg execution time: {stats['avg_execution_time']:.3f}s\")\n",
    "        print(f\"    Avg memory usage: {stats['avg_memory_usage']:.1f}MB\")\n",
    "        print(f\"    Avg throughput: {stats['avg_throughput']:.1f} samples/sec\")\n",
    "        print(f\"    Reliability score: {stats['reliability_score']:.3f}\")\n",
    "\n",
    "    # Display rankings\n",
    "    if 'rankings' in comparison:\n",
    "        print(\"\\nAlgorithm Rankings:\")\n",
    "        for criterion, ranking in comparison['rankings'].items():\n",
    "            print(f\"  {criterion}: {' > '.join(ranking)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Integration with Automated Retraining\n",
    "\n",
    "Demonstrate how performance monitoring integrates with automated retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated retraining integration\n",
    "def simulate_retraining_decision(alert):\n",
    "    \"\"\"Simulate automated retraining decision based on performance alert\"\"\"\n",
    "    if alert.metric_name in ['execution_time', 'memory_usage'] and alert.severity in ['high', 'critical']:\n",
    "        print(f\"\\nðŸ”„ Retraining Decision for {alert.operation_name}:\")\n",
    "        \n",
    "        # Simulate retraining decision logic\n",
    "        degradation_percent = ((alert.current_value - alert.threshold_value) / alert.threshold_value) * 100\n",
    "        \n",
    "        if degradation_percent > 50:\n",
    "            decision = \"IMMEDIATE_RETRAINING\"\n",
    "            confidence = 0.95\n",
    "        elif degradation_percent > 25:\n",
    "            decision = \"SCHEDULED_RETRAINING\"\n",
    "            confidence = 0.8\n",
    "        else:\n",
    "            decision = \"MONITOR_CLOSELY\"\n",
    "            confidence = 0.6\n",
    "        \n",
    "        print(f\"   Decision: {decision}\")\n",
    "        print(f\"   Confidence: {confidence:.2f}\")\n",
    "        print(f\"   Degradation: {degradation_percent:.1f}%\")\n",
    "        \n",
    "        if decision == \"IMMEDIATE_RETRAINING\":\n",
    "            print(\"   ðŸš€ Triggering immediate retraining...\")\n",
    "            # In a real scenario:\n",
    "            # auto_retraining_service.trigger_immediate_retraining(model_id)\n",
    "        elif decision == \"SCHEDULED_RETRAINING\":\n",
    "            print(\"   ðŸ“… Scheduling retraining for next maintenance window...\")\n",
    "            # auto_retraining_service.schedule_retraining(model_id, schedule_time)\n",
    "        else:\n",
    "            print(\"   ðŸ‘€ Increasing monitoring frequency...\")\n",
    "            # monitoring_service.increase_monitoring_frequency(operation_name)\n",
    "\n",
    "# Add retraining decision handler\n",
    "monitoring_service.add_alert_handler(simulate_retraining_decision)\n",
    "\n",
    "print(\"Automated retraining integration configured successfully!\")\n",
    "\n",
    "# Test the system with a large dataset to trigger alerts\n",
    "print(\"\\nTesting alert system with large dataset...\")\n",
    "large_data = generate_sample_data(n_samples=5000, n_features=10)\n",
    "large_dataset = Dataset(name=\"Large Test Data\", data=large_data)\n",
    "\n",
    "# This should trigger alerts due to increased execution time and memory usage\n",
    "result, metrics = monitoring_service.monitor_detection_operation(\n",
    "    detector=detector,\n",
    "    dataset=large_dataset,\n",
    "    operation_func=run_detection\n",
    ")\n",
    "\n",
    "print(f\"\\nLarge dataset test completed:\")\n",
    "print(f\"  Execution time: {metrics.execution_time:.3f}s\")\n",
    "print(f\"  Memory usage: {metrics.memory_usage:.1f}MB\")\n",
    "print(f\"  Throughput: {metrics.samples_per_second:.1f} samples/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export and Reporting\n",
    "\n",
    "Export performance metrics and generate reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export performance metrics\n",
    "print(\"Exporting performance metrics...\")\n",
    "\n",
    "# Export to JSON\n",
    "json_export = monitoring_service.monitor.export_metrics(\n",
    "    format_type=\"json\",\n",
    "    time_window=timedelta(minutes=10)\n",
    ")\n",
    "\n",
    "print(f\"JSON export contains {json_export['total_metrics']} metrics\")\n",
    "\n",
    "# Export to CSV\n",
    "csv_export = monitoring_service.monitor.export_metrics(\n",
    "    format_type=\"csv\",\n",
    "    time_window=timedelta(minutes=10)\n",
    ")\n",
    "\n",
    "print(f\"CSV export generated ({len(csv_export.split(chr(10)))} lines)\")\n",
    "\n",
    "# Display sample of exported data\n",
    "print(\"\\nSample exported data (first 3 metrics):\")\n",
    "for i, metric in enumerate(json_export['metrics'][:3]):\n",
    "    print(f\"\\n  Metric {i+1}:\")\n",
    "    print(f\"    Operation: {metric['operation_name']}\")\n",
    "    print(f\"    Algorithm: {metric['algorithm_name']}\")\n",
    "    print(f\"    Execution time: {metric['execution_time']:.3f}s\")\n",
    "    print(f\"    Memory usage: {metric['memory_usage']:.1f}MB\")\n",
    "    print(f\"    Timestamp: {metric['timestamp']}\")\n",
    "\n",
    "# Generate final dashboard summary\n",
    "dashboard_data = monitoring_service.get_monitoring_dashboard_data()\n",
    "print(f\"\\nFinal Dashboard Summary:\")\n",
    "print(f\"  Total operations monitored: {dashboard_data['system_status']['total_operations_monitored']}\")\n",
    "print(f\"  Active alerts: {len(dashboard_data['active_alerts'])}\")\n",
    "print(f\"  Failed operations: {dashboard_data['system_status']['failed_operations']}\")\n",
    "print(f\"  Monitoring enabled: {dashboard_data['system_status']['monitoring_enabled']}\")\n",
    "\n",
    "if dashboard_data['active_alerts']:\n",
    "    print(\"\\nActive Alerts:\")\n",
    "    for alert in dashboard_data['active_alerts']:\n",
    "        print(f\"  - {alert['severity'].upper()}: {alert['message']}\")\n",
    "\n",
    "# Stop monitoring\n",
    "monitoring_service.stop_monitoring()\n",
    "print(\"\\nâœ… Performance monitoring demonstration completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the comprehensive performance degradation detection capabilities of Pynomaly:\n",
    "\n",
    "1. **Real-time Monitoring**: Continuous tracking of execution time, memory usage, and throughput\n",
    "2. **Baseline Management**: Setting and maintaining performance baselines for comparison\n",
    "3. **Degradation Detection**: Automatic detection of performance regression with configurable thresholds\n",
    "4. **Intelligent Alerting**: Multi-level alerting system with automated response capabilities\n",
    "5. **Trend Analysis**: Historical performance analysis and trend visualization\n",
    "6. **Automated Retraining**: Integration with automated retraining systems for proactive model maintenance\n",
    "7. **Export and Reporting**: Comprehensive data export capabilities for further analysis\n",
    "\n",
    "### Key Benefits\n",
    "\n",
    "- **Proactive Monitoring**: Detect performance issues before they impact production\n",
    "- **Automated Response**: Reduce manual intervention with intelligent alerting and retraining\n",
    "- **Comprehensive Metrics**: Track all aspects of model performance in one system\n",
    "- **Scalable Architecture**: Designed to handle high-volume production environments\n",
    "- **Easy Integration**: Simple API for integration with existing ML pipelines\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Integrate with your existing ML pipeline\n",
    "2. Configure appropriate thresholds for your use case\n",
    "3. Set up automated retraining workflows\n",
    "4. Configure alerting and notification systems\n",
    "5. Monitor and adjust baselines over time\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
