# Sample Notebook for Performance Degradation Detection

This notebook demonstrates the use of the `Pynomaly` library to detect performance degradation over time in a production model.

## Setup

```python
import pandas as pd
import numpy as np

from pynomaly.application.services.performance_monitoring_service import PerformanceMonitoringService
from pynomaly.infrastructure.monitoring.performance_monitor import PerformanceMonitor
from pynomaly.domain.entities import Dataset, Detector
from pynomaly.infrastructure.adapters.sklearn_adapter import SklearnAdapter
```

## Initialize Performance Monitoring

```python
# Create a performance monitor
performance_monitor = PerformanceMonitor(
    max_history=1000,
    alert_thresholds={
        "execution_time": 30.0,     # seconds
        "memory_usage": 1000.0,     # MB
        "cpu_usage": 80.0,          # percentage
        "samples_per_second": 100.0 # minimum throughput
    },
    monitoring_interval=1.0
)

# Initialize the monitoring service
monitoring_service = PerformanceMonitoringService(performance_monitor=performance_monitor)
```

## Simulate Model Deployment & Monitoring

```python
# Generate synthetic data
data = pd.DataFrame(np.random.normal(0, 1, (1000, 5)))
dataset = Dataset(name="Synthetic Data", data=data)

detector = SklearnAdapter(
    algorithm_name="IsolationForest",
    name="Synthetic Detector"
)

# Detection and monitoring function
def detect_and_monitor(detector, dataset):
    detector.fit(dataset)
    result = detector.detect(dataset)
    return result

# Monitor detection operation with the service
result, metrics = monitoring_service.monitor_detection_operation(
    detector=detector,
    dataset=dataset,
    operation_func=detect_and_monitor
)

print(f"Detection completed in {metrics.execution_time:.2f}s")
print(f"Memory usage: {metrics.memory_usage:.1f}MB")
print(f"Throughput: {metrics.samples_per_second:.1f} samples/sec")
```

## Analyze Performance Trends

```python
from datetime import timedelta

# Analyze performance trends over a week
trends = monitoring_service.get_performance_trends(
    operation_name="isolation_forest_detection",
    time_window=timedelta(days=7),
    bucket_size=timedelta(hours=1)
)

print(f"Analyzed {trends['total_operations']} operations")
print(f"Execution time trend: {trends['trends']['execution_time']}")
print(f"Memory usage trend: {trends['trends']['memory_usage']}")
```

## Automatic Retraining Integration

```python
from pynomaly.application.services.auto_retraining_service import AutoRetrainingService

# Setup retraining
auto_retraining = AutoRetrainingService()

# Retraining handler
def check_and_retrain(alert):
    if alert.metric_name == "accuracy" and alert.severity in ["high", "critical"]:
        decision = auto_retraining.should_retrain_model(
            model_id=get_model_id_from_operation(alert.operation_name),
            performance_metrics=get_current_metrics(alert.operation_name)
        )
        if decision.should_retrain:
            print(f"Triggering retraining: {decision.primary_trigger}")
            plan = auto_retraining.create_retraining_plan(
                model_id=get_model_id_from_operation(alert.operation_name),
                trigger=decision.primary_trigger
            )
            result = auto_retraining.execute_retraining_plan(plan)
            if result.success:
                print(f"Retraining completed successfully")
                print(f"Performance improvement: {result.performance_improvement}")
            else:
                print(f"Retraining failed: {result.error_message}")
    
# Attach handler to monitoring service
monitoring_service.add_alert_handler(check_and_retrain)
```

This notebook demonstrates the end-to-end workflow for performance monitoring and degradation detection in `Pynomaly`. By following this guide, you can ensure that models deployed in production remain performant and reliable.
