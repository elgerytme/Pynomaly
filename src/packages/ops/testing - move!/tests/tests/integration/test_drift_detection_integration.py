#!/usr/bin/env python3
"""Integration test for drift detection and monitoring functionality."""

import asyncio
import os
import sys

import numpy as np

# Add the src directory to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))


async def test_drift_detection_integration():
    """Test comprehensive drift detection integration."""
    print("üîç Testing Pynomaly Drift Detection Integration")
    print("=" * 55)

    try:
        # Test imports
        from monorepo.application.services.drift_detection_service import (
            DriftDetectionService,
        )
        from monorepo.application.use_cases.drift_monitoring_use_case import (
            DriftMonitoringUseCase,
        )
        from monorepo.domain.entities.drift_detection import (
            DriftDetectionMethod,
            DriftSeverity,
            ModelMonitoringConfig,
        )

        print("‚úÖ All drift detection imports successful")

        # Test drift detection service
        print("\nüîß Testing Drift Detection Service")
        print("-" * 35)

        drift_service = DriftDetectionService()
        print("‚úÖ Drift detection service created")

        # Create sample datasets
        print("\nüìä Creating Sample Datasets")
        print("-" * 30)

        # Reference dataset (normal distribution)
        np.random.seed(42)
        reference_data = np.random.randn(1000, 4)
        print(f"‚úÖ Reference dataset created: {reference_data.shape}")

        # Current dataset (with drift - shifted distribution)
        current_data_no_drift = np.random.randn(500, 4)
        current_data_with_drift = np.random.randn(500, 4) + 1.5  # Significant shift
        print("‚úÖ Current datasets created (with and without drift)")

        feature_names = ["temperature", "humidity", "pressure", "wind_speed"]

        # Test data drift detection - no drift
        print("\nüîç Testing Data Drift Detection - No Drift")
        print("-" * 45)

        result_no_drift = await drift_service.detect_data_drift(
            detector_id="weather_detector",
            reference_data=reference_data,
            current_data=current_data_no_drift,
            feature_names=feature_names,
        )

        print("‚úÖ No drift detection completed")
        print(f"   Drift detected: {result_no_drift.drift_detected}")
        print(f"   Severity: {result_no_drift.severity.value}")
        print(f"   Affected features: {len(result_no_drift.affected_features)}")
        print(f"   Confidence: {result_no_drift.confidence:.3f}")

        # Test data drift detection - with drift
        print("\nüö® Testing Data Drift Detection - With Drift")
        print("-" * 45)

        result_with_drift = await drift_service.detect_data_drift(
            detector_id="weather_detector",
            reference_data=reference_data,
            current_data=current_data_with_drift,
            feature_names=feature_names,
            detection_methods=[
                DriftDetectionMethod.KOLMOGOROV_SMIRNOV,
                DriftDetectionMethod.JENSEN_SHANNON,
                DriftDetectionMethod.POPULATION_STABILITY_INDEX,
            ],
        )

        print("‚úÖ Drift detection completed")
        print(f"   Drift detected: {result_with_drift.drift_detected}")
        print(f"   Severity: {result_with_drift.severity.value}")
        print(f"   Affected features: {len(result_with_drift.affected_features)}")
        print(f"   Features: {result_with_drift.affected_features}")
        print(f"   Confidence: {result_with_drift.confidence:.3f}")
        print(f"   Recommendations: {len(result_with_drift.recommendations)}")

        # Display feature drift scores
        print("\nüìà Feature Drift Scores:")
        for feature, score in result_with_drift.feature_drift_scores.items():
            print(f"   {feature}: {score:.3f}")

        # Test performance drift detection
        print("\nüìâ Testing Performance Drift Detection")
        print("-" * 38)

        reference_metrics = {
            "accuracy": 0.95,
            "precision": 0.92,
            "recall": 0.90,
            "f1": 0.91,
            "auc": 0.94,
        }

        # Simulate performance degradation
        current_metrics = {
            "accuracy": 0.83,
            "precision": 0.80,
            "recall": 0.78,
            "f1": 0.79,
            "auc": 0.81,
        }

        perf_result = await drift_service.detect_performance_drift(
            detector_id="weather_detector",
            reference_metrics=reference_metrics,
            current_metrics=current_metrics,
            threshold=0.05,
        )

        print("‚úÖ Performance drift detection completed")
        print(f"   Performance drift detected: {perf_result.drift_detected}")
        print(f"   Severity: {perf_result.severity.value}")
        print(f"   Affected metrics: {len(perf_result.affected_features)}")

        # Display performance changes
        print("\nüìä Performance Changes:")
        for metric, change in perf_result.feature_drift_scores.items():
            print(f"   {metric}: {change:.3f} ({change*100:.1f}%)")

        # Test prediction drift detection
        print("\nüéØ Testing Prediction Drift Detection")
        print("-" * 38)

        # Reference predictions (binary classification)
        reference_predictions = np.random.choice([0, 1], size=1000, p=[0.7, 0.3])

        # Current predictions (different distribution)
        current_predictions = np.random.choice([0, 1], size=500, p=[0.4, 0.6])

        pred_result = await drift_service.detect_prediction_drift(
            detector_id="weather_detector",
            reference_predictions=reference_predictions,
            current_predictions=current_predictions,
        )

        print("‚úÖ Prediction drift detection completed")
        print(f"   Prediction drift detected: {pred_result.drift_detected}")
        print(f"   Severity: {pred_result.severity.value}")
        print(f"   Confidence: {pred_result.confidence:.3f}")

        # Test monitoring configuration
        print("\n‚öôÔ∏è Testing Monitoring Configuration")
        print("-" * 35)

        monitoring_config = ModelMonitoringConfig(
            detector_id="weather_detector",
            enabled=True,
            check_interval_hours=6,
            reference_window_days=30,
            comparison_window_days=7,
            min_sample_size=100,
            enabled_methods=[
                DriftDetectionMethod.KOLMOGOROV_SMIRNOV,
                DriftDetectionMethod.JENSEN_SHANNON,
            ],
            drift_thresholds={
                "ks_statistic": 0.2,
                "js_divergence": 0.1,
                "psi_score": 0.25,
                "performance_drop": 0.05,
            },
            alert_on_severity=[DriftSeverity.HIGH, DriftSeverity.CRITICAL],
            notification_channels=["email", "slack"],
            features_to_monitor=feature_names,
        )

        print("‚úÖ Monitoring configuration created")
        print(f"   Check interval: {monitoring_config.check_interval_hours} hours")
        print(f"   Methods enabled: {len(monitoring_config.enabled_methods)}")
        print(
            f"   Alert severities: {[s.value for s in monitoring_config.alert_on_severity]}"
        )
        print(f"   Notification channels: {monitoring_config.notification_channels}")

        # Test monitoring setup
        monitoring_status = await drift_service.setup_monitoring(
            "weather_detector", monitoring_config
        )

        print("‚úÖ Monitoring setup completed")
        print(f"   Status: {monitoring_status.status.value}")
        print(f"   Checks performed: {monitoring_status.checks_performed}")
        print(f"   Health score: {monitoring_status.overall_health_score:.3f}")

        # Test drift monitoring use case
        print("\nüéõÔ∏è Testing Drift Monitoring Use Case")
        print("-" * 37)

        drift_use_case = DriftMonitoringUseCase(drift_detection_service=drift_service)
        print("‚úÖ Drift monitoring use case created")

        # Test immediate drift check
        immediate_result = await drift_use_case.perform_drift_check(
            detector_id="weather_detector",
            reference_data=reference_data,
            current_data=current_data_with_drift,
            feature_names=feature_names,
        )

        print("‚úÖ Immediate drift check completed")
        print(f"   Drift detected: {immediate_result.drift_detected}")
        print(f"   Severity: {immediate_result.severity.value}")

        # Test performance drift check through use case
        perf_use_case_result = await drift_use_case.check_performance_drift(
            detector_id="weather_detector",
            reference_metrics=reference_metrics,
            current_metrics=current_metrics,
        )

        print("‚úÖ Performance drift check through use case completed")
        print(f"   Performance drift: {perf_use_case_result.drift_detected}")

        # Test monitoring status retrieval
        status = await drift_use_case.get_monitoring_status("weather_detector")
        if status:
            print("‚úÖ Monitoring status retrieved")
            print(f"   Status: {status.status.value}")
            print(f"   Last check: {status.last_check_at}")
            print(f"   Next check: {status.next_check_at}")

        # Test active monitors listing
        active_monitors = await drift_use_case.list_active_monitors()
        print(f"‚úÖ Active monitors listed: {len(active_monitors)} monitors")
        for monitor in active_monitors:
            print(f"   ‚Ä¢ {monitor}")

        # Test drift report generation
        print("\nüìã Testing Drift Report Generation")
        print("-" * 35)

        drift_report = await drift_use_case.generate_drift_report(
            detector_id="weather_detector", period_days=30
        )

        print("‚úÖ Drift report generated")
        print(
            f"   Report period: {drift_report.report_period_start.strftime('%Y-%m-%d')} to {drift_report.report_period_end.strftime('%Y-%m-%d')}"
        )
        print(f"   Total checks: {drift_report.total_checks}")
        print(f"   Drift detections: {drift_report.drift_detections}")
        print(f"   Drift rate: {drift_report.get_drift_rate():.1f}%")
        print(f"   Average health score: {drift_report.average_health_score:.3f}")

        # Test system health
        print("\nüè• Testing System Health")
        print("-" * 25)

        system_health = await drift_use_case.get_system_health()
        print("‚úÖ System health retrieved")
        print(f"   Total monitors: {system_health.get('total_monitors', 0)}")
        print(f"   Active monitors: {system_health.get('active_monitors', 0)}")
        print(f"   System status: {system_health.get('system_status', 'unknown')}")
        print(
            f"   Average health score: {system_health.get('average_health_score', 0):.3f}"
        )
        print(
            f"   Recent drift detections: {system_health.get('recent_drift_detections', 0)}"
        )

        # Test monitoring pause/resume
        print("\n‚è∏Ô∏è Testing Monitoring Control")
        print("-" * 30)

        # Pause monitoring
        pause_success = await drift_use_case.pause_monitoring("weather_detector")
        print(f"‚úÖ Monitoring pause: {pause_success}")

        # Resume monitoring
        resume_success = await drift_use_case.resume_monitoring("weather_detector")
        print(f"‚úÖ Monitoring resume: {resume_success}")

        # Test statistical methods directly
        print("\nüßÆ Testing Statistical Methods")
        print("-" * 32)

        from monorepo.application.services.drift_detection_service import (
            StatisticalDriftDetector,
        )

        stat_detector = StatisticalDriftDetector()

        # Test KS test
        ref_sample = np.random.randn(1000)
        curr_sample = np.random.randn(500) + 1.0

        ks_stat, ks_p = stat_detector.kolmogorov_smirnov_test(ref_sample, curr_sample)
        print(f"‚úÖ Kolmogorov-Smirnov test: statistic={ks_stat:.3f}, p-value={ks_p:.6f}")

        # Test Jensen-Shannon divergence
        js_div = stat_detector.jensen_shannon_divergence(ref_sample, curr_sample)
        print(f"‚úÖ Jensen-Shannon divergence: {js_div:.3f}")

        # Test PSI
        psi = stat_detector.population_stability_index(ref_sample, curr_sample)
        print(f"‚úÖ Population Stability Index: {psi:.3f}")

        # Test Wasserstein distance
        wasserstein = stat_detector.wasserstein_distance(ref_sample, curr_sample)
        print(f"‚úÖ Wasserstein distance: {wasserstein:.3f}")

        print("\nüéâ Drift Detection Integration Summary")
        print("=" * 45)
        print("‚úÖ Core Services: DriftDetectionService, StatisticalDriftDetector")
        print("‚úÖ Use Cases: DriftMonitoringUseCase with comprehensive orchestration")
        print("‚úÖ Detection Methods:")
        print("   ‚Ä¢ Data drift detection with multiple statistical tests")
        print("   ‚Ä¢ Performance drift monitoring and alerting")
        print("   ‚Ä¢ Prediction drift analysis")
        print("   ‚Ä¢ Feature-level drift scoring and analysis")
        print("‚úÖ Monitoring Features:")
        print("   ‚Ä¢ Automated monitoring configuration and scheduling")
        print("   ‚Ä¢ Health score tracking and trend analysis")
        print("   ‚Ä¢ Alert management with severity-based notifications")
        print("   ‚Ä¢ Comprehensive reporting and system health monitoring")
        print("‚úÖ Statistical Methods:")
        print("   ‚Ä¢ Kolmogorov-Smirnov test for distribution comparison")
        print("   ‚Ä¢ Jensen-Shannon divergence for probabilistic drift")
        print("   ‚Ä¢ Population Stability Index for categorical features")
        print("   ‚Ä¢ Wasserstein distance for distribution differences")
        print("‚úÖ Production Features:")
        print("   ‚Ä¢ Configurable thresholds and detection sensitivity")
        print("   ‚Ä¢ Multi-channel notification support")
        print("   ‚Ä¢ Pause/resume monitoring capabilities")
        print("   ‚Ä¢ Comprehensive error handling and graceful degradation")

        print("\nüìä Key Capabilities:")
        print("   ‚Ä¢ Real-time drift detection with configurable sensitivity")
        print("   ‚Ä¢ Multi-method ensemble approach for robust detection")
        print("   ‚Ä¢ Feature-level analysis for root cause identification")
        print("   ‚Ä¢ Performance monitoring with business impact assessment")
        print("   ‚Ä¢ Automated alerting with severity-based escalation")
        print("   ‚Ä¢ Historical trend analysis and reporting")
        print("   ‚Ä¢ System health monitoring and operational visibility")
        print("   ‚Ä¢ Production-ready monitoring with high availability")

        return True

    except Exception as e:
        print(f"‚ùå Error testing drift detection integration: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = asyncio.run(test_drift_detection_integration())
    print(f"\n{'‚úÖ SUCCESS' if success else '‚ùå FAILED'}")
    sys.exit(0 if success else 1)
