#!/usr/bin/env python3
"""
Tabular Data Anomaly Detection Template

This template provides a comprehensive framework for detecting anomalies in tabular datasets,
including feature engineering, multiple algorithm comparison, and business-focused reporting.
"""

import warnings
from datetime import datetime

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

warnings.filterwarnings('ignore')

import os

# Pynomaly imports (adjust path as needed)
import sys

# Statistical imports
from scipy import stats
from scipy.stats import chi2_contingency
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, VarianceThreshold, f_classif
from sklearn.metrics import auc, classification_report, confusion_matrix, roc_curve
from sklearn.model_selection import cross_val_score, train_test_split

# Machine learning imports
from sklearn.preprocessing import (
    LabelEncoder,
    MinMaxScaler,
    OneHotEncoder,
    StandardScaler,
)

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'src'))

from pynomaly_detection.domain.entities.dataset import Dataset
from pynomaly_detection.domain.value_objects.contamination_rate import ContaminationRate
from pynomaly_detection.infrastructure.adapters.pyod_adapter import PyODAdapter
from pynomaly_detection.infrastructure.adapters.sklearn_adapter import SklearnAdapter


class TabularAnomalyDetector:
    """
    Comprehensive tabular data anomaly detection with advanced preprocessing,
    multiple algorithm comparison, and business intelligence reporting.

    Features:
    - Automated data profiling and quality assessment
    - Advanced feature engineering and selection
    - Multiple anomaly detection algorithms
    - Ensemble methods and voting
    - Statistical significance testing
    - Business-focused reporting and visualization
    """

    def __init__(self,
                 contamination_rate: float = 0.05,
                 algorithms: list = None,
                 feature_selection: bool = True,
                 ensemble_method: str = "voting",
                 random_state: int = 42):
        """
        Initialize the tabular anomaly detector.

        Args:
            contamination_rate: Expected proportion of anomalies
            algorithms: List of algorithms to use (default: ['IsolationForest', 'LOF', 'OneClassSVM'])
            feature_selection: Whether to perform automatic feature selection
            ensemble_method: 'voting', 'averaging', or 'stacking'
            random_state: Random seed for reproducibility
        """
        self.contamination_rate = contamination_rate
        self.algorithms = algorithms or ['IsolationForest', 'LocalOutlierFactor', 'OneClassSVM']
        self.feature_selection = feature_selection
        self.ensemble_method = ensemble_method
        self.random_state = random_state

        # Initialize components
        self.scaler = StandardScaler()
        self.feature_selector = None
        self.label_encoders = {}
        self.detectors = {}
        self.results = {}
        self.data_profile = {}

        np.random.seed(random_state)

    def load_data(self, data_source, target_column=None, id_columns=None):
        """
        Load tabular data from various sources with comprehensive validation.

        Args:
            data_source: File path, DataFrame, or data generator
            target_column: Name of target column for supervised evaluation
            id_columns: List of ID columns to exclude from features

        Returns:
            Processed DataFrame
        """
        if isinstance(data_source, str):
            # Load from file
            if data_source.endswith('.csv'):
                df = pd.read_csv(data_source)
            elif data_source.endswith('.json'):
                df = pd.read_json(data_source)
            elif data_source.endswith('.xlsx') or data_source.endswith('.xls'):
                df = pd.read_excel(data_source)
            elif data_source.endswith('.parquet'):
                df = pd.read_parquet(data_source)
            else:
                raise ValueError(f"Unsupported file format: {data_source}")
        elif isinstance(data_source, pd.DataFrame):
            df = data_source.copy()
        else:
            raise ValueError("Data source must be file path or DataFrame")

        # Store original data and metadata
        self.original_data = df
        self.target_column = target_column
        self.id_columns = id_columns or []

        print(f"✅ Loaded tabular data: {df.shape}")
        print(f"📊 Columns: {len(df.columns)}\")\n        print(f\"🎯 Target column: {target_column}\")\n        print(f\"🔑 ID columns: {self.id_columns}\")\n        \n        return df\n    \n    def profile_data(self, df=None):\n        \"\"\"\n        Comprehensive data profiling and quality assessment.\n        \n        Args:\n            df: DataFrame to profile (uses loaded data if None)\n            \n        Returns:\n            Dictionary with profiling results\n        \"\"\"\n        if df is None:\n            df = self.original_data\n        \n        print(\"=== DATA PROFILING AND QUALITY ASSESSMENT ===\")\n        \n        profile = {\n            'basic_info': {},\n            'data_quality': {},\n            'feature_analysis': {},\n            'correlation_analysis': {},\n            'statistical_tests': {}\n        }\n        \n        # Basic information\n        profile['basic_info'] = {\n            'shape': df.shape,\n            'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2,\n            'dtypes': df.dtypes.value_counts().to_dict(),\n            'numeric_columns': len(df.select_dtypes(include=[np.number]).columns),\n            'categorical_columns': len(df.select_dtypes(include=['object']).columns),\n            'datetime_columns': len(df.select_dtypes(include=['datetime']).columns)\n        }\n        \n        print(f\"📊 Basic Information:\")\n        print(f\"  • Shape: {profile['basic_info']['shape']}\")\n        print(f\"  • Memory usage: {profile['basic_info']['memory_usage_mb']:.2f} MB\")\n        print(f\"  • Numeric columns: {profile['basic_info']['numeric_columns']}\")\n        print(f\"  • Categorical columns: {profile['basic_info']['categorical_columns']}\")\n        \n        # Data quality assessment\n        missing_info = df.isnull().sum()\n        missing_pct = (missing_info / len(df)) * 100\n        \n        profile['data_quality'] = {\n            'missing_values': missing_info.to_dict(),\n            'missing_percentages': missing_pct.to_dict(),\n            'duplicate_rows': df.duplicated().sum(),\n            'duplicate_percentage': (df.duplicated().sum() / len(df)) * 100,\n            'columns_with_missing': (missing_info > 0).sum(),\n            'high_missing_columns': missing_pct[missing_pct > 50].index.tolist()\n        }\n        \n        print(f\"\\n❓ Data Quality:\")\n        print(f\"  • Missing values: {missing_info.sum()} ({missing_pct.mean():.1f}% avg)\")\n        print(f\"  • Duplicate rows: {profile['data_quality']['duplicate_rows']} ({profile['data_quality']['duplicate_percentage']:.1f}%)\")\n        print(f\"  • Columns with >50% missing: {len(profile['data_quality']['high_missing_columns'])}\")\n        \n        # Feature analysis\n        numeric_cols = df.select_dtypes(include=[np.number]).columns\n        categorical_cols = df.select_dtypes(include=['object']).columns\n        \n        feature_analysis = {\n            'numeric_features': {},\n            'categorical_features': {},\n            'constant_features': [],\n            'high_cardinality_features': []\n        }\n        \n        # Numeric feature analysis\n        for col in numeric_cols:\n            if col not in self.id_columns and col != self.target_column:\n                series = df[col].dropna()\n                if len(series) > 0:\n                    feature_analysis['numeric_features'][col] = {\n                        'mean': series.mean(),\n                        'std': series.std(),\n                        'min': series.min(),\n                        'max': series.max(),\n                        'skewness': series.skew(),\n                        'kurtosis': series.kurtosis(),\n                        'outlier_count_iqr': self._count_iqr_outliers(series),\n                        'outlier_count_zscore': self._count_zscore_outliers(series),\n                        'unique_values': series.nunique(),\n                        'is_constant': series.nunique() <= 1\n                    }\n                    \n                    if series.nunique() <= 1:\n                        feature_analysis['constant_features'].append(col)\n        \n        # Categorical feature analysis\n        for col in categorical_cols:\n            if col not in self.id_columns and col != self.target_column:\n                series = df[col].dropna()\n                if len(series) > 0:\n                    unique_count = series.nunique()\n                    feature_analysis['categorical_features'][col] = {\n                        'unique_count': unique_count,\n                        'most_frequent': series.mode().iloc[0] if not series.mode().empty else None,\n                        'most_frequent_count': series.value_counts().iloc[0],\n                        'cardinality_ratio': unique_count / len(series),\n                        'is_high_cardinality': unique_count > len(series) * 0.8,\n                        'is_constant': unique_count <= 1\n                    }\n                    \n                    if unique_count > len(series) * 0.8:\n                        feature_analysis['high_cardinality_features'].append(col)\n                    \n                    if unique_count <= 1:\n                        feature_analysis['constant_features'].append(col)\n        \n        profile['feature_analysis'] = feature_analysis\n        \n        print(f\"\\n🔍 Feature Analysis:\")\n        print(f\"  • Constant features: {len(feature_analysis['constant_features'])}\")\n        print(f\"  • High cardinality features: {len(feature_analysis['high_cardinality_features'])}\")\n        \n        # Correlation analysis\n        if len(numeric_cols) > 1:\n            correlation_matrix = df[numeric_cols].corr()\n            \n            # Find high correlation pairs\n            high_corr_pairs = []\n            for i in range(len(correlation_matrix.columns)):\n                for j in range(i+1, len(correlation_matrix.columns)):\n                    corr_val = correlation_matrix.iloc[i, j]\n                    if abs(corr_val) > 0.8 and not np.isnan(corr_val):\n                        high_corr_pairs.append((\n                            correlation_matrix.columns[i],\n                            correlation_matrix.columns[j],\n                            corr_val\n                        ))\n            \n            profile['correlation_analysis'] = {\n                'correlation_matrix': correlation_matrix,\n                'high_correlation_pairs': high_corr_pairs,\n                'max_correlation': correlation_matrix.abs().max().max(),\n                'avg_correlation': correlation_matrix.abs().mean().mean()\n            }\n            \n            print(f\"\\n🔗 Correlation Analysis:\")\n            print(f\"  • High correlation pairs (|r| > 0.8): {len(high_corr_pairs)}\")\n            print(f\"  • Maximum correlation: {profile['correlation_analysis']['max_correlation']:.3f}\")\n            \n            if high_corr_pairs:\n                for col1, col2, corr in high_corr_pairs[:3]:  # Show top 3\n                    print(f\"    - {col1} ↔ {col2}: {corr:.3f}\")\n        \n        # Target variable analysis (if available)\n        if self.target_column and self.target_column in df.columns:\n            target_analysis = self._analyze_target_variable(df)\n            profile['target_analysis'] = target_analysis\n            \n            print(f\"\\n🎯 Target Variable Analysis:\")\n            print(f\"  • Class distribution: {target_analysis['class_distribution']}\")\n            print(f\"  • Imbalance ratio: {target_analysis['imbalance_ratio']:.3f}\")\n        \n        # Store profile\n        self.data_profile = profile\n        \n        return profile\n    \n    def _count_iqr_outliers(self, series):\n        \"\"\"Count outliers using IQR method.\"\"\"\n        Q1 = series.quantile(0.25)\n        Q3 = series.quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n        return ((series < lower_bound) | (series > upper_bound)).sum()\n    \n    def _count_zscore_outliers(self, series, threshold=3):\n        \"\"\"Count outliers using Z-score method.\"\"\"\n        z_scores = np.abs(stats.zscore(series))\n        return (z_scores > threshold).sum()\n    \n    def _analyze_target_variable(self, df):\n        \"\"\"Analyze target variable distribution and relationships.\"\"\"\n        target = df[self.target_column]\n        \n        analysis = {\n            'class_distribution': target.value_counts().to_dict(),\n            'class_proportions': target.value_counts(normalize=True).to_dict(),\n            'imbalance_ratio': target.value_counts().min() / target.value_counts().max(),\n            'is_binary': target.nunique() == 2,\n            'missing_values': target.isnull().sum()\n        }\n        \n        # Feature-target relationships\n        numeric_cols = df.select_dtypes(include=[np.number]).columns\n        numeric_cols = [col for col in numeric_cols if col != self.target_column and col not in self.id_columns]\n        \n        if len(numeric_cols) > 0 and analysis['is_binary']:\n            # Statistical tests for numeric features\n            significant_features = []\n            for col in numeric_cols:\n                try:\n                    group1 = df[df[self.target_column] == target.unique()[0]][col].dropna()\n                    group2 = df[df[self.target_column] == target.unique()[1]][col].dropna()\n                    \n                    if len(group1) > 5 and len(group2) > 5:\n                        stat, p_value = stats.ttest_ind(group1, group2)\n                        if p_value < 0.05:\n                            significant_features.append((col, p_value))\n                except:\n                    continue\n            \n            analysis['significant_numeric_features'] = significant_features\n        \n        return analysis\n    \n    def preprocess_data(self, df=None, handle_missing='auto', scale_features=True):\n        \"\"\"\n        Comprehensive data preprocessing pipeline.\n        \n        Args:\n            df: DataFrame to preprocess (uses loaded data if None)\n            handle_missing: Strategy for missing values ('auto', 'drop', 'median', 'mode')\n            scale_features: Whether to scale numeric features\n            \n        Returns:\n            Preprocessed feature matrix and target vector\n        \"\"\"\n        if df is None:\n            df = self.original_data\n        \n        print(\"=== DATA PREPROCESSING ===\")\n        \n        # Make a copy\n        df_processed = df.copy()\n        \n        # Remove ID columns\n        if self.id_columns:\n            id_cols_present = [col for col in self.id_columns if col in df_processed.columns]\n            if id_cols_present:\n                df_processed = df_processed.drop(columns=id_cols_present)\n                print(f\"🗑️ Removed ID columns: {id_cols_present}\")\n        \n        # Separate features and target\n        if self.target_column and self.target_column in df_processed.columns:\n            y = df_processed[self.target_column]\n            X = df_processed.drop(columns=[self.target_column])\n            print(f\"🎯 Target separated: {self.target_column}\")\n        else:\n            y = None\n            X = df_processed\n            print(\"📊 Unsupervised mode: no target column\")\n        \n        # Handle missing values\n        missing_before = X.isnull().sum().sum()\n        if missing_before > 0:\n            print(f\"❓ Handling {missing_before} missing values with '{handle_missing}' strategy...\")\n            X = self._handle_missing_values(X, strategy=handle_missing)\n            missing_after = X.isnull().sum().sum()\n            print(f\"  ✅ Missing values reduced from {missing_before} to {missing_after}\")\n        \n        # Remove constant features\n        if self.data_profile and 'feature_analysis' in self.data_profile:\n            constant_features = self.data_profile['feature_analysis']['constant_features']\n            constant_in_x = [col for col in constant_features if col in X.columns]\n            if constant_in_x:\n                X = X.drop(columns=constant_in_x)\n                print(f\"🗑️ Removed {len(constant_in_x)} constant features\")\n        \n        # Encode categorical variables\n        categorical_cols = X.select_dtypes(include=['object']).columns\n        if len(categorical_cols) > 0:\n            print(f\"🏷️ Encoding {len(categorical_cols)} categorical columns...\")\n            X = self._encode_categorical_features(X, categorical_cols)\n        \n        # Feature selection (if enabled)\n        if self.feature_selection and len(X.columns) > 10:\n            print(f\"🎯 Performing feature selection...\")\n            X = self._select_features(X, y)\n        \n        # Scale numeric features\n        if scale_features:\n            numeric_cols = X.select_dtypes(include=[np.number]).columns\n            if len(numeric_cols) > 0:\n                print(f\"📏 Scaling {len(numeric_cols)} numeric features...\")\n                X[numeric_cols] = self.scaler.fit_transform(X[numeric_cols])\n                print(f\"  ✅ StandardScaler applied\")\n        \n        print(f\"\\n✅ Preprocessing complete\")\n        print(f\"  • Final feature shape: {X.shape}\")\n        print(f\"  • Feature columns: {len(X.columns)}\")\n        \n        return X, y\n    \n    def _handle_missing_values(self, X, strategy='auto'):\n        \"\"\"Handle missing values using specified strategy.\"\"\"\n        if strategy == 'auto':\n            # Auto strategy: use median for numeric, mode for categorical\n            strategy = 'intelligent'\n        \n        if strategy == 'drop':\n            return X.dropna()\n        \n        elif strategy == 'intelligent':\n            X_filled = X.copy()\n            \n            # Numeric columns: use median\n            numeric_cols = X.select_dtypes(include=[np.number]).columns\n            for col in numeric_cols:\n                if X[col].isnull().any():\n                    median_val = X[col].median()\n                    X_filled[col].fillna(median_val, inplace=True)\n            \n            # Categorical columns: use mode\n            categorical_cols = X.select_dtypes(include=['object']).columns\n            for col in categorical_cols:\n                if X[col].isnull().any():\n                    mode_val = X[col].mode().iloc[0] if not X[col].mode().empty else 'Unknown'\n                    X_filled[col].fillna(mode_val, inplace=True)\n            \n            return X_filled\n        \n        elif strategy == 'median':\n            return X.fillna(X.median())\n        \n        elif strategy == 'mode':\n            return X.fillna(X.mode().iloc[0])\n        \n        else:\n            raise ValueError(f\"Unknown missing value strategy: {strategy}\")\n    \n    def _encode_categorical_features(self, X, categorical_cols):\n        \"\"\"Encode categorical features using appropriate methods.\"\"\"\n        X_encoded = X.copy()\n        \n        for col in categorical_cols:\n            unique_count = X[col].nunique()\n            \n            if unique_count <= 10:  # Low cardinality: one-hot encoding\n                dummies = pd.get_dummies(X[col], prefix=col, drop_first=True)\n                X_encoded = pd.concat([X_encoded.drop(columns=[col]), dummies], axis=1)\n                print(f\"  • {col}: one-hot encoded ({unique_count} categories)\")\n            \n            else:  # High cardinality: label encoding\n                le = LabelEncoder()\n                X_encoded[col] = le.fit_transform(X[col].astype(str))\n                self.label_encoders[col] = le\n                print(f\"  • {col}: label encoded ({unique_count} categories)\")\n        \n        return X_encoded\n    \n    def _select_features(self, X, y=None):\n        \"\"\"Perform automatic feature selection.\"\"\"\n        original_features = len(X.columns)\n        \n        # Variance threshold (remove low-variance features)\n        variance_selector = VarianceThreshold(threshold=0.01)\n        X_var = variance_selector.fit_transform(X)\n        selected_features = X.columns[variance_selector.get_support()]\n        X_selected = pd.DataFrame(X_var, columns=selected_features, index=X.index)\n        \n        print(f\"  • Variance threshold: {original_features} → {len(selected_features)} features\")\n        \n        # If target is available, use univariate selection\n        if y is not None and len(selected_features) > 20:\n            try:\n                k_best = min(20, len(selected_features))\n                selector = SelectKBest(score_func=f_classif, k=k_best)\n                X_final = selector.fit_transform(X_selected, y)\n                final_features = selected_features[selector.get_support()]\n                X_selected = pd.DataFrame(X_final, columns=final_features, index=X.index)\n                \n                print(f\"  • Univariate selection: {len(selected_features)} → {len(final_features)} features\")\n                \n                self.feature_selector = selector\n            except Exception as e:\n                print(f\"  ⚠️ Univariate selection failed: {str(e)}\")\n        \n        return X_selected\n    \n    def detect_anomalies(self, X=None, y=None, algorithms=None):\n        \"\"\"\n        Detect anomalies using multiple algorithms.\n        \n        Args:\n            X: Feature matrix (auto-generated if None)\n            y: Target vector (for evaluation)\n            algorithms: List of algorithms to use\n            \n        Returns:\n            Dictionary with detection results for each algorithm\n        \"\"\"\n        if X is None:\n            X, y = self.preprocess_data()\n        \n        if algorithms is None:\n            algorithms = self.algorithms\n        \n        print(f\"=== ANOMALY DETECTION ===\")\n        print(f\"🤖 Testing {len(algorithms)} algorithms...\")\n        \n        # Create Pynomaly dataset\n        dataset = Dataset(\n            name=\"tabular_features\",\n            data=X.values if hasattr(X, 'values') else X,\n            feature_names=list(X.columns) if hasattr(X, 'columns') else None\n        )\n        \n        contamination_rate = ContaminationRate(self.contamination_rate)\n        results = {}\n        \n        for algorithm in algorithms:\n            print(f\"\\n🔄 Running {algorithm}...\")\n            \n            try:\n                start_time = datetime.now()\n                \n                # Initialize detector\n                detector = SklearnAdapter(algorithm, contamination_rate=contamination_rate)\n                \n                # Fit and detect\n                result = detector.fit_detect(dataset)\n                \n                # Extract results\n                scores = np.array([score.value for score in result.scores])\n                threshold = np.percentile(scores, (1 - self.contamination_rate) * 100)\n                predictions = (scores > threshold).astype(int)\n                \n                execution_time = (datetime.now() - start_time).total_seconds()\n                \n                results[algorithm] = {\n                    'scores': scores,\n                    'predictions': predictions,\n                    'threshold': threshold,\n                    'anomaly_count': predictions.sum(),\n                    'anomaly_rate': predictions.mean(),\n                    'execution_time': execution_time,\n                    'detector': detector\n                }\n                \n                print(f\"  ✅ {algorithm} completed in {execution_time:.2f}s\")\n                print(f\"     Detected {predictions.sum()} anomalies ({predictions.mean():.1%})\")\n                \n            except Exception as e:\n                print(f\"  ❌ {algorithm} failed: {str(e)}\")\n                results[algorithm] = {'error': str(e)}\n        \n        # Create ensemble if multiple algorithms succeeded\n        successful_algorithms = [name for name, result in results.items() if 'error' not in result]\n        if len(successful_algorithms) > 1:\n            print(f\"\\n🔗 Creating ensemble from {len(successful_algorithms)} algorithms...\")\n            ensemble_result = self._create_ensemble(results, successful_algorithms)\n            results['Ensemble'] = ensemble_result\n            print(f\"  ✅ Ensemble created using {self.ensemble_method} method\")\n        \n        self.results = results\n        \n        print(f\"\\n🎯 Detection completed! {len(successful_algorithms)} algorithms successful.\")\n        \n        return results\n    \n    def _create_ensemble(self, results, algorithms):\n        \"\"\"Create ensemble predictions from multiple algorithms.\"\"\"\n        if self.ensemble_method == 'voting':\n            # Majority voting on predictions\n            predictions_matrix = np.column_stack([results[alg]['predictions'] for alg in algorithms])\n            ensemble_predictions = (np.mean(predictions_matrix, axis=1) > 0.5).astype(int)\n            \n            # Average scores\n            scores_matrix = np.column_stack([results[alg]['scores'] for alg in algorithms])\n            ensemble_scores = np.mean(scores_matrix, axis=1)\n            \n        elif self.ensemble_method == 'averaging':\n            # Simple averaging of scores\n            scores_matrix = np.column_stack([results[alg]['scores'] for alg in algorithms])\n            ensemble_scores = np.mean(scores_matrix, axis=1)\n            threshold = np.percentile(ensemble_scores, (1 - self.contamination_rate) * 100)\n            ensemble_predictions = (ensemble_scores > threshold).astype(int)\n            \n        elif self.ensemble_method == 'weighted_averaging':\n            # Weighted averaging (could be based on performance if y is available)\n            weights = np.ones(len(algorithms)) / len(algorithms)  # Equal weights for now\n            \n            scores_matrix = np.column_stack([results[alg]['scores'] for alg in algorithms])\n            ensemble_scores = np.average(scores_matrix, axis=1, weights=weights)\n            threshold = np.percentile(ensemble_scores, (1 - self.contamination_rate) * 100)\n            ensemble_predictions = (ensemble_scores > threshold).astype(int)\n            \n        else:\n            raise ValueError(f\"Unknown ensemble method: {self.ensemble_method}\")\n        \n        return {\n            'scores': ensemble_scores,\n            'predictions': ensemble_predictions,\n            'threshold': threshold if 'threshold' in locals() else np.percentile(ensemble_scores, (1 - self.contamination_rate) * 100),\n            'anomaly_count': ensemble_predictions.sum(),\n            'anomaly_rate': ensemble_predictions.mean(),\n            'method': self.ensemble_method,\n            'component_algorithms': algorithms\n        }\n    \n    def evaluate_performance(self, y_true, results=None):\n        \"\"\"\n        Evaluate detection performance against ground truth.\n        \n        Args:\n            y_true: Ground truth labels\n            results: Detection results (uses stored results if None)\n            \n        Returns:\n            Performance metrics for each algorithm\n        \"\"\"\n        if results is None:\n            results = self.results\n        \n        if not results:\n            print(\"❌ No results to evaluate. Run detect_anomalies() first.\")\n            return {}\n        \n        from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n        \n        print(\"=== PERFORMANCE EVALUATION ===\")\n        \n        performance_results = {}\n        \n        for algorithm, result in results.items():\n            if 'error' in result:\n                continue\n            \n            predictions = result['predictions']\n            scores = result['scores']\n            \n            # Align lengths if needed\n            min_length = min(len(y_true), len(predictions))\n            y_aligned = y_true[:min_length]\n            pred_aligned = predictions[:min_length]\n            scores_aligned = scores[:min_length]\n            \n            try:\n                metrics = {\n                    'precision': precision_score(y_aligned, pred_aligned),\n                    'recall': recall_score(y_aligned, pred_aligned),\n                    'f1_score': f1_score(y_aligned, pred_aligned),\n                    'roc_auc': roc_auc_score(y_aligned, scores_aligned),\n                    'average_precision': average_precision_score(y_aligned, scores_aligned),\n                    'execution_time': result.get('execution_time', 0)\n                }\n                \n                performance_results[algorithm] = metrics\n                \n                print(f\"\\n📊 {algorithm}:\")\n                print(f\"  • Precision: {metrics['precision']:.4f}\")\n                print(f\"  • Recall: {metrics['recall']:.4f}\")\n                print(f\"  • F1-Score: {metrics['f1_score']:.4f}\")\n                print(f\"  • ROC-AUC: {metrics['roc_auc']:.4f}\")\n                print(f\"  • Average Precision: {metrics['average_precision']:.4f}\")\n                \n            except Exception as e:\n                print(f\"  ❌ {algorithm} evaluation failed: {str(e)}\")\n        \n        # Identify best performers\n        if performance_results:\n            best_f1 = max(performance_results.items(), key=lambda x: x[1]['f1_score'])\n            best_auc = max(performance_results.items(), key=lambda x: x[1]['roc_auc'])\n            fastest = min(performance_results.items(), key=lambda x: x[1]['execution_time'])\n            \n            print(f\"\\n🏆 Best Performers:\")\n            print(f\"  • Best F1-Score: {best_f1[0]} ({best_f1[1]['f1_score']:.4f})\")\n            print(f\"  • Best ROC-AUC: {best_auc[0]} ({best_auc[1]['roc_auc']:.4f})\")\n            print(f\"  • Fastest: {fastest[0]} ({fastest[1]['execution_time']:.2f}s)\")\n        \n        return performance_results\n    \n    def visualize_results(self, X=None, results=None, save_path=None):\n        \"\"\"Create comprehensive visualizations of the analysis.\"\"\"\n        if X is None:\n            X, _ = self.preprocess_data()\n        if results is None:\n            results = self.results\n        \n        if not results:\n            print(\"❌ No results to visualize. Run detect_anomalies() first.\")\n            return\n        \n        # Filter successful results\n        successful_results = {name: result for name, result in results.items() if 'error' not in result}\n        \n        if not successful_results:\n            print(\"❌ No successful results to visualize.\")\n            return\n        \n        # Create visualization\n        n_algorithms = len(successful_results)\n        n_cols = min(3, n_algorithms)\n        n_rows = max(2, (n_algorithms + n_cols - 1) // n_cols)\n        \n        fig = plt.figure(figsize=(6 * n_cols, 6 * n_rows))\n        \n        # PCA for 2D visualization\n        if X.shape[1] > 2:\n            pca = PCA(n_components=2, random_state=self.random_state)\n            X_pca = pca.fit_transform(X)\n            print(f\"📊 PCA applied: {X.shape[1]} → 2 dimensions (explained variance: {pca.explained_variance_ratio_.sum():.1%})\")\n        else:\n            X_pca = X.values if hasattr(X, 'values') else X\n        \n        plot_idx = 1\n        \n        # Individual algorithm results\n        for algorithm, result in successful_results.items():\n            ax = plt.subplot(n_rows, n_cols, plot_idx)\n            \n            predictions = result['predictions']\n            scores = result['scores']\n            \n            # Ensure alignment\n            min_length = min(len(X_pca), len(predictions))\n            X_plot = X_pca[:min_length]\n            pred_plot = predictions[:min_length]\n            scores_plot = scores[:min_length]\n            \n            # Scatter plot colored by anomaly scores\n            scatter = ax.scatter(X_plot[:, 0], X_plot[:, 1], \n                               c=scores_plot, cmap='viridis', \n                               alpha=0.6, s=20)\n            \n            # Highlight detected anomalies\n            anomaly_mask = pred_plot.astype(bool)\n            if anomaly_mask.any():\n                ax.scatter(X_plot[anomaly_mask, 0], X_plot[anomaly_mask, 1], \n                          c='red', s=50, alpha=0.8, marker='x', linewidths=2)\n            \n            ax.set_title(f'{algorithm}\\n{result[\"anomaly_count\"]} anomalies ({result[\"anomaly_rate\"]:.1%})', \n                        fontweight='bold')\n            ax.set_xlabel('First Principal Component' if X.shape[1] > 2 else 'Feature 1')\n            ax.set_ylabel('Second Principal Component' if X.shape[1] > 2 else 'Feature 2')\n            \n            # Add colorbar\n            plt.colorbar(scatter, ax=ax, label='Anomaly Score')\n            \n            plot_idx += 1\n        \n        # Additional analysis plots\n        if plot_idx <= n_rows * n_cols:\n            # Score distribution comparison\n            ax = plt.subplot(n_rows, n_cols, plot_idx)\n            \n            for algorithm, result in successful_results.items():\n                scores = result['scores']\n                ax.hist(scores, bins=30, alpha=0.6, label=algorithm, density=True)\n            \n            ax.set_xlabel('Anomaly Score')\n            ax.set_ylabel('Density')\n            ax.set_title('Score Distributions', fontweight='bold')\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n            \n            plot_idx += 1\n        \n        if plot_idx <= n_rows * n_cols:\n            # Detection rate comparison\n            ax = plt.subplot(n_rows, n_cols, plot_idx)\n            \n            algorithms = list(successful_results.keys())\n            detection_rates = [successful_results[alg]['anomaly_rate'] for alg in algorithms]\n            execution_times = [successful_results[alg].get('execution_time', 0) for alg in algorithms]\n            \n            bars = ax.bar(algorithms, detection_rates, alpha=0.7)\n            ax.set_ylabel('Anomaly Detection Rate')\n            ax.set_title('Detection Rate Comparison', fontweight='bold')\n            ax.tick_params(axis='x', rotation=45)\n            \n            # Add execution time as text\n            for bar, time_val in zip(bars, execution_times):\n                height = bar.get_height()\n                ax.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n                       f'{time_val:.1f}s', ha='center', va='bottom', fontsize=8)\n            \n            ax.axhline(y=self.contamination_rate, color='red', linestyle='--',\n                      label=f'Expected Rate ({self.contamination_rate:.1%})')\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n        \n        plt.suptitle('Tabular Anomaly Detection Results', fontsize=16, fontweight='bold')\n        plt.tight_layout()\n        \n        if save_path:\n            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n            print(f\"📊 Visualization saved to: {save_path}\")\n        \n        plt.show()\n    \n    def generate_business_report(self, X=None, y=None, results=None, output_path=None):\n        \"\"\"Generate a comprehensive business-focused report.\"\"\"\n        if X is None:\n            X, y = self.preprocess_data()\n        if results is None:\n            results = self.results\n        \n        if not results:\n            print(\"❌ No results to report. Run detect_anomalies() first.\")\n            return\n        \n        report = []\n        report.append(\"# Tabular Data Anomaly Detection Report\\n\")\n        report.append(f\"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n        \n        # Executive Summary\n        report.append(\"## Executive Summary\\n\")\n        \n        successful_algorithms = [name for name, result in results.items() if 'error' not in result]\n        total_data_points = len(X)\n        \n        report.append(f\"- **Dataset Size**: {total_data_points:,} records with {X.shape[1]} features\\n\")\n        report.append(f\"- **Algorithms Tested**: {len(successful_algorithms)}\\n\")\n        \n        if successful_algorithms:\n            # Get ensemble or best algorithm results\n            if 'Ensemble' in results:\n                primary_result = results['Ensemble']\n                primary_method = 'Ensemble'\n            else:\n                # Use the first successful algorithm\n                primary_method = successful_algorithms[0]\n                primary_result = results[primary_method]\n            \n            anomaly_count = primary_result['anomaly_count']\n            anomaly_rate = primary_result['anomaly_rate']\n            \n            report.append(f\"- **Primary Method**: {primary_method}\\n\")\n            report.append(f\"- **Anomalies Detected**: {anomaly_count:,} ({anomaly_rate:.1%})\\n\")\n            \n            # Risk assessment\n            if anomaly_rate > 0.1:\n                risk_level = \"HIGH\"\n                risk_color = \"🔴\"\n            elif anomaly_rate > 0.05:\n                risk_level = \"MEDIUM\"\n                risk_color = \"🟡\"\n            else:\n                risk_level = \"LOW\"\n                risk_color = \"🟢\"\n            \n            report.append(f\"- **Risk Level**: {risk_color} {risk_level}\\n\")\n        \n        # Data Quality Summary\n        if self.data_profile:\n            report.append(\"## Data Quality Assessment\\n\")\n            \n            quality = self.data_profile['data_quality']\n            report.append(f\"- **Missing Values**: {sum(quality['missing_values'].values())} ({quality['missing_percentages']})\\n\")\n            report.append(f\"- **Duplicate Records**: {quality['duplicate_rows']} ({quality['duplicate_percentage']:.1f}%)\\n\")\n            \n            if quality['high_missing_columns']:\n                report.append(f\"- **High Missing Columns**: {len(quality['high_missing_columns'])} columns with >50% missing\\n\")\n        \n        # Algorithm Performance\n        report.append(\"## Detection Results by Algorithm\\n\")\n        report.append(\"| Algorithm | Anomalies | Rate | Execution Time |\\n\")\n        report.append(\"|-----------|-----------|------|----------------|\\n\")\n        \n        for algorithm, result in results.items():\n            if 'error' not in result:\n                count = result['anomaly_count']\n                rate = result['anomaly_rate']\n                time_val = result.get('execution_time', 0)\n                report.append(f\"| {algorithm} | {count:,} | {rate:.1%} | {time_val:.2f}s |\\n\")\n        \n        # Feature Analysis\n        if self.data_profile and 'feature_analysis' in self.data_profile:\n            feature_analysis = self.data_profile['feature_analysis']\n            \n            report.append(\"## Feature Analysis\\n\")\n            report.append(f\"- **Numeric Features**: {len(feature_analysis['numeric_features'])}\\n\")\n            report.append(f\"- **Categorical Features**: {len(feature_analysis['categorical_features'])}\\n\")\n            report.append(f\"- **Constant Features Removed**: {len(feature_analysis['constant_features'])}\\n\")\n            \n            if feature_analysis['high_cardinality_features']:\n                report.append(f\"- **High Cardinality Features**: {len(feature_analysis['high_cardinality_features'])}\\n\")\n        \n        # Business Recommendations\n        report.append(\"## Business Recommendations\\n\")\n        \n        if successful_algorithms:\n            if anomaly_rate > 0.1:\n                report.append(\"- 🚨 **HIGH PRIORITY**: Anomaly rate exceeds 10%. Immediate investigation recommended.\\n\")\n                report.append(\"- 🔍 **Action**: Review high-scoring anomalies for business impact.\\n\")\n                report.append(\"- ⚙️ **Consider**: Adjusting detection threshold or investigating data quality issues.\\n\")\n            \n            elif anomaly_rate < 0.01:\n                report.append(\"- 🔍 **SENSITIVITY**: Very low anomaly rate detected (<1%).\\n\")\n                report.append(\"- ⚙️ **Consider**: Lowering detection threshold for more sensitive monitoring.\\n\")\n                report.append(\"- 📊 **Review**: Ensure current settings align with business expectations.\\n\")\n            \n            else:\n                report.append(\"- ✅ **OPTIMAL**: Anomaly rate within expected business range.\\n\")\n                report.append(\"- 📊 **Monitor**: Continue regular monitoring and review cycles.\\n\")\n            \n            # Algorithm-specific recommendations\n            if len(successful_algorithms) > 1:\n                report.append(\"- 🤝 **ENSEMBLE**: Multiple algorithms available - consider ensemble approach for robustness.\\n\")\n            \n            # Performance recommendations\n            fastest_time = min([results[alg].get('execution_time', float('inf')) \n                              for alg in successful_algorithms])\n            if fastest_time > 60:  # More than 1 minute\n                report.append(\"- ⚡ **PERFORMANCE**: Consider algorithm optimization for faster processing.\\n\")\n        \n        # Data Quality Recommendations\n        if self.data_profile:\n            quality = self.data_profile['data_quality']\n            if quality['duplicate_percentage'] > 5:\n                report.append(\"- 🧹 **DATA QUALITY**: High duplicate rate detected. Consider data deduplication.\\n\")\n            \n            if len(quality['high_missing_columns']) > 0:\n                report.append(\"- ❓ **MISSING DATA**: Columns with high missing rates may need data collection improvement.\\n\")\n        \n        # Next Steps\n        report.append(\"## Recommended Next Steps\\n\")\n        report.append(\"1. 🔍 **Investigate High-Scoring Anomalies**: Review business context of top anomalies\\n\")\n        report.append(\"2. 📊 **Validate Results**: Work with domain experts to confirm anomaly relevance\\n\")\n        report.append(\"3. ⚙️ **Optimize Parameters**: Fine-tune detection thresholds based on business feedback\\n\")\n        report.append(\"4. 📈 **Monitor Trends**: Establish regular monitoring schedule for ongoing detection\\n\")\n        report.append(\"5. 🔄 **Model Updates**: Plan periodic model retraining with new data\\n\")\n        \n        # Technical Appendix\n        report.append(\"## Technical Details\\n\")\n        report.append(f\"- **Contamination Rate**: {self.contamination_rate}\\n\")\n        report.append(f\"- **Random Seed**: {self.random_state}\\n\")\n        report.append(f\"- **Feature Selection**: {'Enabled' if self.feature_selection else 'Disabled'}\\n\")\n        report.append(f\"- **Ensemble Method**: {self.ensemble_method}\\n\")\n        \n        report_text = ''.join(report)\n        \n        if output_path:\n            with open(output_path, 'w') as f:\n                f.write(report_text)\n            print(f\"📄 Business report saved to: {output_path}\")\n        \n        return report_text\n\n\n# Example usage and testing\nif __name__ == \"__main__\":\n    # Create sample tabular data with anomalies\n    def generate_sample_data(n_samples=1000, n_features=10, contamination=0.05):\n        \"\"\"Generate sample tabular data with embedded anomalies.\"\"\"\n        np.random.seed(42)\n        \n        # Generate normal data\n        X_normal = np.random.multivariate_normal(\n            mean=np.zeros(n_features),\n            cov=np.eye(n_features),\n            size=int(n_samples * (1 - contamination))\n        )\n        \n        # Generate anomalous data\n        n_anomalies = int(n_samples * contamination)\n        X_anomalies = np.random.multivariate_normal(\n            mean=np.ones(n_features) * 3,  # Shifted mean\n            cov=np.eye(n_features) * 2,    # Different covariance\n            size=n_anomalies\n        )\n        \n        # Combine data\n        X = np.vstack([X_normal, X_anomalies])\n        y = np.hstack([np.zeros(len(X_normal)), np.ones(n_anomalies)])\n        \n        # Add some categorical features\n        categories = ['A', 'B', 'C', 'D']\n        cat_feature = np.random.choice(categories, size=n_samples)\n        \n        # Create DataFrame\n        feature_names = [f'numeric_{i}' for i in range(n_features)]\n        df = pd.DataFrame(X, columns=feature_names)\n        df['category'] = cat_feature\n        df['id'] = range(n_samples)\n        df['target'] = y\n        \n        # Add some missing values\n        missing_indices = np.random.choice(n_samples, size=int(n_samples * 0.02), replace=False)\n        missing_columns = np.random.choice(feature_names, size=len(missing_indices))\n        for idx, col in zip(missing_indices, missing_columns):\n            df.loc[idx, col] = np.nan\n        \n        return df\n    \n    print(\"🧪 Running Tabular Anomaly Detection Example\")\n    print(\"=\" * 50)\n    \n    # Generate sample data\n    print(\"\\n📊 Generating sample tabular data...\")\n    sample_data = generate_sample_data(n_samples=2000, n_features=8, contamination=0.03)\n    print(f\"Generated dataset: {sample_data.shape}\")\n    print(f\"True anomalies: {sample_data['target'].sum()} ({sample_data['target'].mean():.1%})\")\n    \n    # Initialize detector\n    detector = TabularAnomalyDetector(\n        contamination_rate=0.05,\n        algorithms=['IsolationForest', 'LocalOutlierFactor', 'OneClassSVM'],\n        feature_selection=True,\n        ensemble_method='voting',\n        random_state=42\n    )\n    \n    # Load and profile data\n    print(\"\\n🔄 Loading and profiling data...\")\n    df = detector.load_data(sample_data, target_column='target', id_columns=['id'])\n    profile = detector.profile_data()\n    \n    # Preprocess data\n    print(\"\\n🔧 Preprocessing data...\")\n    X, y = detector.preprocess_data()\n    print(f\"Preprocessed data shape: {X.shape}\")\n    \n    # Detect anomalies\n    print(\"\\n🔍 Detecting anomalies...\")\n    results = detector.detect_anomalies(X, y)\n    \n    # Evaluate performance\n    print(\"\\n📊 Evaluating performance...\")\n    performance = detector.evaluate_performance(sample_data['target'].values)\n    \n    # Create visualizations\n    print(\"\\n📊 Creating visualizations...\")\n    detector.visualize_results(X, results)\n    \n    # Generate business report\n    print(\"\\n📄 Generating business report...\")\n    report = detector.generate_business_report(X, y, results)\n    print(\"\\nReport Preview:\")\n    print(report[:800] + \"...\")\n    \n    print(\"\\n✅ Tabular anomaly detection example completed!\")"
