{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm Comparison Tutorial\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/organization/anomaly-detection/blob/main/docs/notebooks/02_algorithm_comparison_tutorial.ipynb)\n",
    "\n",
    "**Objective**: Compare different anomaly detection algorithms side-by-side with interactive visualizations and parameter tuning.\n",
    "\n",
    "**Duration**: 45 minutes  \n",
    "**Level**: Intermediate  \n",
    "**Prerequisites**: Basic Python and ML knowledge\n",
    "\n",
    "## üìã What You'll Learn\n",
    "\n",
    "- Compare Isolation Forest, LOF, and One-Class SVM algorithms\n",
    "- Understand algorithm strengths and weaknesses\n",
    "- Interactive parameter tuning with real-time feedback\n",
    "- Performance metrics and visualization\n",
    "- How to choose the right algorithm for your data\n",
    "\n",
    "## üöÄ Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if running in Colab)\n",
    "# !pip install anomaly-detection plotly ipywidgets scikit-learn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üéâ All packages imported successfully!\")\n",
    "print(\"üìä Ready to compare anomaly detection algorithms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Mock Implementation for Tutorial\n",
    "\n",
    "Since we're demonstrating the concepts, we'll use a mock implementation that shows how the real package would work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import time\n",
    "\n",
    "class MockDetectionService:\n",
    "    \"\"\"Mock implementation of the DetectionService for educational purposes.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.algorithms = {\n",
    "            'isolation_forest': IsolationForest,\n",
    "            'lof': LocalOutlierFactor,\n",
    "            'one_class_svm': OneClassSVM\n",
    "        }\n",
    "    \n",
    "    def detect(self, data, algorithm='isolation_forest', **kwargs):\n",
    "        \"\"\"Detect anomalies using the specified algorithm.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if algorithm == 'isolation_forest':\n",
    "            model = IsolationForest(\n",
    "                contamination=kwargs.get('contamination', 0.1),\n",
    "                n_estimators=kwargs.get('n_estimators', 100),\n",
    "                random_state=kwargs.get('random_state', 42)\n",
    "            )\n",
    "            predictions = model.fit_predict(data)\n",
    "            scores = model.score_samples(data)\n",
    "            \n",
    "        elif algorithm == 'lof':\n",
    "            model = LocalOutlierFactor(\n",
    "                n_neighbors=kwargs.get('n_neighbors', 20),\n",
    "                contamination=kwargs.get('contamination', 0.1)\n",
    "            )\n",
    "            predictions = model.fit_predict(data)\n",
    "            scores = model.negative_outlier_factor_\n",
    "            \n",
    "        elif algorithm == 'one_class_svm':\n",
    "            model = OneClassSVM(\n",
    "                kernel=kwargs.get('kernel', 'rbf'),\n",
    "                gamma=kwargs.get('gamma', 'scale'),\n",
    "                nu=kwargs.get('nu', 0.1)\n",
    "            )\n",
    "            predictions = model.fit_predict(data)\n",
    "            scores = model.score_samples(data)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        return MockResult(predictions, scores, len(data), processing_time, algorithm)\n",
    "\n",
    "class MockResult:\n",
    "    \"\"\"Mock result class that mimics the real DetectionResult.\"\"\"\n",
    "    \n",
    "    def __init__(self, predictions, scores, total_samples, processing_time, algorithm):\n",
    "        self.predictions = predictions\n",
    "        self.scores = scores\n",
    "        self.total_samples = total_samples\n",
    "        self.processing_time = processing_time\n",
    "        self.algorithm = algorithm\n",
    "        self.anomaly_count = np.sum(predictions == -1)\n",
    "        self.anomaly_rate = self.anomaly_count / total_samples\n",
    "\n",
    "# Initialize the mock service\n",
    "detection_service = MockDetectionService()\n",
    "print(\"‚úÖ Mock DetectionService initialized\")\n",
    "print(\"üìù Available algorithms:\", list(detection_service.algorithms.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≤ Generate Synthetic Dataset\n",
    "\n",
    "Let's create a synthetic dataset with known anomalies to test our algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples=1000, n_features=2, contamination=0.1, random_state=42):\n",
    "    \"\"\"Generate synthetic data with known anomalies.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # Normal data (multivariate normal distribution)\n",
    "    n_normal = int(n_samples * (1 - contamination))\n",
    "    n_anomalies = n_samples - n_normal\n",
    "    \n",
    "    # Create correlation matrix\n",
    "    if n_features == 2:\n",
    "        cov_matrix = [[1, 0.3], [0.3, 1]]\n",
    "    else:\n",
    "        cov_matrix = np.eye(n_features)\n",
    "        # Add some correlation\n",
    "        for i in range(n_features-1):\n",
    "            cov_matrix[i, i+1] = 0.3\n",
    "            cov_matrix[i+1, i] = 0.3\n",
    "    \n",
    "    # Normal samples\n",
    "    normal_data = np.random.multivariate_normal(\n",
    "        mean=np.zeros(n_features), \n",
    "        cov=cov_matrix, \n",
    "        size=n_normal\n",
    "    )\n",
    "    \n",
    "    # Anomalous samples (different distribution)\n",
    "    anomaly_data = np.random.multivariate_normal(\n",
    "        mean=np.ones(n_features) * 4, \n",
    "        cov=np.eye(n_features) * 0.5, \n",
    "        size=n_anomalies\n",
    "    )\n",
    "    \n",
    "    # Combine data\n",
    "    X = np.vstack([normal_data, anomaly_data])\n",
    "    y_true = np.hstack([np.ones(n_normal), -np.ones(n_anomalies)])\n",
    "    \n",
    "    # Shuffle the data\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    X = X[indices]\n",
    "    y_true = y_true[indices]\n",
    "    \n",
    "    return X, y_true\n",
    "\n",
    "# Generate the dataset\n",
    "X, y_true = generate_synthetic_data(n_samples=1000, n_features=2, contamination=0.1)\n",
    "\n",
    "print(f\"üìä Generated dataset:\")\n",
    "print(f\"   Samples: {X.shape[0]}\")\n",
    "print(f\"   Features: {X.shape[1]}\")\n",
    "print(f\"   Normal samples: {np.sum(y_true == 1)}\")\n",
    "print(f\"   Anomalous samples: {np.sum(y_true == -1)}\")\n",
    "print(f\"   Contamination rate: {np.sum(y_true == -1) / len(y_true):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualize the Dataset\n",
    "\n",
    "Let's visualize our synthetic dataset to understand the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dataset(X, y_true, title=\"Synthetic Dataset\"):\n",
    "    \"\"\"Plot the 2D dataset with true labels.\"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Normal points\n",
    "    normal_mask = y_true == 1\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X[normal_mask, 0],\n",
    "        y=X[normal_mask, 1],\n",
    "        mode='markers',\n",
    "        name='Normal',\n",
    "        marker=dict(color='blue', size=6, opacity=0.7),\n",
    "        hovertemplate='Normal<br>X: %{x:.2f}<br>Y: %{y:.2f}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    # Anomalous points\n",
    "    anomaly_mask = y_true == -1\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X[anomaly_mask, 0],\n",
    "        y=X[anomaly_mask, 1],\n",
    "        mode='markers',\n",
    "        name='Anomaly (True)',\n",
    "        marker=dict(color='red', size=8, opacity=0.9, symbol='x'),\n",
    "        hovertemplate='Anomaly<br>X: %{x:.2f}<br>Y: %{y:.2f}<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title='Feature 1',\n",
    "        yaxis_title='Feature 2',\n",
    "        showlegend=True,\n",
    "        width=600,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Plot the dataset\n",
    "fig = plot_dataset(X, y_true, \"Ground Truth: Normal vs Anomalous Points\")\n",
    "fig.show()\n",
    "\n",
    "print(\"üí° The red X marks show the true anomalies we want to detect!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Algorithm Comparison Function\n",
    "\n",
    "Let's create a comprehensive function to compare algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_algorithms(X, y_true, algorithms_config):\n",
    "    \"\"\"Compare multiple algorithms and return results.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for algo_name, config in algorithms_config.items():\n",
    "        print(f\"üîç Testing {algo_name}...\")\n",
    "        \n",
    "        # Detect anomalies\n",
    "        result = detection_service.detect(X, algorithm=config['algorithm'], **config['params'])\n",
    "        \n",
    "        # Calculate metrics\n",
    "        y_pred = result.predictions\n",
    "        \n",
    "        # Convert to binary classification metrics (1 = normal, 0 = anomaly)\n",
    "        y_true_binary = (y_true == 1).astype(int)\n",
    "        y_pred_binary = (y_pred == 1).astype(int)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_binary, y_pred_binary).ravel()\n",
    "        \n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        \n",
    "        # For anomaly detection, we often care more about detecting anomalies\n",
    "        anomaly_precision = tn / (tn + fn) if (tn + fn) > 0 else 0  # True anomalies detected\n",
    "        anomaly_recall = tn / (tn + fp) if (tn + fp) > 0 else 0     # Precision of anomaly detection\n",
    "        \n",
    "        results[algo_name] = {\n",
    "            'result': result,\n",
    "            'predictions': y_pred,\n",
    "            'scores': result.scores,\n",
    "            'processing_time': result.processing_time,\n",
    "            'anomaly_count': result.anomaly_count,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'anomaly_precision': anomaly_precision,\n",
    "            'anomaly_recall': anomaly_recall,\n",
    "            'true_positives': tp,\n",
    "            'false_positives': fp,\n",
    "            'true_negatives': tn,\n",
    "            'false_negatives': fn\n",
    "        }\n",
    "        \n",
    "        print(f\"   ‚úÖ Completed in {result.processing_time:.3f}s\")\n",
    "        print(f\"   üìä Detected {result.anomaly_count} anomalies ({result.anomaly_rate:.1%})\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Algorithm comparison function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Run Algorithm Comparison\n",
    "\n",
    "Now let's compare three popular algorithms with default parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define algorithms and their configurations\n",
    "algorithms_config = {\n",
    "    'Isolation Forest': {\n",
    "        'algorithm': 'isolation_forest',\n",
    "        'params': {\n",
    "            'contamination': 0.1,\n",
    "            'n_estimators': 100,\n",
    "            'random_state': 42\n",
    "        }\n",
    "    },\n",
    "    'Local Outlier Factor': {\n",
    "        'algorithm': 'lof',\n",
    "        'params': {\n",
    "            'n_neighbors': 20,\n",
    "            'contamination': 0.1\n",
    "        }\n",
    "    },\n",
    "    'One-Class SVM': {\n",
    "        'algorithm': 'one_class_svm',\n",
    "        'params': {\n",
    "            'kernel': 'rbf',\n",
    "            'gamma': 'scale',\n",
    "            'nu': 0.1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run comparison\n",
    "print(\"üöÄ Starting algorithm comparison...\\n\")\n",
    "comparison_results = compare_algorithms(X, y_true, algorithms_config)\n",
    "print(\"\\n‚úÖ Algorithm comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Results Visualization\n",
    "\n",
    "Let's create comprehensive visualizations to compare the algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_dashboard(X, y_true, results):\n",
    "    \"\"\"Create a comprehensive comparison dashboard.\"\"\"\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('Algorithm Predictions', 'Performance Metrics', \n",
    "                       'Processing Time', 'Anomaly Score Distributions'),\n",
    "        specs=[[{\"type\": \"scatter\"}, {\"type\": \"bar\"}],\n",
    "               [{\"type\": \"bar\"}, {\"type\": \"histogram\"}]]\n",
    "    )\n",
    "    \n",
    "    # Color palette for algorithms\n",
    "    colors = ['red', 'green', 'purple']\n",
    "    \n",
    "    # 1. Algorithm Predictions (scatter plot)\n",
    "    for i, (algo_name, result) in enumerate(results.items()):\n",
    "        y_pred = result['predictions']\n",
    "        \n",
    "        # Predicted anomalies\n",
    "        anomaly_mask = y_pred == -1\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X[anomaly_mask, 0],\n",
    "                y=X[anomaly_mask, 1],\n",
    "                mode='markers',\n",
    "                name=f'{algo_name}',\n",
    "                marker=dict(color=colors[i], size=8, opacity=0.7, symbol='circle-open'),\n",
    "                legendgroup=algo_name,\n",
    "                hovertemplate=f'{algo_name}<br>X: %{{x:.2f}}<br>Y: %{{y:.2f}}<extra></extra>'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Add true anomalies\n",
    "    true_anomaly_mask = y_true == -1\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X[true_anomaly_mask, 0],\n",
    "            y=X[true_anomaly_mask, 1],\n",
    "            mode='markers',\n",
    "            name='True Anomalies',\n",
    "            marker=dict(color='black', size=6, symbol='x'),\n",
    "            hovertemplate='True Anomaly<br>X: %{x:.2f}<br>Y: %{y:.2f}<extra></extra>'\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # 2. Performance Metrics (bar chart)\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "    for i, (algo_name, result) in enumerate(results.items()):\n",
    "        metric_values = [result[metric] for metric in metrics]\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=metrics,\n",
    "                y=metric_values,\n",
    "                name=algo_name,\n",
    "                marker_color=colors[i],\n",
    "                legendgroup=algo_name,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # 3. Processing Time (bar chart)\n",
    "    algo_names = list(results.keys())\n",
    "    processing_times = [results[name]['processing_time'] for name in algo_names]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(\n",
    "            x=algo_names,\n",
    "            y=processing_times,\n",
    "            marker_color=colors,\n",
    "            name='Processing Time',\n",
    "            showlegend=False,\n",
    "            hovertemplate='%{x}<br>Time: %{y:.3f}s<extra></extra>'\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # 4. Anomaly Score Distributions (histograms)\n",
    "    for i, (algo_name, result) in enumerate(results.items()):\n",
    "        scores = result['scores']\n",
    "        fig.add_trace(\n",
    "            go.Histogram(\n",
    "                x=scores,\n",
    "                name=f'{algo_name} Scores',\n",
    "                marker_color=colors[i],\n",
    "                opacity=0.7,\n",
    "                legendgroup=algo_name,\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=\"Anomaly Detection Algorithm Comparison Dashboard\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update axes labels\n",
    "    fig.update_xaxes(title_text=\"Feature 1\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Feature 2\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Score\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Time (seconds)\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Anomaly Score\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Count\", row=2, col=2)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create and display the dashboard\n",
    "dashboard_fig = create_comparison_dashboard(X, y_true, comparison_results)\n",
    "dashboard_fig.show()\n",
    "\n",
    "print(\"üìä Comparison dashboard displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Performance Summary Table\n",
    "\n",
    "Let's create a detailed performance comparison table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_performance_table(results):\n",
    "    \"\"\"Create a comprehensive performance comparison table.\"\"\"\n",
    "    \n",
    "    performance_data = []\n",
    "    \n",
    "    for algo_name, result in results.items():\n",
    "        performance_data.append({\n",
    "            'Algorithm': algo_name,\n",
    "            'Accuracy': f\"{result['accuracy']:.3f}\",\n",
    "            'Precision': f\"{result['precision']:.3f}\",\n",
    "            'Recall': f\"{result['recall']:.3f}\",\n",
    "            'F1-Score': f\"{result['f1_score']:.3f}\",\n",
    "            'Anomalies Detected': result['anomaly_count'],\n",
    "            'Processing Time (s)': f\"{result['processing_time']:.3f}\",\n",
    "            'True Positives': result['true_positives'],\n",
    "            'False Positives': result['false_positives'],\n",
    "            'True Negatives': result['true_negatives'],\n",
    "            'False Negatives': result['false_negatives']\n",
    "        })\n",
    "    \n",
    "    df_performance = pd.DataFrame(performance_data)\n",
    "    \n",
    "    return df_performance\n",
    "\n",
    "# Create and display the performance table\n",
    "performance_df = create_performance_table(comparison_results)\n",
    "\n",
    "print(\"üìä Algorithm Performance Comparison\")\n",
    "print(\"=\" * 80)\n",
    "display(performance_df)\n",
    "\n",
    "# Find the best algorithm for each metric\n",
    "print(\"\\nüèÜ Best Performing Algorithm by Metric:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "numeric_cols = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "for col in numeric_cols:\n",
    "    performance_df[col] = performance_df[col].astype(float)\n",
    "    best_idx = performance_df[col].idxmax()\n",
    "    best_algo = performance_df.loc[best_idx, 'Algorithm']\n",
    "    best_score = performance_df.loc[best_idx, col]\n",
    "    print(f\"üìà {col}: {best_algo} ({best_score:.3f})\")\n",
    "\n",
    "# Find fastest algorithm\n",
    "performance_df['Processing Time (s)'] = performance_df['Processing Time (s)'].astype(float)\n",
    "fastest_idx = performance_df['Processing Time (s)'].idxmin()\n",
    "fastest_algo = performance_df.loc[fastest_idx, 'Algorithm']\n",
    "fastest_time = performance_df.loc[fastest_idx, 'Processing Time (s)']\n",
    "print(f\"‚ö° Fastest: {fastest_algo} ({fastest_time:.3f}s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Interactive Parameter Tuning\n",
    "\n",
    "Now let's create interactive widgets to tune algorithm parameters in real-time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interactive_tuning():\n",
    "    \"\"\"Create interactive parameter tuning widgets.\"\"\"\n",
    "    \n",
    "    # Output widget to display results\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    # Algorithm selection\n",
    "    algorithm_dropdown = widgets.Dropdown(\n",
    "        options=['isolation_forest', 'lof', 'one_class_svm'],\n",
    "        value='isolation_forest',\n",
    "        description='Algorithm:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Common parameters\n",
    "    contamination_slider = widgets.FloatSlider(\n",
    "        value=0.1,\n",
    "        min=0.01,\n",
    "        max=0.5,\n",
    "        step=0.01,\n",
    "        description='Contamination:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # Isolation Forest specific\n",
    "    n_estimators_slider = widgets.IntSlider(\n",
    "        value=100,\n",
    "        min=10,\n",
    "        max=300,\n",
    "        step=10,\n",
    "        description='N Estimators:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # LOF specific\n",
    "    n_neighbors_slider = widgets.IntSlider(\n",
    "        value=20,\n",
    "        min=5,\n",
    "        max=100,\n",
    "        step=5,\n",
    "        description='N Neighbors:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    # One-Class SVM specific\n",
    "    kernel_dropdown = widgets.Dropdown(\n",
    "        options=['rbf', 'linear', 'poly'],\n",
    "        value='rbf',\n",
    "        description='Kernel:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    nu_slider = widgets.FloatSlider(\n",
    "        value=0.1,\n",
    "        min=0.01,\n",
    "        max=0.5,\n",
    "        step=0.01,\n",
    "        description='Nu:',\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "    \n",
    "    def update_visualization(*args):\n",
    "        \"\"\"Update visualization based on parameter changes.\"\"\"\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            \n",
    "            # Get current parameter values\n",
    "            algorithm = algorithm_dropdown.value\n",
    "            contamination = contamination_slider.value\n",
    "            \n",
    "            # Prepare parameters based on algorithm\n",
    "            if algorithm == 'isolation_forest':\n",
    "                params = {\n",
    "                    'contamination': contamination,\n",
    "                    'n_estimators': n_estimators_slider.value,\n",
    "                    'random_state': 42\n",
    "                }\n",
    "            elif algorithm == 'lof':\n",
    "                params = {\n",
    "                    'contamination': contamination,\n",
    "                    'n_neighbors': n_neighbors_slider.value\n",
    "                }\n",
    "            else:  # one_class_svm\n",
    "                params = {\n",
    "                    'kernel': kernel_dropdown.value,\n",
    "                    'nu': nu_slider.value\n",
    "                }\n",
    "            \n",
    "            # Run detection\n",
    "            result = detection_service.detect(X, algorithm=algorithm, **params)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            y_pred = result.predictions\n",
    "            y_true_binary = (y_true == 1).astype(int)\n",
    "            y_pred_binary = (y_pred == 1).astype(int)\n",
    "            \n",
    "            tn, fp, fn, tp = confusion_matrix(y_true_binary, y_pred_binary).ravel()\n",
    "            accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "            \n",
    "            # Create visualization\n",
    "            fig = go.Figure()\n",
    "            \n",
    "            # Normal points\n",
    "            normal_mask = y_pred == 1\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=X[normal_mask, 0],\n",
    "                y=X[normal_mask, 1],\n",
    "                mode='markers',\n",
    "                name='Predicted Normal',\n",
    "                marker=dict(color='lightblue', size=6, opacity=0.7)\n",
    "            ))\n",
    "            \n",
    "            # Predicted anomalies\n",
    "            anomaly_mask = y_pred == -1\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=X[anomaly_mask, 0],\n",
    "                y=X[anomaly_mask, 1],\n",
    "                mode='markers',\n",
    "                name='Predicted Anomaly',\n",
    "                marker=dict(color='orange', size=8, opacity=0.8, symbol='circle-open')\n",
    "            ))\n",
    "            \n",
    "            # True anomalies\n",
    "            true_anomaly_mask = y_true == -1\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=X[true_anomaly_mask, 0],\n",
    "                y=X[true_anomaly_mask, 1],\n",
    "                mode='markers',\n",
    "                name='True Anomaly',\n",
    "                marker=dict(color='red', size=6, symbol='x')\n",
    "            ))\n",
    "            \n",
    "            fig.update_layout(\n",
    "                title=f'{algorithm.replace(\"_\", \" \").title()} Results',\n",
    "                xaxis_title='Feature 1',\n",
    "                yaxis_title='Feature 2',\n",
    "                width=600,\n",
    "                height=400\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "            \n",
    "            # Display metrics\n",
    "            print(f\"üìä Performance Metrics:\")\n",
    "            print(f\"   Accuracy: {accuracy:.3f}\")\n",
    "            print(f\"   Precision: {precision:.3f}\")\n",
    "            print(f\"   Recall: {recall:.3f}\")\n",
    "            print(f\"   F1-Score: {f1:.3f}\")\n",
    "            print(f\"   Detected Anomalies: {result.anomaly_count}\")\n",
    "            print(f\"   Processing Time: {result.processing_time:.3f}s\")\n",
    "    \n",
    "    # Connect widgets to update function\n",
    "    algorithm_dropdown.observe(update_visualization, names='value')\n",
    "    contamination_slider.observe(update_visualization, names='value')\n",
    "    n_estimators_slider.observe(update_visualization, names='value')\n",
    "    n_neighbors_slider.observe(update_visualization, names='value')\n",
    "    kernel_dropdown.observe(update_visualization, names='value')\n",
    "    nu_slider.observe(update_visualization, names='value')\n",
    "    \n",
    "    # Show/hide algorithm-specific widgets\n",
    "    def update_widget_visibility(*args):\n",
    "        algorithm = algorithm_dropdown.value\n",
    "        \n",
    "        if algorithm == 'isolation_forest':\n",
    "            n_estimators_slider.layout.display = 'block'\n",
    "            n_neighbors_slider.layout.display = 'none'\n",
    "            kernel_dropdown.layout.display = 'none'\n",
    "            nu_slider.layout.display = 'none'\n",
    "        elif algorithm == 'lof':\n",
    "            n_estimators_slider.layout.display = 'none'\n",
    "            n_neighbors_slider.layout.display = 'block'\n",
    "            kernel_dropdown.layout.display = 'none'\n",
    "            nu_slider.layout.display = 'none'\n",
    "        else:  # one_class_svm\n",
    "            n_estimators_slider.layout.display = 'none'\n",
    "            n_neighbors_slider.layout.display = 'none'\n",
    "            kernel_dropdown.layout.display = 'block'\n",
    "            nu_slider.layout.display = 'block'\n",
    "    \n",
    "    algorithm_dropdown.observe(update_widget_visibility, names='value')\n",
    "    update_widget_visibility()  # Initial call\n",
    "    \n",
    "    # Create widget layout\n",
    "    controls = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>üéõÔ∏è Interactive Parameter Tuning</h3>\"),\n",
    "        algorithm_dropdown,\n",
    "        contamination_slider,\n",
    "        n_estimators_slider,\n",
    "        n_neighbors_slider,\n",
    "        kernel_dropdown,\n",
    "        nu_slider\n",
    "    ])\n",
    "    \n",
    "    # Initial visualization\n",
    "    update_visualization()\n",
    "    \n",
    "    return widgets.HBox([controls, output])\n",
    "\n",
    "# Create and display interactive tuning interface\n",
    "print(\"üéõÔ∏è Creating interactive parameter tuning interface...\")\n",
    "interactive_widget = create_interactive_tuning()\n",
    "display(interactive_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Key Insights and Algorithm Selection Guide\n",
    "\n",
    "Based on our comparison, here are the key insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_insights(results):\n",
    "    \"\"\"Generate insights and recommendations based on results.\"\"\"\n",
    "    \n",
    "    print(\"üí° Algorithm Comparison Insights\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Find best algorithm for each metric\n",
    "    best_accuracy = max(results.items(), key=lambda x: x[1]['accuracy'])\n",
    "    best_f1 = max(results.items(), key=lambda x: x[1]['f1_score'])\n",
    "    fastest = min(results.items(), key=lambda x: x[1]['processing_time'])\n",
    "    \n",
    "    print(f\"üéØ Best Overall Performance: {best_accuracy[0]} (Accuracy: {best_accuracy[1]['accuracy']:.3f})\")\n",
    "    print(f\"‚öñÔ∏è Best F1-Score: {best_f1[0]} (F1: {best_f1[1]['f1_score']:.3f})\")\n",
    "    print(f\"‚ö° Fastest Algorithm: {fastest[0]} ({fastest[1]['processing_time']:.3f}s)\")\n",
    "    \n",
    "    print(\"\\nüìã Algorithm Characteristics:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    algorithm_chars = {\n",
    "        'Isolation Forest': {\n",
    "            'strengths': ['Fast training', 'Good for large datasets', 'Handles high dimensions well'],\n",
    "            'weaknesses': ['Less interpretable', 'May struggle with local anomalies'],\n",
    "            'best_for': 'General purpose, large-scale anomaly detection'\n",
    "        },\n",
    "        'Local Outlier Factor': {\n",
    "            'strengths': ['Good for local anomalies', 'Interpretable scores', 'Handles varying densities'],\n",
    "            'weaknesses': ['Slower on large datasets', 'Memory intensive', 'Sensitive to parameters'],\n",
    "            'best_for': 'Local anomaly detection, interpretability needed'\n",
    "        },\n",
    "        'One-Class SVM': {\n",
    "            'strengths': ['Good boundary definition', 'Kernel flexibility', 'Robust to outliers'],\n",
    "            'weaknesses': ['Slow training', 'Hard to tune', 'Not suitable for large datasets'],\n",
    "            'best_for': 'Small to medium datasets, complex boundaries'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for algo_name, chars in algorithm_chars.items():\n",
    "        if algo_name in results:\n",
    "            result = results[algo_name]\n",
    "            print(f\"\\nüîç {algo_name}:\")\n",
    "            print(f\"   ‚úÖ Strengths: {', '.join(chars['strengths'])}\")\n",
    "            print(f\"   ‚ö†Ô∏è Weaknesses: {', '.join(chars['weaknesses'])}\")\n",
    "            print(f\"   üéØ Best for: {chars['best_for']}\")\n",
    "            print(f\"   üìä Our results: Accuracy={result['accuracy']:.3f}, F1={result['f1_score']:.3f}, Time={result['processing_time']:.3f}s\")\n",
    "    \n",
    "    print(\"\\nüéØ Selection Guidelines:\")\n",
    "    print(\"-\" * 25)\n",
    "    print(\"‚Ä¢ For **large datasets (>10K samples)**: Use Isolation Forest\")\n",
    "    print(\"‚Ä¢ For **local anomaly patterns**: Use Local Outlier Factor\")\n",
    "    print(\"‚Ä¢ For **complex non-linear boundaries**: Use One-Class SVM\")\n",
    "    print(\"‚Ä¢ For **maximum accuracy**: Use ensemble methods (combine multiple algorithms)\")\n",
    "    print(\"‚Ä¢ For **real-time detection**: Use Isolation Forest (fastest)\")\n",
    "    print(\"‚Ä¢ For **interpretability**: Use Local Outlier Factor\")\n",
    "\n",
    "# Generate insights\n",
    "generate_insights(comparison_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Next Steps and Advanced Topics\n",
    "\n",
    "Congratulations! You have successfully compared different anomaly detection algorithms. Here are the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéì Learning Path: What's Next?\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "next_steps = [\n",
    "    {\n",
    "        'title': 'üéØ Ensemble Methods',\n",
    "        'description': 'Combine multiple algorithms for better performance',\n",
    "        'notebook': '06_ensemble_methods_deep_dive.ipynb',\n",
    "        'difficulty': 'Advanced',\n",
    "        'time': '55 minutes'\n",
    "    },\n",
    "    {\n",
    "        'title': 'üè¶ Real-world Use Case: Fraud Detection',\n",
    "        'description': 'End-to-end fraud detection pipeline',\n",
    "        'notebook': '03_fraud_detection_end_to_end.ipynb',\n",
    "        'difficulty': 'Intermediate',\n",
    "        'time': '60 minutes'\n",
    "    },\n",
    "    {\n",
    "        'title': 'üì° Real-time Streaming Detection',\n",
    "        'description': 'Detect anomalies in streaming data',\n",
    "        'notebook': '07_real_time_streaming_detection.ipynb',\n",
    "        'difficulty': 'Advanced',\n",
    "        'time': '50 minutes'\n",
    "    },\n",
    "    {\n",
    "        'title': 'üîç Model Explainability',\n",
    "        'description': 'Understand why data points are anomalous',\n",
    "        'notebook': '08_model_explainability_tutorial.ipynb',\n",
    "        'difficulty': 'Intermediate',\n",
    "        'time': '40 minutes'\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"\\n{i}. {step['title']}\")\n",
    "    print(f\"   üìù {step['description']}\")\n",
    "    print(f\"   üìì Notebook: {step['notebook']}\")\n",
    "    print(f\"   üéØ Difficulty: {step['difficulty']}\")\n",
    "    print(f\"   ‚è±Ô∏è Time: {step['time']}\")\n",
    "\n",
    "print(\"\\nüîó Additional Resources:\")\n",
    "print(\"-\" * 25)\n",
    "print(\"‚Ä¢ üìñ Algorithm Documentation: ../algorithms.md\")\n",
    "print(\"‚Ä¢ üéØ Quickstart Templates: ../quickstart.md\")\n",
    "print(\"‚Ä¢ üìä Example Datasets: ../datasets/README.md\")\n",
    "print(\"‚Ä¢ üèóÔ∏è Production Deployment: ../deployment.md\")\n",
    "print(\"‚Ä¢ üîß Troubleshooting Guide: ../troubleshooting.md\")\n",
    "\n",
    "print(\"\\n‚úÖ Tutorial Complete!\")\n",
    "print(\"üéâ You now understand how to compare and select anomaly detection algorithms!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù Summary\n",
    "\n",
    "In this tutorial, you learned:\n",
    "\n",
    "‚úÖ **Algorithm Comparison**: How to systematically compare different anomaly detection algorithms  \n",
    "‚úÖ **Performance Metrics**: Understanding accuracy, precision, recall, and F1-score for anomaly detection  \n",
    "‚úÖ **Interactive Tuning**: Real-time parameter adjustment with immediate visual feedback  \n",
    "‚úÖ **Algorithm Selection**: Guidelines for choosing the right algorithm for your use case  \n",
    "‚úÖ **Visualization**: Creating comprehensive dashboards to analyze results  \n",
    "\n",
    "### üéØ Key Takeaways\n",
    "\n",
    "1. **No single algorithm is best for all cases** - algorithm choice depends on your data characteristics\n",
    "2. **Parameter tuning is crucial** - default parameters may not be optimal for your specific dataset\n",
    "3. **Visualization helps understanding** - always plot your results to gain insights\n",
    "4. **Consider trade-offs** - balance between accuracy, speed, and interpretability\n",
    "5. **Ensemble methods** often provide the best overall performance\n",
    "\n",
    "### üîó Related Notebooks\n",
    "\n",
    "- **[Ensemble Methods Deep Dive](06_ensemble_methods_deep_dive.ipynb)** - Learn to combine algorithms\n",
    "- **[Fraud Detection End-to-End](03_fraud_detection_end_to_end.ipynb)** - Apply these concepts to real-world fraud detection\n",
    "- **[Model Explainability](08_model_explainability_tutorial.ipynb)** - Understand model decisions\n",
    "\n",
    "**Happy anomaly detecting!** üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}