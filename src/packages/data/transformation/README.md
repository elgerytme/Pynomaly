# Pynomaly Data Transformation Package

A comprehensive, well-architected data transformation and feature engineering package designed for data processing workflows. Built with Clean Architecture principles and modern Python best practices.

## Features

### üîÑ Data Integration
- Multi-source data connectors (CSV, JSON, Parquet, databases, APIs)
- Streaming data processing with Kafka and Apache Beam
- Cloud storage integration (AWS S3, Google Cloud, Azure)
- Memory-efficient processing with Dask and Polars

### üßπ Data Cleaning
- Automated missing value detection and imputation
- Outlier detection and treatment
- Duplicate record identification and removal
- Data validation and quality checks
- Schema inference and validation

### üìä Data Preparation
- Feature scaling and normalization
- Categorical encoding (one-hot, target, ordinal)
- Data type conversions and optimization
- Train/validation/test splitting strategies
- Data balancing techniques

### üõ†Ô∏è Feature Engineering
- Automated feature generation with Featuretools
- Time series feature extraction with tsfresh
- Domain-specific transformations
- Feature selection algorithms
- Polynomial and interaction features

### ‚ö° Performance & Scalability
- GPU acceleration with RAPIDS cuDF
- Distributed processing with Dask
- Memory optimization techniques
- Lazy evaluation with Polars
- Streaming processing capabilities

## Architecture

The package follows Clean Architecture principles:

```
data_transformation/
‚îú‚îÄ‚îÄ domain/              # Core business logic (entities, value objects, services)
‚îú‚îÄ‚îÄ application/         # Use cases and DTOs
‚îú‚îÄ‚îÄ infrastructure/      # External adapters and implementations
‚îî‚îÄ‚îÄ tests/              # Comprehensive test suite
```

## Quick Start

```python
from data_transformation import DataPipelineUseCase, PipelineConfig

# Configure transformation pipeline
config = PipelineConfig(
    source_type="csv",
    cleaning_strategy="auto",
    feature_engineering=True,
    scaling_method="standard"
)

# Create and execute pipeline
pipeline = DataPipelineUseCase(config)
result = pipeline.execute("data.csv")

print(f"Processed {result.records_processed} records")
print(f"Generated {result.features_created} features")
```

## Installation

```bash
# Basic installation
pip install pynomaly-data-transformation

# With enhanced features
pip install pynomaly-data-transformation[enhanced]

# With all features
pip install pynomaly-data-transformation[all]
```

## Configuration

The package supports flexible configuration through:
- YAML/JSON configuration files
- Environment variables
- Programmatic configuration
- Auto-discovery and recommendation

## Performance

Benchmarked processing capabilities:
- **CSV Files**: 1M+ records/second with Polars
- **Memory Usage**: 50% reduction with optimized dtypes
- **GPU Acceleration**: 10x speedup for numerical operations
- **Streaming**: Real-time processing with <100ms latency

## Integration

Seamlessly integrates with:
- Scikit-learn pipelines
- MLflow and experiment tracking
- Apache Airflow workflows
- Jupyter notebooks
- Pynomaly anomaly detection algorithms

## Contributing

This package follows the repository's development standards:
- Type hints and strict mypy checking
- Comprehensive test coverage (>90%)
- Code quality with ruff and pre-commit hooks
- Clean Architecture and Domain-Driven Design principles

## License

MIT License - see LICENSE file for details.