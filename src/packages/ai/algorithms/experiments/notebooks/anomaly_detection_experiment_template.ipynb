{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Experiment Template\n",
    "\n",
    "**Purpose**: Comprehensive template for anomaly detection experiments using Pynomaly\n",
    "\n",
    "**Version**: 1.0.0\n",
    "\n",
    "**Created**: 2025-06-24\n",
    "\n",
    "**Author**: [Your Name]\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Experiment Overview\n",
    "\n",
    "This notebook provides a standardized framework for conducting anomaly detection experiments. It includes:\n",
    "\n",
    "- üìä **Data exploration and quality assessment**\n",
    "- üîß **Data preprocessing and feature engineering**\n",
    "- ü§ñ **Multiple algorithm comparison**\n",
    "- üìà **Performance evaluation and statistical testing**\n",
    "- üìä **Comprehensive visualization and reporting**\n",
    "- üîÑ **Reproducible experiment workflow**\n",
    "\n",
    "### üéØ Experiment Objectives\n",
    "\n",
    "1. [Define your primary objective here]\n",
    "2. [Define your secondary objective here]\n",
    "3. [Define success criteria here]\n",
    "\n",
    "### üìù Experiment Metadata\n",
    "\n",
    "- **Dataset**: [Dataset name and description]\n",
    "- **Target**: [What you're trying to detect]\n",
    "- **Algorithms**: [List of algorithms to compare]\n",
    "- **Evaluation**: [Primary evaluation metric]\n",
    "- **Timeline**: [Expected duration]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Scientific computing\n",
    "import os\n",
    "\n",
    "# Pynomaly imports\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), \"src\"))\n",
    "\n",
    "from pynomaly.domain.entities.dataset import Dataset\n",
    "from pynomaly.domain.value_objects.contamination_rate import ContaminationRate\n",
    "from pynomaly.infrastructure.adapters.sklearn_adapter import SklearnAdapter\n",
    "\n",
    "# Visualization setup\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ Setup complete\")\n",
    "print(f\"üìÖ Experiment started: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = \"data/your_dataset.csv\"  # Update this path\n",
    "TARGET_COLUMN = \"is_anomaly\"  # Update if different\n",
    "ID_COLUMNS = [\"id\"]  # Columns to exclude from features\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"‚úÖ Data loaded successfully: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File not found: {DATA_PATH}\")\n",
    "    print(\"Please update DATA_PATH with the correct file path\")\n",
    "    # Create sample data for demonstration\n",
    "    from sklearn.datasets import make_classification\n",
    "\n",
    "    X, y = make_classification(\n",
    "        n_samples=1000,\n",
    "        n_features=10,\n",
    "        n_clusters_per_class=1,\n",
    "        n_redundant=0,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "    df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(10)])\n",
    "    df[TARGET_COLUMN] = y\n",
    "    print(f\"üìä Using sample data: {df.shape}\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nüìä Dataset shape: {df.shape}\")\n",
    "print(f\"üìã Columns: {list(df.columns)}\")\n",
    "print(f\"üéØ Target column: {TARGET_COLUMN}\")\n",
    "\n",
    "# Check if target exists\n",
    "if TARGET_COLUMN in df.columns:\n",
    "    print(\"\\nüéØ Target distribution:\")\n",
    "    print(df[TARGET_COLUMN].value_counts())\n",
    "    print(f\"üìä Anomaly rate: {df[TARGET_COLUMN].mean():.1%}\")\n",
    "else:\n",
    "    print(\n",
    "        f\"‚ö†Ô∏è Target column '{TARGET_COLUMN}' not found - assuming unsupervised learning\"\n",
    "    )\n",
    "    TARGET_COLUMN = None\n",
    "\n",
    "# Display first few rows\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df):\n",
    "    \"\"\"Comprehensive data quality assessment.\"\"\"\n",
    "    print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "\n",
    "    # Basic statistics\n",
    "    print(\"\\nüìä Basic Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Total rows: {len(df):,}\")\n",
    "    print(f\"  ‚Ä¢ Total columns: {len(df.columns)}\")\n",
    "    print(f\"  ‚Ä¢ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_summary = pd.DataFrame(\n",
    "        {\"Missing Count\": missing, \"Missing %\": missing_pct}\n",
    "    ).sort_values(\"Missing %\", ascending=False)\n",
    "\n",
    "    print(\"\\n‚ùì Missing Values:\")\n",
    "    if missing.sum() > 0:\n",
    "        print(missing_summary[missing_summary[\"Missing Count\"] > 0])\n",
    "    else:\n",
    "        print(\"  ‚úÖ No missing values found\")\n",
    "\n",
    "    # Duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nüîÑ Duplicate rows: {duplicates} ({duplicates / len(df) * 100:.1f}%)\")\n",
    "\n",
    "    # Data types\n",
    "    print(\"\\nüìã Data Types:\")\n",
    "    dtype_summary = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_summary.items():\n",
    "        print(f\"  ‚Ä¢ {dtype}: {count} columns\")\n",
    "\n",
    "    # Numeric columns analysis\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(\"\\nüìà Numeric Columns Summary:\")\n",
    "        display(df[numeric_cols].describe().round(3))\n",
    "\n",
    "    # Categorical columns analysis\n",
    "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(\"\\nüìù Categorical Columns:\")\n",
    "        for col in categorical_cols:\n",
    "            unique_count = df[col].nunique()\n",
    "            print(f\"  ‚Ä¢ {col}: {unique_count} unique values\")\n",
    "            if unique_count <= 10:\n",
    "                print(f\"    Values: {list(df[col].unique())}\")\n",
    "\n",
    "    return missing_summary, duplicates\n",
    "\n",
    "\n",
    "# Perform assessment\n",
    "missing_summary, duplicates = assess_data_quality(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "if TARGET_COLUMN:\n",
    "    numeric_cols = [col for col in numeric_cols if col != TARGET_COLUMN]\n",
    "\n",
    "if len(numeric_cols) > 1:\n",
    "    # Correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "\n",
    "    # Create heatmap\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(\n",
    "        correlation_matrix,\n",
    "        mask=mask,\n",
    "        annot=True,\n",
    "        cmap=\"coolwarm\",\n",
    "        center=0,\n",
    "        square=True,\n",
    "        linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.5},\n",
    "    )\n",
    "    plt.title(\"Feature Correlation Matrix\", fontsize=16, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # High correlation pairs\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i + 1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.8:\n",
    "                high_corr_pairs.append(\n",
    "                    (\n",
    "                        correlation_matrix.columns[i],\n",
    "                        correlation_matrix.columns[j],\n",
    "                        corr_val,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    if high_corr_pairs:\n",
    "        print(\"üîó High correlation pairs (|r| > 0.8):\")\n",
    "        for col1, col2, corr in high_corr_pairs:\n",
    "            print(f\"  ‚Ä¢ {col1} ‚Üî {col2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No highly correlated feature pairs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution analysis\n",
    "if len(numeric_cols) > 0:\n",
    "    # Select up to 6 most important features for visualization\n",
    "    viz_cols = numeric_cols[:6]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(\"Feature Distributions\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, col in enumerate(viz_cols):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "\n",
    "            # Plot distribution\n",
    "            if TARGET_COLUMN and TARGET_COLUMN in df.columns:\n",
    "                # Separate by target class\n",
    "                normal_data = df[df[TARGET_COLUMN] == 0][col].dropna()\n",
    "                anomaly_data = df[df[TARGET_COLUMN] == 1][col].dropna()\n",
    "\n",
    "                ax.hist(normal_data, bins=30, alpha=0.7, label=\"Normal\", density=True)\n",
    "                ax.hist(anomaly_data, bins=30, alpha=0.7, label=\"Anomaly\", density=True)\n",
    "                ax.legend()\n",
    "            else:\n",
    "                # Single distribution\n",
    "                ax.hist(df[col].dropna(), bins=30, alpha=0.7, density=True)\n",
    "\n",
    "            ax.set_title(f\"{col}\", fontweight=\"bold\")\n",
    "            ax.set_ylabel(\"Density\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for i in range(len(viz_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, target_column=None, id_columns=None):\n",
    "    \"\"\"Comprehensive data preprocessing pipeline.\"\"\"\n",
    "    print(\"=== DATA PREPROCESSING ===\")\n",
    "\n",
    "    # Make a copy\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # Remove ID columns\n",
    "    if id_columns:\n",
    "        id_cols_present = [col for col in id_columns if col in df_processed.columns]\n",
    "        if id_cols_present:\n",
    "            df_processed = df_processed.drop(columns=id_cols_present)\n",
    "            print(f\"üóëÔ∏è Removed ID columns: {id_cols_present}\")\n",
    "\n",
    "    # Separate features and target\n",
    "    if target_column and target_column in df_processed.columns:\n",
    "        y = df_processed[target_column]\n",
    "        X = df_processed.drop(columns=[target_column])\n",
    "        print(f\"üéØ Target separated: {target_column}\")\n",
    "    else:\n",
    "        y = None\n",
    "        X = df_processed\n",
    "        print(\"üìä Unsupervised mode: no target column\")\n",
    "\n",
    "    # Handle missing values\n",
    "    missing_before = X.isnull().sum().sum()\n",
    "    if missing_before > 0:\n",
    "        print(f\"‚ùì Handling {missing_before} missing values...\")\n",
    "\n",
    "        # Numeric columns: fill with median\n",
    "        numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if X[col].isnull().any():\n",
    "                median_val = X[col].median()\n",
    "                X[col].fillna(median_val, inplace=True)\n",
    "                print(f\"  ‚Ä¢ {col}: filled with median ({median_val:.3f})\")\n",
    "\n",
    "        # Categorical columns: fill with mode\n",
    "        categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "        for col in categorical_cols:\n",
    "            if X[col].isnull().any():\n",
    "                mode_val = (\n",
    "                    X[col].mode().iloc[0] if not X[col].mode().empty else \"Unknown\"\n",
    "                )\n",
    "                X[col].fillna(mode_val, inplace=True)\n",
    "                print(f\"  ‚Ä¢ {col}: filled with mode ({mode_val})\")\n",
    "\n",
    "    # Encode categorical variables\n",
    "    categorical_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"üè∑Ô∏è Encoding {len(categorical_cols)} categorical columns...\")\n",
    "\n",
    "        for col in categorical_cols:\n",
    "            unique_count = X[col].nunique()\n",
    "            if unique_count <= 10:  # One-hot encode low cardinality\n",
    "                dummies = pd.get_dummies(X[col], prefix=col, drop_first=True)\n",
    "                X = pd.concat([X.drop(columns=[col]), dummies], axis=1)\n",
    "                print(f\"  ‚Ä¢ {col}: one-hot encoded ({unique_count} categories)\")\n",
    "            else:  # Label encode high cardinality\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col])\n",
    "                print(f\"  ‚Ä¢ {col}: label encoded ({unique_count} categories)\")\n",
    "\n",
    "    # Scale numeric features\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"üìè Scaling {len(numeric_cols)} numeric features...\")\n",
    "        scaler = StandardScaler()\n",
    "        X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
    "        print(\"  ‚úÖ StandardScaler applied\")\n",
    "\n",
    "    print(\"\\n‚úÖ Preprocessing complete\")\n",
    "    print(f\"  ‚Ä¢ Final shape: {X.shape}\")\n",
    "    print(f\"  ‚Ä¢ Feature columns: {len(X.columns)}\")\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Apply preprocessing\n",
    "X, y = preprocess_data(df, TARGET_COLUMN, ID_COLUMNS)\n",
    "\n",
    "# Display processed data info\n",
    "print(\"\\nüìä Processed data summary:\")\n",
    "print(f\"  ‚Ä¢ Features shape: {X.shape}\")\n",
    "if y is not None:\n",
    "    print(f\"  ‚Ä¢ Target shape: {y.shape}\")\n",
    "    print(f\"  ‚Ä¢ Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Show sample of processed features\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Algorithm Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm configuration\n",
    "CONTAMINATION_RATE = 0.1  # Expected proportion of anomalies\n",
    "ALGORITHMS = {\n",
    "    \"IsolationForest\": {\n",
    "        \"adapter\": SklearnAdapter,\n",
    "        \"params\": {\n",
    "            \"contamination\": CONTAMINATION_RATE,\n",
    "            \"n_estimators\": 100,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "        },\n",
    "    },\n",
    "    \"LocalOutlierFactor\": {\n",
    "        \"adapter\": SklearnAdapter,\n",
    "        \"params\": {\n",
    "            \"contamination\": CONTAMINATION_RATE,\n",
    "            \"n_neighbors\": 20,\n",
    "            \"novelty\": True,\n",
    "        },\n",
    "    },\n",
    "    \"OneClassSVM\": {\n",
    "        \"adapter\": SklearnAdapter,\n",
    "        \"params\": {\"nu\": CONTAMINATION_RATE, \"kernel\": \"rbf\", \"gamma\": \"scale\"},\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"ü§ñ Configured {len(ALGORITHMS)} algorithms:\")\n",
    "for name, config in ALGORITHMS.items():\n",
    "    print(f\"  ‚Ä¢ {name}: {config['params']}\")\n",
    "\n",
    "# Create Pynomaly dataset\n",
    "dataset = Dataset(name=\"experiment_dataset\", data=X, feature_names=list(X.columns))\n",
    "\n",
    "print(f\"\\nüìä Dataset created: {dataset.summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Experiment Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "results = {}\n",
    "execution_times = {}\n",
    "\n",
    "print(\"=== EXPERIMENT EXECUTION ===\")\n",
    "print(f\"üöÄ Starting experiment with {len(ALGORITHMS)} algorithms...\\n\")\n",
    "\n",
    "for alg_name, config in ALGORITHMS.items():\n",
    "    print(f\"üîÑ Running {alg_name}...\")\n",
    "\n",
    "    try:\n",
    "        # Start timing\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        # Create contamination rate object\n",
    "        contamination_rate = ContaminationRate(CONTAMINATION_RATE)\n",
    "\n",
    "        # Initialize adapter\n",
    "        if config[\"adapter\"] == SklearnAdapter:\n",
    "            adapter = SklearnAdapter(alg_name, contamination_rate=contamination_rate)\n",
    "        else:\n",
    "            adapter = config[\"adapter\"](alg_name, contamination_rate=contamination_rate)\n",
    "\n",
    "        # Fit and detect\n",
    "        result = adapter.fit_detect(dataset)\n",
    "\n",
    "        # Record execution time\n",
    "        end_time = datetime.now()\n",
    "        execution_time = (end_time - start_time).total_seconds()\n",
    "        execution_times[alg_name] = execution_time\n",
    "\n",
    "        # Extract scores and predictions\n",
    "        scores = np.array([score.value for score in result.scores])\n",
    "        predictions = (\n",
    "            scores > np.percentile(scores, (1 - CONTAMINATION_RATE) * 100)\n",
    "        ).astype(int)\n",
    "\n",
    "        # Store results\n",
    "        results[alg_name] = {\n",
    "            \"scores\": scores,\n",
    "            \"predictions\": predictions,\n",
    "            \"result_object\": result,\n",
    "            \"execution_time\": execution_time,\n",
    "        }\n",
    "\n",
    "        print(f\"  ‚úÖ {alg_name} completed in {execution_time:.2f}s\")\n",
    "        print(f\"     Detected {predictions.sum()} anomalies ({predictions.mean():.1%})\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {alg_name} failed: {str(e)}\")\n",
    "        results[alg_name] = {\"error\": str(e)}\n",
    "\n",
    "print(\n",
    "    f\"\\nüéØ Experiment completed! {len([r for r in results.values() if 'error' not in r])} algorithms successful.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance evaluation (if ground truth is available)\n",
    "if y is not None:\n",
    "    from sklearn.metrics import (\n",
    "        average_precision_score,\n",
    "        f1_score,\n",
    "        precision_score,\n",
    "        recall_score,\n",
    "        roc_auc_score,\n",
    "    )\n",
    "\n",
    "    print(\"=== PERFORMANCE EVALUATION ===\")\n",
    "\n",
    "    # Create performance summary\n",
    "    performance_df = pd.DataFrame()\n",
    "\n",
    "    for alg_name, result in results.items():\n",
    "        if \"error\" in result:\n",
    "            continue\n",
    "\n",
    "        predictions = result[\"predictions\"]\n",
    "        scores = result[\"scores\"]\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            \"Algorithm\": alg_name,\n",
    "            \"Precision\": precision_score(y, predictions),\n",
    "            \"Recall\": recall_score(y, predictions),\n",
    "            \"F1-Score\": f1_score(y, predictions),\n",
    "            \"ROC-AUC\": roc_auc_score(y, scores),\n",
    "            \"PR-AUC\": average_precision_score(y, scores),\n",
    "            \"Execution Time (s)\": result[\"execution_time\"],\n",
    "        }\n",
    "\n",
    "        performance_df = pd.concat(\n",
    "            [performance_df, pd.DataFrame([metrics])], ignore_index=True\n",
    "        )\n",
    "\n",
    "    # Display performance table\n",
    "    if not performance_df.empty:\n",
    "        print(\"üìä Performance Summary:\")\n",
    "        display(performance_df.round(4))\n",
    "\n",
    "        # Identify best algorithms\n",
    "        best_f1 = performance_df.loc[performance_df[\"F1-Score\"].idxmax(), \"Algorithm\"]\n",
    "        best_auc = performance_df.loc[performance_df[\"ROC-AUC\"].idxmax(), \"Algorithm\"]\n",
    "        fastest = performance_df.loc[\n",
    "            performance_df[\"Execution Time (s)\"].idxmin(), \"Algorithm\"\n",
    "        ]\n",
    "\n",
    "        print(\"\\nüèÜ Best Performers:\")\n",
    "        print(\n",
    "            f\"  ‚Ä¢ Best F1-Score: {best_f1} ({performance_df[performance_df['Algorithm'] == best_f1]['F1-Score'].iloc[0]:.4f})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  ‚Ä¢ Best ROC-AUC: {best_auc} ({performance_df[performance_df['Algorithm'] == best_auc]['ROC-AUC'].iloc[0]:.4f})\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  ‚Ä¢ Fastest: {fastest} ({performance_df[performance_df['Algorithm'] == fastest]['Execution Time (s)'].iloc[0]:.2f}s)\"\n",
    "        )\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No ground truth available - skipping supervised evaluation\")\n",
    "\n",
    "    # Unsupervised evaluation metrics\n",
    "    print(\"=== UNSUPERVISED EVALUATION ===\")\n",
    "\n",
    "    unsupervised_df = pd.DataFrame()\n",
    "\n",
    "    for alg_name, result in results.items():\n",
    "        if \"error\" in result:\n",
    "            continue\n",
    "\n",
    "        scores = result[\"scores\"]\n",
    "        predictions = result[\"predictions\"]\n",
    "\n",
    "        metrics = {\n",
    "            \"Algorithm\": alg_name,\n",
    "            \"Anomalies Detected\": predictions.sum(),\n",
    "            \"Anomaly Rate\": predictions.mean(),\n",
    "            \"Score Mean\": scores.mean(),\n",
    "            \"Score Std\": scores.std(),\n",
    "            \"Execution Time (s)\": result[\"execution_time\"],\n",
    "        }\n",
    "\n",
    "        unsupervised_df = pd.concat(\n",
    "            [unsupervised_df, pd.DataFrame([metrics])], ignore_index=True\n",
    "        )\n",
    "\n",
    "    if not unsupervised_df.empty:\n",
    "        print(\"üìä Unsupervised Metrics Summary:\")\n",
    "        display(unsupervised_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves (if ground truth available)\n",
    "if y is not None and len([r for r in results.values() if \"error\" not in r]) > 0:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    colors = [\"blue\", \"red\", \"green\", \"orange\", \"purple\"]\n",
    "\n",
    "    for i, (alg_name, result) in enumerate(results.items()):\n",
    "        if \"error\" in result:\n",
    "            continue\n",
    "\n",
    "        scores = result[\"scores\"]\n",
    "\n",
    "        # Calculate ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y, scores)\n",
    "        auc_score = roc_auc_score(y, scores)\n",
    "\n",
    "        # Plot ROC curve\n",
    "        plt.plot(\n",
    "            fpr,\n",
    "            tpr,\n",
    "            color=colors[i % len(colors)],\n",
    "            linewidth=2,\n",
    "            label=f\"{alg_name} (AUC = {auc_score:.3f})\",\n",
    "        )\n",
    "\n",
    "    # Plot diagonal line\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", linewidth=1, alpha=0.8)\n",
    "\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=12, fontweight=\"bold\")\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=12, fontweight=\"bold\")\n",
    "    plt.title(\"ROC Curves Comparison\", fontsize=16, fontweight=\"bold\")\n",
    "    plt.legend(loc=\"lower right\", fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score distributions comparison\n",
    "successful_results = {\n",
    "    name: result for name, result in results.items() if \"error\" not in result\n",
    "}\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle(\"Algorithm Comparison Analysis\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    # 1. Score distributions\n",
    "    ax1 = axes[0, 0]\n",
    "    for alg_name, result in successful_results.items():\n",
    "        scores = result[\"scores\"]\n",
    "        ax1.hist(scores, bins=30, alpha=0.6, label=alg_name, density=True)\n",
    "    ax1.set_xlabel(\"Anomaly Score\")\n",
    "    ax1.set_ylabel(\"Density\")\n",
    "    ax1.set_title(\"Score Distributions\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Execution time comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    alg_names = list(successful_results.keys())\n",
    "    exec_times = [successful_results[name][\"execution_time\"] for name in alg_names]\n",
    "    bars = ax2.bar(alg_names, exec_times, alpha=0.7)\n",
    "    ax2.set_ylabel(\"Execution Time (seconds)\")\n",
    "    ax2.set_title(\"Execution Time Comparison\")\n",
    "    ax2.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    # Add value labels on bars\n",
    "    for bar, time_val in zip(bars, exec_times, strict=False):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.01,\n",
    "            f\"{time_val:.2f}s\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    # 3. Anomaly detection rates\n",
    "    ax3 = axes[1, 0]\n",
    "    detection_rates = [\n",
    "        successful_results[name][\"predictions\"].mean() for name in alg_names\n",
    "    ]\n",
    "    bars = ax3.bar(alg_names, detection_rates, alpha=0.7, color=\"coral\")\n",
    "    ax3.set_ylabel(\"Anomaly Detection Rate\")\n",
    "    ax3.set_title(\"Detection Rate Comparison\")\n",
    "    ax3.tick_params(axis=\"x\", rotation=45)\n",
    "    ax3.axhline(\n",
    "        y=CONTAMINATION_RATE,\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        label=f\"Expected Rate ({CONTAMINATION_RATE:.1%})\",\n",
    "    )\n",
    "    ax3.legend()\n",
    "\n",
    "    # Add value labels\n",
    "    for bar, rate in zip(bars, detection_rates, strict=False):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(\n",
    "            bar.get_x() + bar.get_width() / 2.0,\n",
    "            height + 0.001,\n",
    "            f\"{rate:.1%}\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    # 4. Performance radar chart (if ground truth available)\n",
    "    ax4 = axes[1, 1]\n",
    "    if y is not None and not performance_df.empty:\n",
    "        # Create radar chart\n",
    "        metrics = [\"Precision\", \"Recall\", \"F1-Score\", \"ROC-AUC\", \"PR-AUC\"]\n",
    "\n",
    "        # Select up to 3 algorithms for clarity\n",
    "        top_algorithms = performance_df.nlargest(3, \"F1-Score\")[\"Algorithm\"].tolist()\n",
    "\n",
    "        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Complete the circle\n",
    "\n",
    "        for i, alg in enumerate(top_algorithms):\n",
    "            values = (\n",
    "                performance_df[performance_df[\"Algorithm\"] == alg][metrics]\n",
    "                .iloc[0]\n",
    "                .tolist()\n",
    "            )\n",
    "            values += values[:1]  # Complete the circle\n",
    "\n",
    "            ax4.plot(angles, values, \"o-\", linewidth=2, label=alg)\n",
    "            ax4.fill(angles, values, alpha=0.25)\n",
    "\n",
    "        ax4.set_xticks(angles[:-1])\n",
    "        ax4.set_xticklabels(metrics)\n",
    "        ax4.set_ylim(0, 1)\n",
    "        ax4.set_title(\"Performance Comparison (Top 3)\")\n",
    "        ax4.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.0))\n",
    "        ax4.grid(True)\n",
    "    else:\n",
    "        ax4.text(\n",
    "            0.5,\n",
    "            0.5,\n",
    "            \"Performance Radar\\n(Ground Truth Required)\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            transform=ax4.transAxes,\n",
    "            fontsize=12,\n",
    "            style=\"italic\",\n",
    "        )\n",
    "        ax4.set_title(\"Performance Radar Chart\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Statistical Analysis and Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical significance testing (if ground truth available)\n",
    "if y is not None and len(successful_results) > 1:\n",
    "    print(\"=== STATISTICAL SIGNIFICANCE TESTING ===\")\n",
    "\n",
    "    # Cross-validation for statistical testing\n",
    "    from scipy.stats import friedmanchisquare, wilcoxon\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    # Collect cross-validation scores\n",
    "    cv_scores = {}\n",
    "\n",
    "    print(\"üîÑ Performing 5-fold cross-validation...\")\n",
    "\n",
    "    for alg_name in successful_results.keys():\n",
    "        try:\n",
    "            # Create sklearn-compatible estimator for CV\n",
    "            if alg_name == \"IsolationForest\":\n",
    "                from sklearn.ensemble import IsolationForest\n",
    "\n",
    "                estimator = IsolationForest(\n",
    "                    contamination=CONTAMINATION_RATE, random_state=RANDOM_STATE\n",
    "                )\n",
    "            elif alg_name == \"LocalOutlierFactor\":\n",
    "                from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "                estimator = LocalOutlierFactor(\n",
    "                    contamination=CONTAMINATION_RATE, novelty=True\n",
    "                )\n",
    "            elif alg_name == \"OneClassSVM\":\n",
    "                from sklearn.svm import OneClassSVM\n",
    "\n",
    "                estimator = OneClassSVM(nu=CONTAMINATION_RATE)\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            # Perform cross-validation\n",
    "            scores = cross_val_score(estimator, X, y, cv=5, scoring=\"f1\")\n",
    "            cv_scores[alg_name] = scores\n",
    "\n",
    "            print(f\"  ‚Ä¢ {alg_name}: {scores.mean():.4f} ¬± {scores.std():.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå {alg_name}: CV failed ({str(e)})\")\n",
    "\n",
    "    # Pairwise statistical tests\n",
    "    if len(cv_scores) > 1:\n",
    "        print(\"\\nüìä Pairwise Wilcoxon Signed-Rank Tests:\")\n",
    "\n",
    "        algorithm_names = list(cv_scores.keys())\n",
    "        significance_results = []\n",
    "\n",
    "        for i, alg1 in enumerate(algorithm_names):\n",
    "            for alg2 in algorithm_names[i + 1 :]:\n",
    "                try:\n",
    "                    statistic, p_value = wilcoxon(cv_scores[alg1], cv_scores[alg2])\n",
    "                    is_significant = p_value < 0.05\n",
    "\n",
    "                    better_alg = (\n",
    "                        alg1\n",
    "                        if np.mean(cv_scores[alg1]) > np.mean(cv_scores[alg2])\n",
    "                        else alg2\n",
    "                    )\n",
    "\n",
    "                    result = {\n",
    "                        \"Comparison\": f\"{alg1} vs {alg2}\",\n",
    "                        \"P-value\": p_value,\n",
    "                        \"Significant\": \"‚úÖ\" if is_significant else \"‚ùå\",\n",
    "                        \"Better Algorithm\": better_alg,\n",
    "                    }\n",
    "\n",
    "                    significance_results.append(result)\n",
    "\n",
    "                    significance_symbol = (\n",
    "                        \"***\"\n",
    "                        if p_value < 0.001\n",
    "                        else \"**\"\n",
    "                        if p_value < 0.01\n",
    "                        else \"*\"\n",
    "                        if p_value < 0.05\n",
    "                        else \"ns\"\n",
    "                    )\n",
    "                    print(\n",
    "                        f\"  ‚Ä¢ {alg1} vs {alg2}: p = {p_value:.4f} ({significance_symbol}) - {better_alg} performs better\"\n",
    "                    )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ùå {alg1} vs {alg2}: Test failed ({str(e)})\")\n",
    "\n",
    "        # Overall significance test\n",
    "        if len(cv_scores) > 2:\n",
    "            try:\n",
    "                scores_matrix = [cv_scores[alg] for alg in algorithm_names]\n",
    "                statistic, p_value = friedmanchisquare(*scores_matrix)\n",
    "\n",
    "                print(\"\\nüî¨ Friedman Test (Overall):\")\n",
    "                print(f\"  ‚Ä¢ Statistic: {statistic:.4f}\")\n",
    "                print(f\"  ‚Ä¢ P-value: {p_value:.4f}\")\n",
    "\n",
    "                if p_value < 0.05:\n",
    "                    print(\"  ‚Ä¢ ‚úÖ Significant differences detected between algorithms\")\n",
    "                else:\n",
    "                    print(\"  ‚Ä¢ ‚ùå No significant differences between algorithms\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Friedman test failed: {str(e)}\")\n",
    "\n",
    "        # Create significance summary table\n",
    "        if significance_results:\n",
    "            significance_df = pd.DataFrame(significance_results)\n",
    "            print(\"\\nüìã Statistical Significance Summary:\")\n",
    "            display(significance_df)\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Statistical testing requires ground truth and multiple algorithms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Final Results Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== EXPERIMENT SUMMARY AND RECOMMENDATIONS ===\")\n",
    "print(f\"üìÖ Experiment completed: {datetime.now()}\")\n",
    "print(f\"üìä Dataset: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
    "print(f\"ü§ñ Algorithms tested: {len(ALGORITHMS)}\")\n",
    "print(f\"‚úÖ Successful runs: {len(successful_results)}\")\n",
    "\n",
    "if y is not None and not performance_df.empty:\n",
    "    print(\"\\nüèÜ BEST PERFORMING ALGORITHMS:\")\n",
    "\n",
    "    # Sort by F1-score\n",
    "    top_performers = performance_df.nlargest(3, \"F1-Score\")\n",
    "\n",
    "    for i, (_, row) in enumerate(top_performers.iterrows(), 1):\n",
    "        print(f\"  {i}. {row['Algorithm']}:\")\n",
    "        print(f\"     ‚Ä¢ F1-Score: {row['F1-Score']:.4f}\")\n",
    "        print(f\"     ‚Ä¢ Precision: {row['Precision']:.4f}\")\n",
    "        print(f\"     ‚Ä¢ Recall: {row['Recall']:.4f}\")\n",
    "        print(f\"     ‚Ä¢ ROC-AUC: {row['ROC-AUC']:.4f}\")\n",
    "        print(f\"     ‚Ä¢ Execution Time: {row['Execution Time (s)']:.2f}s\")\n",
    "\n",
    "    print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "\n",
    "    best_algorithm = top_performers.iloc[0][\"Algorithm\"]\n",
    "    best_f1 = top_performers.iloc[0][\"F1-Score\"]\n",
    "\n",
    "    print(f\"  ‚Ä¢ ü•á Primary recommendation: {best_algorithm}\")\n",
    "    print(f\"    - Achieved best F1-score of {best_f1:.4f}\")\n",
    "\n",
    "    # Performance-based recommendations\n",
    "    if best_f1 > 0.8:\n",
    "        print(\"    - ‚úÖ Excellent performance (F1 > 0.8) - ready for production\")\n",
    "    elif best_f1 > 0.6:\n",
    "        print(\"    - ‚ö†Ô∏è Good performance (F1 > 0.6) - consider hyperparameter tuning\")\n",
    "    else:\n",
    "        print(\n",
    "            \"    - üîß Moderate performance (F1 < 0.6) - requires further optimization\"\n",
    "        )\n",
    "\n",
    "    # Speed recommendations\n",
    "    fastest_alg = performance_df.loc[performance_df[\"Execution Time (s)\"].idxmin()]\n",
    "    if fastest_alg[\"Algorithm\"] != best_algorithm:\n",
    "        print(f\"  ‚Ä¢ ‚ö° For speed-critical applications: {fastest_alg['Algorithm']}\")\n",
    "        print(f\"    - Fastest execution time: {fastest_alg['Execution Time (s)']:.2f}s\")\n",
    "        print(f\"    - F1-Score: {fastest_alg['F1-Score']:.4f}\")\n",
    "\n",
    "    # Balance recommendation\n",
    "    performance_df[\"efficiency_score\"] = (\n",
    "        performance_df[\"F1-Score\"] / performance_df[\"Execution Time (s)\"]\n",
    "    )\n",
    "    most_efficient = performance_df.loc[performance_df[\"efficiency_score\"].idxmax()]\n",
    "\n",
    "    print(f\"  ‚Ä¢ ‚öñÔ∏è Best performance/speed balance: {most_efficient['Algorithm']}\")\n",
    "    print(f\"    - Efficiency score: {most_efficient['efficiency_score']:.4f}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nüìä UNSUPERVISED ANALYSIS SUMMARY:\")\n",
    "    if not unsupervised_df.empty:\n",
    "        for _, row in unsupervised_df.iterrows():\n",
    "            print(\n",
    "                f\"  ‚Ä¢ {row['Algorithm']}: {row['Anomalies Detected']} anomalies ({row['Anomaly Rate']:.1%})\"\n",
    "            )\n",
    "\n",
    "    print(\"\\nüí° GENERAL RECOMMENDATIONS:\")\n",
    "    print(\"  ‚Ä¢ Collect ground truth labels for supervised evaluation\")\n",
    "    print(\"  ‚Ä¢ Consider domain expert validation of detected anomalies\")\n",
    "    print(\"  ‚Ä¢ Implement ensemble methods for improved robustness\")\n",
    "\n",
    "print(\"\\nüîÑ NEXT STEPS:\")\n",
    "print(\"  1. üéØ Hyperparameter tuning on best-performing algorithm\")\n",
    "print(\"  2. üß™ Feature engineering and selection\")\n",
    "print(\"  3. üîó Ensemble method development\")\n",
    "print(\"  4. üìà Cross-validation with larger datasets\")\n",
    "print(\"  5. üöÄ Production deployment planning\")\n",
    "\n",
    "print(\"\\nüìã EXPERIMENT METADATA:\")\n",
    "print(f\"  ‚Ä¢ Random seed: {RANDOM_STATE}\")\n",
    "print(f\"  ‚Ä¢ Contamination rate: {CONTAMINATION_RATE}\")\n",
    "print(f\"  ‚Ä¢ Total execution time: {sum(execution_times.values()):.2f}s\")\n",
    "print(\"  ‚Ä¢ Reproducible: ‚úÖ (random seed set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Results and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment results\n",
    "experiment_results = {\n",
    "    \"metadata\": {\n",
    "        \"experiment_date\": datetime.now().isoformat(),\n",
    "        \"dataset_shape\": X.shape,\n",
    "        \"algorithms_tested\": list(ALGORITHMS.keys()),\n",
    "        \"contamination_rate\": CONTAMINATION_RATE,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "    },\n",
    "    \"performance_metrics\": performance_df.to_dict(\"records\")\n",
    "    if y is not None and not performance_df.empty\n",
    "    else None,\n",
    "    \"execution_times\": execution_times,\n",
    "    \"successful_algorithms\": list(successful_results.keys()),\n",
    "    \"failed_algorithms\": [\n",
    "        name for name, result in results.items() if \"error\" in result\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "output_file = f\"experiment_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "try:\n",
    "    import json\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(experiment_results, f, indent=2, default=str)\n",
    "    print(f\"üíæ Results saved to: {output_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to save results: {str(e)}\")\n",
    "\n",
    "# Export performance summary to CSV\n",
    "if y is not None and not performance_df.empty:\n",
    "    csv_file = f\"performance_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    try:\n",
    "        performance_df.to_csv(csv_file, index=False)\n",
    "        print(f\"üìä Performance summary saved to: {csv_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save CSV: {str(e)}\")\n",
    "\n",
    "print(\"\\n‚úÖ Experiment completed successfully!\")\n",
    "print(\"üìã Check the output files for detailed results and further analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Experiment Notes\n",
    "\n",
    "**Add your observations, insights, and future work ideas here:**\n",
    "\n",
    "### Key Findings\n",
    "- [Add your key findings here]\n",
    "- [Any unexpected results or patterns]\n",
    "- [Algorithm-specific observations]\n",
    "\n",
    "### Lessons Learned\n",
    "- [What worked well]\n",
    "- [What could be improved]\n",
    "- [Data quality insights]\n",
    "\n",
    "### Future Work\n",
    "- [Planned improvements]\n",
    "- [Additional algorithms to test]\n",
    "- [Feature engineering ideas]\n",
    "- [Production deployment considerations]\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Template Usage**: This template provides a comprehensive framework for anomaly detection experiments. Customize the sections based on your specific needs and domain requirements.\n",
    "\n",
    "**üìö Documentation**: For more information about Pynomaly, visit the [documentation](https://github.com/your-org/pynomaly) or check the examples directory.\n",
    "\n",
    "**ü§ù Support**: If you encounter issues or have questions, please create an issue in the GitHub repository or contact the development team."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
