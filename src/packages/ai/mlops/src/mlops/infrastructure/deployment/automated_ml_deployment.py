#!/usr/bin/env python3
"""
Automated ML Model Deployment System
Advanced deployment automation with blue-green, canary, and A/B testing capabilities.
"""

import asyncio
import json
import logging
import os
import sys
import time
import yaml
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import requests
import kubernetes
from kubernetes import client, config
import docker
import boto3
import numpy as np

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ml-deployment.log'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)


class DeploymentStrategy(Enum):
    """Deployment strategy types."""
    BLUE_GREEN = "blue_green"
    CANARY = "canary"
    ROLLING = "rolling"
    A_B_TESTING = "a_b_testing"
    SHADOW = "shadow"


class DeploymentStatus(Enum):
    """Deployment status."""
    PENDING = "pending"
    BUILDING = "building"
    DEPLOYING = "deploying"
    TESTING = "testing"
    ACTIVE = "active"
    FAILED = "failed"
    ROLLING_BACK = "rolling_back"
    ROLLED_BACK = "rolled_back"


@dataclass
class ModelDeploymentConfig:
    """Model deployment configuration."""
    model_id: str
    version: str
    strategy: DeploymentStrategy
    environment: str
    replicas: int
    resources: Dict[str, str]
    auto_scaling: Dict[str, Any]
    health_checks: Dict[str, Any]
    traffic_config: Dict[str, Any]
    monitoring: Dict[str, Any]


@dataclass
class DeploymentMetrics:
    """Deployment performance metrics."""
    response_time_p95: float
    response_time_p99: float
    throughput_rps: float
    error_rate: float
    cpu_usage: float
    memory_usage: float
    success_rate: float
    availability: float


class AutomatedMLDeployment:
    """Automated ML model deployment system."""
    
    def __init__(self, config_path: str = "mlops/config/deployment-config.yaml"):
        self.config_path = Path(config_path)
        self.config = self._load_config()
        self.k8s_client = None
        self.docker_client = None
        self.active_deployments = {}
        self.deployment_history = []
        self._initialize_clients()
    
    def _load_config(self) -> Dict[str, Any]:
        """Load deployment configuration."""
        if not self.config_path.exists():
            logger.warning(f"Config file not found: {self.config_path}")
            return self._create_default_config()
        
        with open(self.config_path) as f:
            return yaml.safe_load(f)
    
    def _create_default_config(self) -> Dict[str, Any]:
        """Create default deployment configuration."""
        default_config = {
            "deployment": {
                "name": "automated_ml_deployment",
                "version": "1.0.0",
                "environments": {
                    "staging": {
                        "kubernetes_namespace": "ml-staging",
                        "cluster_endpoint": "https://staging-k8s.company.com",
                        "ingress_class": "nginx",
                        "domain": "staging-ml.company.com",
                        "default_strategy": "rolling"
                    },
                    "production": {
                        "kubernetes_namespace": "ml-production",
                        "cluster_endpoint": "https://prod-k8s.company.com",
                        "ingress_class": "nginx",
                        "domain": "ml.company.com",
                        "default_strategy": "blue_green"
                    }
                },
                "container_registry": {
                    "url": "ml-registry.company.com",
                    "namespace": "ml-models",
                    "authentication": {
                        "type": "service_account",
                        "credentials_path": "/var/secrets/registry-creds"
                    }
                },
                "strategies": {
                    "blue_green": {
                        "switch_traffic_delay": 300,  # 5 minutes
                        "keep_old_version": True,
                        "rollback_timeout": 600,
                        "health_check_retries": 5
                    },
                    "canary": {
                        "initial_traffic": 5,  # 5%
                        "traffic_increments": [10, 25, 50, 100],
                        "stage_duration": 600,  # 10 minutes
                        "success_threshold": 0.99,
                        "error_threshold": 0.01
                    },
                    "a_b_testing": {
                        "traffic_split": 50,  # 50/50 split
                        "test_duration": 86400,  # 24 hours
                        "confidence_level": 0.95,
                        "minimum_sample_size": 1000
                    },
                    "shadow": {
                        "shadow_traffic": 100,  # 100% shadow traffic
                        "comparison_metrics": ["accuracy", "latency", "throughput"],
                        "duration": 3600  # 1 hour
                    }
                },
                "resources": {
                    "small": {
                        "cpu": "250m",
                        "memory": "512Mi",
                        "replicas": 2
                    },
                    "medium": {
                        "cpu": "500m",
                        "memory": "1Gi",
                        "replicas": 3
                    },
                    "large": {
                        "cpu": "1000m",
                        "memory": "2Gi",
                        "replicas": 5
                    }
                },
                "auto_scaling": {
                    "enabled": True,
                    "min_replicas": 2,
                    "max_replicas": 20,
                    "cpu_threshold": 70,
                    "memory_threshold": 80,
                    "scale_up_cooldown": 300,
                    "scale_down_cooldown": 600
                },
                "health_checks": {
                    "liveness_probe": {
                        "path": "/health/live",
                        "initial_delay": 30,
                        "period": 10,
                        "timeout": 5,
                        "failure_threshold": 3
                    },
                    "readiness_probe": {
                        "path": "/health/ready",
                        "initial_delay": 15,
                        "period": 5,
                        "timeout": 3,
                        "failure_threshold": 3
                    }
                },
                "monitoring": {
                    "prometheus_enabled": True,
                    "grafana_dashboard": True,
                    "log_aggregation": True,
                    "distributed_tracing": True,
                    "custom_metrics": [
                        "model_inference_time",
                        "model_accuracy",
                        "prediction_confidence",
                        "data_drift_score"
                    ]
                },
                "security": {
                    "network_policies": True,
                    "pod_security_policies": True,
                    "rbac_enabled": True,
                    "secrets_management": "kubernetes",
                    "image_scanning": True
                }
            },
            "rollback": {
                "automatic": {
                    "enabled": True,
                    "triggers": {
                        "error_rate_threshold": 0.05,
                        "response_time_threshold": 2000,  # 2 seconds
                        "success_rate_threshold": 0.95
                    },
                    "cooldown_period": 300
                },
                "manual": {
                    "approval_required": True,
                    "authorized_users": ["ml-ops-team"],
                    "notification_channels": ["slack", "email"]
                }
            }
        }
        
        # Save default config
        self.config_path.parent.mkdir(parents=True, exist_ok=True)
        with open(self.config_path, 'w') as f:
            yaml.dump(default_config, f, default_flow_style=False)
        
        logger.info(f"Created default deployment configuration: {self.config_path}")
        return default_config
    
    def _initialize_clients(self):
        """Initialize Kubernetes and Docker clients."""
        try:
            # Initialize Kubernetes client
            try:
                config.load_incluster_config()  # For in-cluster deployment
            except:
                try:
                    config.load_kube_config()  # For local development
                except:
                    logger.warning("Could not load Kubernetes config, using mock client")
                    self.k8s_client = None
                    return
            
            self.k8s_client = {
                'apps_v1': client.AppsV1Api(),
                'core_v1': client.CoreV1Api(),
                'networking_v1': client.NetworkingV1Api(),
                'autoscaling_v1': client.AutoscalingV1Api()
            }
            
            # Initialize Docker client
            try:
                self.docker_client = docker.from_env()
            except Exception as e:
                logger.warning(f"Could not initialize Docker client: {e}")
                self.docker_client = None
            
            logger.info("Deployment clients initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize clients: {e}")
            self.k8s_client = None
            self.docker_client = None
    
    async def deploy_model(
        self,
        model_id: str,
        version: str,
        environment: str = "staging",
        strategy: Optional[DeploymentStrategy] = None,
        config_override: Optional[Dict[str, Any]] = None
    ) -> str:
        """Deploy model with specified strategy."""
        deployment_id = f"deploy-{model_id}-{version}-{environment}-{int(time.time())}"
        
        logger.info(f"ðŸš€ Starting deployment: {deployment_id}")
        
        try:
            # Get environment configuration
            env_config = self.config["deployment"]["environments"][environment]
            
            # Determine deployment strategy
            if strategy is None:
                strategy = DeploymentStrategy(env_config["default_strategy"])
            
            # Build deployment configuration
            deployment_config = self._build_deployment_config(
                model_id, version, environment, strategy, config_override
            )
            
            # Initialize deployment record
            deployment_record = {
                "deployment_id": deployment_id,
                "model_id": model_id,
                "version": version,
                "environment": environment,
                "strategy": strategy.value,
                "status": DeploymentStatus.PENDING.value,
                "created_at": datetime.utcnow().isoformat(),
                "config": asdict(deployment_config),
                "phases": [],
                "metrics": {},
                "rollback_info": None
            }
            
            self.active_deployments[deployment_id] = deployment_record
            
            # Execute deployment based on strategy
            if strategy == DeploymentStrategy.BLUE_GREEN:
                result = await self._execute_blue_green_deployment(deployment_id, deployment_config)
            elif strategy == DeploymentStrategy.CANARY:
                result = await self._execute_canary_deployment(deployment_id, deployment_config)
            elif strategy == DeploymentStrategy.ROLLING:
                result = await self._execute_rolling_deployment(deployment_id, deployment_config)
            elif strategy == DeploymentStrategy.A_B_TESTING:
                result = await self._execute_ab_testing_deployment(deployment_id, deployment_config)
            elif strategy == DeploymentStrategy.SHADOW:
                result = await self._execute_shadow_deployment(deployment_id, deployment_config)
            else:
                raise ValueError(f"Unsupported deployment strategy: {strategy}")
            
            # Update deployment status
            deployment_record["status"] = DeploymentStatus.ACTIVE.value if result["success"] else DeploymentStatus.FAILED.value
            deployment_record["completed_at"] = datetime.utcnow().isoformat()
            deployment_record["result"] = result
            
            # Move to history
            self.deployment_history.append(deployment_record.copy())
            
            if result["success"]:
                logger.info(f"âœ… Deployment completed successfully: {deployment_id}")
            else:
                logger.error(f"âŒ Deployment failed: {deployment_id} - {result.get('error', 'Unknown error')}")
            
            return deployment_id
            
        except Exception as e:
            logger.error(f"ðŸ’¥ Deployment failed with exception: {e}")
            if deployment_id in self.active_deployments:
                self.active_deployments[deployment_id]["status"] = DeploymentStatus.FAILED.value
                self.active_deployments[deployment_id]["error"] = str(e)
            raise
    
    def _build_deployment_config(
        self,
        model_id: str,
        version: str,
        environment: str,
        strategy: DeploymentStrategy,
        config_override: Optional[Dict[str, Any]] = None
    ) -> ModelDeploymentConfig:
        """Build comprehensive deployment configuration."""
        env_config = self.config["deployment"]["environments"][environment]
        resource_config = self.config["deployment"]["resources"]["medium"]  # Default to medium
        
        # Base configuration
        base_config = {
            "model_id": model_id,
            "version": version,
            "strategy": strategy,
            "environment": environment,
            "replicas": resource_config["replicas"],
            "resources": {
                "cpu": resource_config["cpu"],
                "memory": resource_config["memory"]
            },
            "auto_scaling": self.config["deployment"]["auto_scaling"],
            "health_checks": self.config["deployment"]["health_checks"],
            "traffic_config": {
                "domain": env_config["domain"],
                "path": f"/models/{model_id}/v{version}",
                "ingress_class": env_config["ingress_class"]
            },
            "monitoring": self.config["deployment"]["monitoring"]
        }
        
        # Apply strategy-specific configuration
        strategy_config = self.config["deployment"]["strategies"].get(strategy.value, {})
        base_config.update(strategy_config)
        
        # Apply overrides
        if config_override:
            self._deep_update(base_config, config_override)
        
        return ModelDeploymentConfig(**base_config)
    
    def _deep_update(self, base_dict: Dict, update_dict: Dict):
        """Deep update dictionary with nested values."""
        for key, value in update_dict.items():
            if isinstance(value, dict) and key in base_dict and isinstance(base_dict[key], dict):
                self._deep_update(base_dict[key], value)
            else:
                base_dict[key] = value
    
    async def _execute_blue_green_deployment(
        self,
        deployment_id: str,
        config: ModelDeploymentConfig
    ) -> Dict[str, Any]:
        """Execute blue-green deployment strategy."""
        logger.info(f"ðŸ”µðŸŸ¢ Executing blue-green deployment: {deployment_id}")
        
        deployment_record = self.active_deployments[deployment_id]
        deployment_record["status"] = DeploymentStatus.BUILDING.value
        
        try:
            # Phase 1: Build new version (Green)
            logger.info("Phase 1: Building green version...")
            build_result = await self._build_model_container(config)
            if not build_result["success"]:
                return {"success": False, "error": "Container build failed", "phase": "build"}
            
            deployment_record["phases"].append({
                "name": "build",
                "status": "completed",
                "duration": build_result["duration"],
                "timestamp": datetime.utcnow().isoformat()
            })
            
            # Phase 2: Deploy green version
            logger.info("Phase 2: Deploying green version...")
            deployment_record["status"] = DeploymentStatus.DEPLOYING.value
            
            green_deployment = await self._deploy_kubernetes_service(config, "green")
            if not green_deployment["success"]:
                return {"success": False, "error": "Green deployment failed", "phase": "deploy_green"}
            
            deployment_record["phases"].append({
                "name": "deploy_green",
                "status": "completed",
                "endpoint": green_deployment["endpoint"],
                "timestamp": datetime.utcnow().isoformat()
            })
            
            # Phase 3: Health checks and testing
            logger.info("Phase 3: Running health checks...")
            deployment_record["status"] = DeploymentStatus.TESTING.value
            
            health_check_result = await self._run_health_checks(green_deployment["endpoint"], config)
            if not health_check_result["healthy"]:
                await self._cleanup_deployment(green_deployment["deployment_name"])
                return {"success": False, "error": "Health checks failed", "phase": "health_check"}
            
            deployment_record["phases"].append({
                "name": "health_check",
                "status": "completed",
                "health_score": health_check_result["score"],
                "timestamp": datetime.utcnow().isoformat()
            })
            
            # Phase 4: Switch traffic from blue to green
            logger.info("Phase 4: Switching traffic to green...")
            
            traffic_switch_result = await self._switch_traffic(config, "green")
            if not traffic_switch_result["success"]:
                await self._cleanup_deployment(green_deployment["deployment_name"])
                return {"success": False, "error": "Traffic switch failed", "phase": "traffic_switch"}
            
            deployment_record["phases"].append({
                "name": "traffic_switch",
                "status": "completed",
                "timestamp": datetime.utcnow().isoformat()
            })
            
            # Phase 5: Monitor and validate
            logger.info("Phase 5: Monitoring new deployment...")
            
            monitoring_result = await self._monitor_deployment(config, duration=300)  # 5 minutes
            
            deployment_record["phases"].append({
                "name": "monitoring",
                "status": "completed",
                "metrics": monitoring_result["metrics"],
                "timestamp": datetime.utcnow().isoformat()
            })
            
            # Phase 6: Cleanup old version (blue) if successful
            if monitoring_result["stable"]:
                logger.info("Phase 6: Cleaning up blue version...")
                await self._cleanup_old_deployments(config, keep_versions=1)
                
                deployment_record["phases"].append({
                    "name": "cleanup",
                    "status": "completed",
                    "timestamp": datetime.utcnow().isoformat()
                })
            
            logger.info("âœ… Blue-green deployment completed successfully")
            return {
                "success": True,
                "strategy": "blue_green",
                "endpoint": green_deployment["endpoint"],
                "phases_completed": len(deployment_record["phases"]),
                "final_metrics": monitoring_result["metrics"]
            }
            
        except Exception as e:
            logger.error(f"Blue-green deployment failed: {e}")
            return {"success": False, "error": str(e), "phase": "unknown"}
    
    async def _execute_canary_deployment(
        self,
        deployment_id: str,
        config: ModelDeploymentConfig
    ) -> Dict[str, Any]:
        """Execute canary deployment strategy."""
        logger.info(f"ðŸ¤ Executing canary deployment: {deployment_id}")
        
        deployment_record = self.active_deployments[deployment_id]
        strategy_config = self.config["deployment"]["strategies"]["canary"]
        
        try:
            # Phase 1: Build and deploy canary version
            logger.info("Phase 1: Building canary version...")
            deployment_record["status"] = DeploymentStatus.BUILDING.value
            
            build_result = await self._build_model_container(config)
            if not build_result["success"]:
                return {"success": False, "error": "Container build failed"}
            
            canary_deployment = await self._deploy_kubernetes_service(config, "canary")
            if not canary_deployment["success"]:
                return {"success": False, "error": "Canary deployment failed"}
            
            deployment_record["phases"].append({
                "name": "deploy_canary",
                "status": "completed",
                "endpoint": canary_deployment["endpoint"],
                "timestamp": datetime.utcnow().isoformat()
            })
            
            # Phase 2: Progressive traffic rollout
            logger.info("Phase 2: Progressive traffic rollout...")
            deployment_record["status"] = DeploymentStatus.DEPLOYING.value
            
            traffic_increments = [strategy_config["initial_traffic"]] + strategy_config["traffic_increments"]
            
            for i, traffic_percent in enumerate(traffic_increments):
                logger.info(f"Setting canary traffic to {traffic_percent}%")
                
                # Update traffic routing
                traffic_result = await self._set_canary_traffic(config, traffic_percent)
                if not traffic_result["success"]:
                    await self._rollback_canary(config)
                    return {"success": False, "error": f"Traffic routing failed at {traffic_percent}%"}
                
                # Monitor for stage duration
                stage_duration = strategy_config["stage_duration"]
                monitoring_result = await self._monitor_canary_stage(config, stage_duration, traffic_percent)
                
                phase_info = {
                    "name": f"canary_stage_{i+1}",
                    "traffic_percent": traffic_percent,
                    "duration": stage_duration,
                    "metrics": monitoring_result["metrics"],
                    "status": "completed" if monitoring_result["success"] else "failed",
                    "timestamp": datetime.utcnow().isoformat()
                }
                
                deployment_record["phases"].append(phase_info)
                
                # Check if stage passed success criteria
                if not monitoring_result["success"]:
                    logger.warning(f"Canary stage failed at {traffic_percent}% traffic")
                    await self._rollback_canary(config)
                    return {
                        "success": False,
                        "error": f"Canary failed at {traffic_percent}% traffic",
                        "metrics": monitoring_result["metrics"]
                    }
                
                logger.info(f"âœ… Canary stage {i+1} successful at {traffic_percent}% traffic")
            
            # Phase 3: Full rollout
            logger.info("Phase 3: Full canary rollout...")
            
            full_rollout_result = await self._complete_canary_rollout(config)
            if not full_rollout_result["success"]:
                await self._rollback_canary(config)
                return {"success": False, "error": "Full rollout failed"}
            
            deployment_record["phases"].append({
                "name": "full_rollout",
                "status": "completed",
                "timestamp": datetime.utcnow().isoformat()
            })
            
            logger.info("âœ… Canary deployment completed successfully")
            return {
                "success": True,
                "strategy": "canary",
                "endpoint": canary_deployment["endpoint"],
                "stages_completed": len(traffic_increments),
                "final_traffic": 100
            }
            
        except Exception as e:
            logger.error(f"Canary deployment failed: {e}")
            await self._rollback_canary(config)
            return {"success": False, "error": str(e)}
    
    async def _execute_rolling_deployment(
        self,
        deployment_id: str,
        config: ModelDeploymentConfig
    ) -> Dict[str, Any]:
        """Execute rolling deployment strategy."""
        logger.info(f"ðŸ”„ Executing rolling deployment: {deployment_id}")
        
        deployment_record = self.active_deployments[deployment_id]
        
        try:
            # Phase 1: Build new version
            deployment_record["status"] = DeploymentStatus.BUILDING.value
            build_result = await self._build_model_container(config)
            
            if not build_result["success"]:
                return {"success": False, "error": "Container build failed"}
            
            # Phase 2: Rolling update
            logger.info("Phase 2: Executing rolling update...")
            deployment_record["status"] = DeploymentStatus.DEPLOYING.value
            
            rolling_update_result = await self._execute_rolling_update(config)
            
            deployment_record["phases"].append({
                "name": "rolling_update",
                "status": "completed" if rolling_update_result["success"] else "failed",
                "pods_updated": rolling_update_result.get("pods_updated", 0),
                "duration": rolling_update_result.get("duration", 0),
                "timestamp": datetime.utcnow().isoformat()
            })
            
            if not rolling_update_result["success"]:
                return {"success": False, "error": "Rolling update failed"}
            
            # Phase 3: Validation
            logger.info("Phase 3: Validating rolled deployment...")
            deployment_record["status"] = DeploymentStatus.TESTING.value
            
            validation_result = await self._validate_rolling_deployment(config)
            
            deployment_record["phases"].append({
                "name": "validation",
                "status": "completed" if validation_result["valid"] else "failed",
                "metrics": validation_result["metrics"],
                "timestamp": datetime.utcnow().isoformat()
            })
            
            success = rolling_update_result["success"] and validation_result["valid"]
            
            return {
                "success": success,
                "strategy": "rolling",
                "pods_updated": rolling_update_result.get("pods_updated", 0),
                "validation_metrics": validation_result["metrics"]
            }
            
        except Exception as e:
            logger.error(f"Rolling deployment failed: {e}")
            return {"success": False, "error": str(e)}
    
    async def _execute_ab_testing_deployment(
        self,
        deployment_id: str,
        config: ModelDeploymentConfig
    ) -> Dict[str, Any]:
        """Execute A/B testing deployment strategy."""
        logger.info(f"ðŸ§ª Executing A/B testing deployment: {deployment_id}")
        
        deployment_record = self.active_deployments[deployment_id]
        strategy_config = self.config["deployment"]["strategies"]["a_b_testing"]
        
        try:
            # Phase 1: Deploy variant B alongside variant A
            deployment_record["status"] = DeploymentStatus.BUILDING.value
            
            build_result = await self._build_model_container(config)
            if not build_result["success"]:
                return {"success": False, "error": "Container build failed"}
            
            variant_b_deployment = await self._deploy_kubernetes_service(config, "variant-b")
            if not variant_b_deployment["success"]:
                return {"success": False, "error": "Variant B deployment failed"}
            
            # Phase 2: Set up traffic splitting
            logger.info("Phase 2: Setting up A/B traffic splitting...")
            deployment_record["status"] = DeploymentStatus.DEPLOYING.value
            
            traffic_split_result = await self._setup_ab_traffic_split(config, strategy_config["traffic_split"])
            if not traffic_split_result["success"]:
                return {"success": False, "error": "A/B traffic setup failed"}
            
            deployment_record["phases"].append({
                "name": "traffic_split_setup",
                "status": "completed",
                "traffic_split": strategy_config["traffic_split"],
                "timestamp": datetime.utcnow().isoformat()
            })
            
            # Phase 3: Run A/B test
            logger.info("Phase 3: Running A/B test...")
            deployment_record["status"] = DeploymentStatus.TESTING.value
            
            ab_test_result = await self._run_ab_test(
                config,
                duration=strategy_config["test_duration"],
                confidence_level=strategy_config["confidence_level"],
                min_sample_size=strategy_config["minimum_sample_size"]
            )
            
            deployment_record["phases"].append({
                "name": "ab_testing",
                "status": "completed",
                "test_results": ab_test_result,
                "timestamp": datetime.utcnow().isoformat()
            })
            
            # Phase 4: Decide winner and full rollout
            if ab_test_result["significant"] and ab_test_result["winner"] == "variant_b":
                logger.info("Phase 4: Variant B won, rolling out fully...")
                
                full_rollout_result = await self._rollout_ab_winner(config, "variant_b")
                
                deployment_record["phases"].append({
                    "name": "winner_rollout",
                    "status": "completed" if full_rollout_result["success"] else "failed",
                    "winner": "variant_b",
                    "timestamp": datetime.utcnow().isoformat()
                })
                
                return {
                    "success": full_rollout_result["success"],
                    "strategy": "a_b_testing",
                    "winner": "variant_b",
                    "test_results": ab_test_result
                }
            else:
                logger.info("Phase 4: Variant A won or no significant difference, keeping current version...")
                
                # Clean up variant B
                await self._cleanup_ab_test(config, "variant_b")
                
                deployment_record["phases"].append({
                    "name": "cleanup_loser",
                    "status": "completed",
                    "winner": "variant_a",
                    "timestamp": datetime.utcnow().isoformat()
                })
                
                return {
                    "success": True,
                    "strategy": "a_b_testing",
                    "winner": "variant_a",
                    "test_results": ab_test_result,
                    "action": "kept_existing_version"
                }
            
        except Exception as e:
            logger.error(f"A/B testing deployment failed: {e}")
            return {"success": False, "error": str(e)}
    
    async def _execute_shadow_deployment(
        self,
        deployment_id: str,
        config: ModelDeploymentConfig
    ) -> Dict[str, Any]:
        """Execute shadow deployment strategy."""
        logger.info(f"ðŸ‘¥ Executing shadow deployment: {deployment_id}")
        
        deployment_record = self.active_deployments[deployment_id]
        strategy_config = self.config["deployment"]["strategies"]["shadow"]
        
        try:
            # Phase 1: Deploy shadow version
            deployment_record["status"] = DeploymentStatus.BUILDING.value
            
            build_result = await self._build_model_container(config)
            if not build_result["success"]:
                return {"success": False, "error": "Container build failed"}
            
            shadow_deployment = await self._deploy_kubernetes_service(config, "shadow")
            if not shadow_deployment["success"]:
                return {"success": False, "error": "Shadow deployment failed"}
            
            # Phase 2: Set up traffic mirroring
            logger.info("Phase 2: Setting up traffic mirroring...")
            deployment_record["status"] = DeploymentStatus.DEPLOYING.value
            
            mirroring_result = await self._setup_traffic_mirroring(config, strategy_config["shadow_traffic"])
            if not mirroring_result["success"]:
                return {"success": False, "error": "Traffic mirroring setup failed"}
            
            deployment_record["phases"].append({
                "name": "traffic_mirroring",
                "status": "completed",
                "shadow_traffic": strategy_config["shadow_traffic"],
                "timestamp": datetime.utcnow().isoformat()
            })
            
            # Phase 3: Run shadow test
            logger.info("Phase 3: Running shadow test...")
            deployment_record["status"] = DeploymentStatus.TESTING.value
            
            shadow_test_result = await self._run_shadow_test(
                config,
                duration=strategy_config["duration"],
                comparison_metrics=strategy_config["comparison_metrics"]
            )
            
            deployment_record["phases"].append({
                "name": "shadow_testing",
                "status": "completed",
                "test_results": shadow_test_result,
                "timestamp": datetime.utcnow().isoformat()
            })
            
            # Phase 4: Analysis and decision
            if shadow_test_result["recommendation"] == "promote":
                logger.info("Phase 4: Shadow version performed well, promoting...")
                
                promotion_result = await self._promote_shadow_deployment(config)
                
                deployment_record["phases"].append({
                    "name": "promotion",
                    "status": "completed" if promotion_result["success"] else "failed",
                    "timestamp": datetime.utcnow().isoformat()
                })
                
                return {
                    "success": promotion_result["success"],
                    "strategy": "shadow",
                    "action": "promoted",
                    "test_results": shadow_test_result
                }
            else:
                logger.info("Phase 4: Shadow version underperformed, cleaning up...")
                
                # Clean up shadow deployment
                await self._cleanup_shadow_deployment(config)
                
                deployment_record["phases"].append({
                    "name": "cleanup",
                    "status": "completed",
                    "timestamp": datetime.utcnow().isoformat()
                })
                
                return {
                    "success": True,
                    "strategy": "shadow",
                    "action": "rejected",
                    "test_results": shadow_test_result
                }
            
        except Exception as e:
            logger.error(f"Shadow deployment failed: {e}")
            return {"success": False, "error": str(e)}
    
    # Simulate deployment operations (in production, these would use real K8s/Docker APIs)
    
    async def _build_model_container(self, config: ModelDeploymentConfig) -> Dict[str, Any]:
        """Build container image for model."""
        logger.info(f"Building container for {config.model_id}:{config.version}")
        
        start_time = time.time()
        
        # Simulate container build process
        await asyncio.sleep(2)  # Simulate build time
        
        build_duration = time.time() - start_time
        
        return {
            "success": True,
            "image_tag": f"{config.model_id}:{config.version}",
            "image_size": "245MB",
            "duration": build_duration
        }
    
    async def _deploy_kubernetes_service(
        self,
        config: ModelDeploymentConfig,
        variant: str = "main"
    ) -> Dict[str, Any]:
        """Deploy service to Kubernetes."""
        deployment_name = f"{config.model_id}-{variant}"
        
        logger.info(f"Deploying {deployment_name} to Kubernetes")
        
        # Simulate Kubernetes deployment
        await asyncio.sleep(3)  # Simulate deployment time
        
        endpoint = f"https://{config.traffic_config['domain']}/{config.model_id}/{variant}"
        
        return {
            "success": True,
            "deployment_name": deployment_name,
            "endpoint": endpoint,
            "replicas": config.replicas
        }
    
    async def _run_health_checks(self, endpoint: str, config: ModelDeploymentConfig) -> Dict[str, Any]:
        """Run comprehensive health checks."""
        logger.info(f"Running health checks for {endpoint}")
        
        # Simulate health check process
        await asyncio.sleep(1)
        
        # Simulate health check results
        health_score = np.random.uniform(0.85, 0.99)
        healthy = health_score > 0.9
        
        return {
            "healthy": healthy,
            "score": health_score,
            "checks": {
                "liveness": True,
                "readiness": True,
                "model_loaded": True,
                "dependencies": True
            }
        }
    
    async def _switch_traffic(self, config: ModelDeploymentConfig, target_variant: str) -> Dict[str, Any]:
        """Switch traffic to target variant."""
        logger.info(f"Switching traffic to {target_variant}")
        
        # Simulate traffic switching
        await asyncio.sleep(1)
        
        return {
            "success": True,
            "target_variant": target_variant,
            "traffic_percentage": 100
        }
    
    async def _monitor_deployment(self, config: ModelDeploymentConfig, duration: int) -> Dict[str, Any]:
        """Monitor deployment performance."""
        logger.info(f"Monitoring deployment for {duration} seconds")
        
        # Simulate monitoring period
        await asyncio.sleep(min(duration, 5))  # Cap at 5 seconds for demo
        
        # Simulate metrics collection
        metrics = DeploymentMetrics(
            response_time_p95=np.random.uniform(50, 150),
            response_time_p99=np.random.uniform(100, 300),
            throughput_rps=np.random.uniform(100, 500),
            error_rate=np.random.uniform(0.001, 0.01),
            cpu_usage=np.random.uniform(30, 70),
            memory_usage=np.random.uniform(40, 80),
            success_rate=np.random.uniform(0.98, 0.999),
            availability=np.random.uniform(0.995, 1.0)
        )
        
        # Determine if deployment is stable
        stable = (
            metrics.error_rate < 0.01 and
            metrics.success_rate > 0.95 and
            metrics.response_time_p95 < 200
        )
        
        return {
            "stable": stable,
            "metrics": asdict(metrics),
            "duration": duration
        }
    
    async def _cleanup_old_deployments(self, config: ModelDeploymentConfig, keep_versions: int = 1):
        """Clean up old deployment versions."""
        logger.info(f"Cleaning up old deployments, keeping {keep_versions} versions")
        await asyncio.sleep(1)  # Simulate cleanup
    
    async def _set_canary_traffic(self, config: ModelDeploymentConfig, traffic_percent: int) -> Dict[str, Any]:
        """Set canary traffic percentage."""
        logger.info(f"Setting canary traffic to {traffic_percent}%")
        await asyncio.sleep(0.5)
        return {"success": True, "traffic_percent": traffic_percent}
    
    async def _monitor_canary_stage(
        self,
        config: ModelDeploymentConfig,
        duration: int,
        traffic_percent: int
    ) -> Dict[str, Any]:
        """Monitor canary stage performance."""
        logger.info(f"Monitoring canary stage at {traffic_percent}% for {duration}s")
        
        # Simulate monitoring
        await asyncio.sleep(min(duration, 3))  # Cap for demo
        
        # Simulate success/failure based on traffic percentage
        success_probability = 0.95 - (traffic_percent * 0.002)  # Lower success rate at higher traffic
        success = np.random.choice([True, False], p=[success_probability, 1 - success_probability])
        
        metrics = {
            "success_rate": np.random.uniform(0.95, 0.99) if success else np.random.uniform(0.85, 0.94),
            "error_rate": np.random.uniform(0.001, 0.01) if success else np.random.uniform(0.02, 0.05),
            "latency": np.random.uniform(80, 120) if success else np.random.uniform(150, 300)
        }
        
        return {
            "success": success,
            "metrics": metrics,
            "traffic_percent": traffic_percent
        }
    
    async def _rollback_canary(self, config: ModelDeploymentConfig):
        """Rollback canary deployment."""
        logger.info("Rolling back canary deployment")
        await asyncio.sleep(2)  # Simulate rollback
    
    async def _complete_canary_rollout(self, config: ModelDeploymentConfig) -> Dict[str, Any]:
        """Complete canary rollout to 100%."""
        logger.info("Completing canary rollout to 100%")
        await asyncio.sleep(2)
        return {"success": True}
    
    async def _execute_rolling_update(self, config: ModelDeploymentConfig) -> Dict[str, Any]:
        """Execute rolling update."""
        logger.info("Executing rolling update")
        await asyncio.sleep(3)  # Simulate rolling update
        return {
            "success": True,
            "pods_updated": config.replicas,
            "duration": 3
        }
    
    async def _validate_rolling_deployment(self, config: ModelDeploymentConfig) -> Dict[str, Any]:
        """Validate rolling deployment."""
        logger.info("Validating rolling deployment")
        await asyncio.sleep(1)
        
        metrics = {
            "pods_ready": config.replicas,
            "health_check_passed": True,
            "response_time": np.random.uniform(80, 120)
        }
        
        return {
            "valid": True,
            "metrics": metrics
        }
    
    async def _setup_ab_traffic_split(
        self,
        config: ModelDeploymentConfig,
        traffic_split: int
    ) -> Dict[str, Any]:
        """Set up A/B testing traffic split."""
        logger.info(f"Setting up A/B traffic split: {traffic_split}% variant B")
        await asyncio.sleep(1)
        return {"success": True, "split": traffic_split}
    
    async def _run_ab_test(
        self,
        config: ModelDeploymentConfig,
        duration: int,
        confidence_level: float,
        min_sample_size: int
    ) -> Dict[str, Any]:
        """Run A/B test."""
        logger.info(f"Running A/B test for {duration}s")
        
        # Simulate A/B test duration (capped for demo)
        await asyncio.sleep(min(duration, 5))
        
        # Simulate A/B test results
        variant_a_performance = np.random.uniform(0.85, 0.95)
        variant_b_performance = np.random.uniform(0.87, 0.97)  # Slightly better
        
        significant = abs(variant_b_performance - variant_a_performance) > 0.02
        winner = "variant_b" if variant_b_performance > variant_a_performance else "variant_a"
        
        return {
            "significant": significant,
            "winner": winner,
            "confidence": confidence_level,
            "sample_size": min_sample_size * 2,  # Simulate actual samples
            "variant_a_performance": variant_a_performance,
            "variant_b_performance": variant_b_performance,
            "improvement": abs(variant_b_performance - variant_a_performance)
        }
    
    async def _rollout_ab_winner(self, config: ModelDeploymentConfig, winner: str) -> Dict[str, Any]:
        """Roll out A/B test winner."""
        logger.info(f"Rolling out A/B winner: {winner}")
        await asyncio.sleep(2)
        return {"success": True, "winner": winner}
    
    async def _cleanup_ab_test(self, config: ModelDeploymentConfig, loser: str):
        """Clean up A/B test loser."""
        logger.info(f"Cleaning up A/B test loser: {loser}")
        await asyncio.sleep(1)
    
    async def _setup_traffic_mirroring(
        self,
        config: ModelDeploymentConfig,
        mirror_percentage: int
    ) -> Dict[str, Any]:
        """Set up traffic mirroring for shadow deployment."""
        logger.info(f"Setting up traffic mirroring: {mirror_percentage}%")
        await asyncio.sleep(1)
        return {"success": True, "mirror_percentage": mirror_percentage}
    
    async def _run_shadow_test(
        self,
        config: ModelDeploymentConfig,
        duration: int,
        comparison_metrics: List[str]
    ) -> Dict[str, Any]:
        """Run shadow deployment test."""
        logger.info(f"Running shadow test for {duration}s")
        
        # Simulate shadow test
        await asyncio.sleep(min(duration, 3))
        
        # Simulate comparison results
        shadow_better = np.random.choice([True, False], p=[0.7, 0.3])  # Shadow usually better
        
        metrics_comparison = {}
        for metric in comparison_metrics:
            production_value = np.random.uniform(0.8, 0.95)
            shadow_value = production_value * np.random.uniform(1.01, 1.1) if shadow_better else production_value * np.random.uniform(0.9, 0.99)
            
            metrics_comparison[metric] = {
                "production": production_value,
                "shadow": shadow_value,
                "improvement": shadow_value - production_value
            }
        
        recommendation = "promote" if shadow_better else "reject"
        
        return {
            "recommendation": recommendation,
            "metrics_comparison": metrics_comparison,
            "shadow_better": shadow_better
        }
    
    async def _promote_shadow_deployment(self, config: ModelDeploymentConfig) -> Dict[str, Any]:
        """Promote shadow deployment to production."""
        logger.info("Promoting shadow deployment to production")
        await asyncio.sleep(2)
        return {"success": True}
    
    async def _cleanup_shadow_deployment(self, config: ModelDeploymentConfig):
        """Clean up shadow deployment."""
        logger.info("Cleaning up shadow deployment")
        await asyncio.sleep(1)
    
    async def rollback_deployment(self, deployment_id: str, reason: str = "Manual rollback") -> bool:
        """Rollback a deployment."""
        logger.info(f"ðŸ”„ Rolling back deployment: {deployment_id}")
        
        if deployment_id not in self.active_deployments:
            logger.error(f"Deployment not found: {deployment_id}")
            return False
        
        deployment_record = self.active_deployments[deployment_id]
        
        try:
            deployment_record["status"] = DeploymentStatus.ROLLING_BACK.value
            deployment_record["rollback_info"] = {
                "reason": reason,
                "initiated_at": datetime.utcnow().isoformat(),
                "initiated_by": "system"
            }
            
            # Simulate rollback process
            await asyncio.sleep(2)
            
            deployment_record["status"] = DeploymentStatus.ROLLED_BACK.value
            deployment_record["rollback_info"]["completed_at"] = datetime.utcnow().isoformat()
            
            logger.info(f"âœ… Rollback completed: {deployment_id}")
            return True
            
        except Exception as e:
            logger.error(f"Rollback failed: {e}")
            deployment_record["rollback_info"]["error"] = str(e)
            return False
    
    def get_deployment_status(self, deployment_id: str) -> Optional[Dict[str, Any]]:
        """Get deployment status and details."""
        if deployment_id in self.active_deployments:
            return self.active_deployments[deployment_id]
        
        # Check history
        for deployment in self.deployment_history:
            if deployment["deployment_id"] == deployment_id:
                return deployment
        
        return None
    
    def list_active_deployments(self) -> List[Dict[str, Any]]:
        """List all active deployments."""
        return list(self.active_deployments.values())
    
    def generate_deployment_report(self) -> str:
        """Generate comprehensive deployment report."""
        logger.info("Generating deployment report")
        
        report_file = f"deployment-report-{datetime.now().strftime('%Y%m%d-%H%M%S')}.md"
        
        with open(report_file, 'w') as f:
            f.write(f"""# ðŸš€ ML Deployment System Report

**Generated:** {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')} UTC  
**System Version:** {self.config['deployment']['version']}  

## ðŸ“Š Deployment Statistics

### Active Deployments
- **Total Active:** {len(self.active_deployments)}

""")
            
            for deployment in self.active_deployments.values():
                f.write(f"""#### {deployment['deployment_id']}
- **Model:** {deployment['model_id']} v{deployment['version']}
- **Environment:** {deployment['environment']}
- **Strategy:** {deployment['strategy']}
- **Status:** {deployment['status']}
- **Created:** {deployment['created_at']}

""")
            
            f.write(f"""### Recent Deployments (Last 24 hours)
""")
            
            recent_deployments = [
                d for d in self.deployment_history
                if datetime.fromisoformat(d['created_at'].replace('Z', '+00:00')) > datetime.utcnow() - timedelta(days=1)
            ]
            
            f.write(f"- **Total Recent:** {len(recent_deployments)}\n")
            
            # Success rate calculation
            if recent_deployments:
                successful = len([d for d in recent_deployments if d.get('status') == 'active'])
                success_rate = (successful / len(recent_deployments)) * 100
                f.write(f"- **Success Rate:** {success_rate:.1f}%\n")
            
            f.write(f"""
### Deployment Strategies Usage
""")
            
            strategy_counts = {}
            for deployment in self.deployment_history:
                strategy = deployment.get('strategy', 'unknown')
                strategy_counts[strategy] = strategy_counts.get(strategy, 0) + 1
            
            for strategy, count in strategy_counts.items():
                f.write(f"- **{strategy.replace('_', ' ').title()}:** {count} deployments\n")
            
            f.write(f"""
## ðŸ—ï¸ Infrastructure Status

### Environments
""")
            
            for env_name, env_config in self.config["deployment"]["environments"].items():
                f.write(f"""#### {env_name.title()}
- **Namespace:** {env_config['kubernetes_namespace']}
- **Domain:** {env_config['domain']}
- **Default Strategy:** {env_config['default_strategy']}

""")
            
            f.write(f"""### Resource Configurations
""")
            
            for size, resources in self.config["deployment"]["resources"].items():
                f.write(f"""#### {size.title()}
- **CPU:** {resources['cpu']}
- **Memory:** {resources['memory']}
- **Replicas:** {resources['replicas']}

""")
            
            f.write(f"""## ðŸ“ˆ Performance Metrics

### Recent Deployment Performance
""")
            
            if recent_deployments:
                avg_phases = sum(len(d.get('phases', [])) for d in recent_deployments) / len(recent_deployments)
                f.write(f"- **Average Phases Completed:** {avg_phases:.1f}\n")
                
                # Calculate average deployment time
                completed_deployments = [d for d in recent_deployments if 'completed_at' in d]
                if completed_deployments:
                    total_duration = 0
                    for deployment in completed_deployments:
                        start = datetime.fromisoformat(deployment['created_at'].replace('Z', '+00:00'))
                        end = datetime.fromisoformat(deployment['completed_at'].replace('Z', '+00:00'))
                        total_duration += (end - start).total_seconds()
                    
                    avg_duration = total_duration / len(completed_deployments) / 60  # Convert to minutes
                    f.write(f"- **Average Deployment Time:** {avg_duration:.1f} minutes\n")
            
            f.write(f"""
## ðŸ”§ Configuration

### Auto-scaling Settings
- **Enabled:** {'âœ…' if self.config['deployment']['auto_scaling']['enabled'] else 'âŒ'}
- **Min Replicas:** {self.config['deployment']['auto_scaling']['min_replicas']}
- **Max Replicas:** {self.config['deployment']['auto_scaling']['max_replicas']}
- **CPU Threshold:** {self.config['deployment']['auto_scaling']['cpu_threshold']}%

### Security Features
- **Network Policies:** {'âœ…' if self.config['deployment']['security']['network_policies'] else 'âŒ'}
- **RBAC:** {'âœ…' if self.config['deployment']['security']['rbac_enabled'] else 'âŒ'}
- **Image Scanning:** {'âœ…' if self.config['deployment']['security']['image_scanning'] else 'âŒ'}

### Monitoring
- **Prometheus:** {'âœ…' if self.config['deployment']['monitoring']['prometheus_enabled'] else 'âŒ'}
- **Grafana Dashboard:** {'âœ…' if self.config['deployment']['monitoring']['grafana_dashboard'] else 'âŒ'}
- **Distributed Tracing:** {'âœ…' if self.config['deployment']['monitoring']['distributed_tracing'] else 'âŒ'}

## ðŸŽ¯ Recommendations

""")
            
            # Generate recommendations based on analysis
            if len(self.active_deployments) == 0:
                f.write("- **No Active Deployments:** Consider deploying models to utilize the monorepo\n")
            
            if recent_deployments:
                failed_deployments = [d for d in recent_deployments if d.get('status') == 'failed']
                if len(failed_deployments) > len(recent_deployments) * 0.1:
                    f.write("- **High Failure Rate:** Review deployment configurations and infrastructure health\n")
            
            f.write(f"""
## ðŸ“ž Support

For deployment issues:
1. Check deployment logs and status
2. Verify Kubernetes cluster health
3. Review configuration settings
4. Contact MLOps team for assistance

---
*This report was generated automatically by the Automated ML Deployment System*
""")
        
        logger.info(f"Deployment report generated: {report_file}")
        return report_file


async def main():
    """Main function for deployment system operations."""
    logger.info("ðŸš€ Automated ML Deployment System Starting...")
    
    try:
        # Initialize deployment system
        deployment_system = AutomatedMLDeployment()
        
        # Example deployment
        deployment_id = await deployment_system.deploy_model(
            model_id="anomaly_detector",
            version="v1.2.0",
            environment="staging",
            strategy=DeploymentStrategy.CANARY
        )
        
        # Get deployment status
        status = deployment_system.get_deployment_status(deployment_id)
        
        # Generate report
        report_file = deployment_system.generate_deployment_report()
        
        logger.info(f"âœ… Deployment system operations completed!")
        logger.info(f"ðŸ“Š Deployment ID: {deployment_id}")
        logger.info(f"ðŸ“‹ Report: {report_file}")
        
    except Exception as e:
        logger.error(f"ðŸ’¥ Fatal error in deployment system: {e}")
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main())