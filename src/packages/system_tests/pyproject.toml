[build-system]
requires = ["hatchling>=1.21.0"]
build-backend = "hatchling.build"

[project]
name = "system-tests"
version = "0.1.0"
description = "Comprehensive system-wide integration, performance, and security tests"
authors = [
    {name = "System Test Team", email = "system-tests@company.com"}
]
license = {text = "MIT"}
readme = "README.md"
requires-python = ">=3.11"
keywords = ["system-tests", "integration", "performance", "security", "e2e"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: 3.13",
    "Topic :: Software Development :: Testing",
    "Typing :: Typed",
]

# Production dependencies
dependencies = [
    "pytest>=7.4.4",
    "pytest-asyncio>=0.23.3",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.12.0",
    "pytest-benchmark>=4.0.0",
    "pytest-xdist>=3.5.0",
    "pytest-html>=4.1.1",
    "pytest-security>=0.2.0",
    "aiohttp>=3.9.1",
    "requests>=2.31.0",
    "locust>=2.17.0",
    "faker>=22.2.0",
    "factory-boy>=3.3.0",
]

[project.optional-dependencies]
# Development dependencies
dev = [
    "black>=24.0.0",
    "ruff>=0.8.0",
    "mypy>=1.13.0",
    "pre-commit>=4.0.0",
    "bandit>=1.7.10",
    "safety>=3.3.0",
]

# Testing extensions (beyond core dependencies)
test = [
    "pytest-html>=4.1.1",
    "pytest-security>=0.2.0",
    "pytest-timeout>=2.3.0",
    "pytest-randomly>=3.15.0",
    "testcontainers>=4.9.0",
    "selenium>=4.26.0",
    "playwright>=1.49.0",
]

# Documentation dependencies
docs = [
    "mkdocs>=1.6.0",
    "mkdocs-material>=9.5.0",
    "mkdocs-mermaid2-plugin>=1.3.0",
    "mkdocstrings[python]>=0.27.0",
    "mike>=2.1.0",
]

# Performance testing extensions
performance = [
    "artillery>=2.0.0",  # for advanced load testing
    "grafana-client>=3.5.0",  # for performance monitoring
    "influxdb-client>=1.45.0",  # for metrics storage
]

# Security testing extensions
security = [
    "bandit>=1.7.10",
    "safety>=3.3.0",
    "semgrep>=1.95.0",
    "pip-audit>=2.7.0",
    "owasp-zap>=0.1.0",
    "nuclei-python>=1.0.0",
]

# Monitoring and observability
monitoring = [
    "prometheus-client>=0.21.0",
    "opentelemetry-api>=1.29.0",
    "opentelemetry-sdk>=1.29.0",
    "grafana-client>=3.5.0",
    "jaeger-client>=4.8.0",
]

# All optional dependencies
all = [
    "system-tests[dev,test,docs,performance,security,monitoring]"
]

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.black]
line-length = 100
target-version = ['py311']

[tool.ruff]
line-length = 100
select = ["E", "F", "I", "N", "W", "D", "UP", "S", "B", "A", "C4", "T20", "SIM", "RET"]
ignore = ["D100", "D104", "D107", "D212", "D213"]

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true

[tool.pytest.ini_options]
addopts = "-ra -q --strict-markers --cov=system_tests --cov-report=term-missing --cov-report=xml --cov-report=html --html=reports/system_test_report.html --self-contained-html"
testpaths = ["integration", "performance", "security", "e2e"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
asyncio_mode = "auto"
markers = [
    "integration: Integration tests across multiple domains",
    "performance: Performance and load tests",
    "security: Security and vulnerability tests",
    "e2e: End-to-end workflow tests",
    "slow: Slow running tests (>30 seconds)",
    "stress: Stress tests that may impact system performance",
    "load: Load tests with multiple concurrent users",
    "benchmark: Performance benchmark tests",
    "vulnerability: Security vulnerability tests",
    "cross_domain: Tests spanning multiple domain boundaries",
    "enterprise: Enterprise-level integration tests",
    "data_workflow: Data processing workflow tests",
    "ml_workflow: Machine learning workflow tests",
    "auth_security: Authentication and authorization security tests",
    "data_protection: Data protection and privacy tests",
    "network_security: Network-level security tests",
    "injection: Injection attack protection tests",
    "compliance: Compliance and regulatory tests"
]
filterwarnings = [
    "ignore::DeprecationWarning",
    "ignore::PendingDeprecationWarning",
    "ignore::UserWarning"
]
timeout = 300  # 5 minutes default timeout for system tests

[tool.coverage.run]
source = ["../"]
omit = [
    "*/tests/*",
    "*/test_*",
    "*/__pycache__/*",
    "*/venv/*",
    "*/env/*",
    "*/.pytest_cache/*",
    "*/conftest.py"
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if self.debug:",
    "if settings.DEBUG",
    "raise AssertionError",
    "raise NotImplementedError",
    "if 0:",
    "if __name__ == .__main__.:",
    "class .*\\bProtocol\\):",
    "@(abc\\.)?abstractmethod"
]
fail_under = 95
precision = 2
show_missing = true
skip_covered = false
skip_empty = true

[tool.coverage.html]
directory = "htmlcov"
title = "System Tests Coverage Report"

[tool.coverage.xml]
output = "coverage.xml"

# Performance testing configuration
[tool.pytest_benchmark]
min_rounds = 5
max_time = 2.0
min_time = 0.000005
timer = "time.perf_counter"
calibration_precision = 10
warmup = false
warmup_iterations = 100000
disable_gc = false
json = "reports/benchmark.json"
histogram = true

# Security testing configuration
[tool.bandit]
exclude_dirs = ["tests", "venv", "build"]
skips = ["B101", "B601"]  # Skip assert_used and shell_injection_subprocess for tests