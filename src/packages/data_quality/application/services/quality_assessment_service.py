"""Quality Assessment Service.

Service for comprehensive data quality assessment including 6-dimensional scoring,
weighted scoring, issue detection, and quality trend analysis.
"""

import pandas as pd
import numpy as np
from typing import List, Dict, Any, Optional, Tuple
from datetime import datetime, timedelta
from dataclasses import dataclass, field
import logging
from collections import defaultdict
from enum import Enum

from ...domain.entities.quality_profile import DataQualityProfile, DatasetId, ProfileId, ProfileVersion
from ...domain.entities.quality_scores import QualityScores, QualityTrends, QualityTrendPoint, ScoringMethod
from ...domain.entities.quality_issue import (
    QualityIssue, BusinessImpact, RemediationSuggestion, 
    IssueId, SuggestionId, QualityIssueType, IssueStatus,
    ImpactLevel, ComplianceRisk, CustomerImpact, OperationalImpact,
    RemediationAction, EffortEstimate, Priority
)
from ...domain.entities.validation_rule import ValidationResult, ValidationStatus, Severity
from .validation_engine import ValidationEngine

logger = logging.getLogger(__name__)


class QualityDimension(Enum):
    """Quality dimensions for assessment."""
    COMPLETENESS = "completeness"
    ACCURACY = "accuracy"
    CONSISTENCY = "consistency"
    VALIDITY = "validity"
    UNIQUENESS = "uniqueness"
    TIMELINESS = "timeliness"


@dataclass(frozen=True)
class QualityAssessmentConfig:
    """Configuration for quality assessment."""
    scoring_method: ScoringMethod = ScoringMethod.WEIGHTED_AVERAGE
    dimension_weights: Dict[str, float] = field(default_factory=lambda: {
        'completeness': 0.20,
        'accuracy': 0.25,
        'consistency': 0.20,
        'validity': 0.20,
        'uniqueness': 0.10,
        'timeliness': 0.05
    })
    
    # Thresholds for issue detection
    critical_threshold: float = 0.5
    high_threshold: float = 0.7
    medium_threshold: float = 0.8
    low_threshold: float = 0.9
    
    # Analysis settings
    enable_trend_analysis: bool = True
    trend_period_days: int = 30
    enable_business_impact_analysis: bool = True
    enable_remediation_suggestions: bool = True
    
    # Performance settings
    sample_size_for_analysis: int = 50000
    max_issues_per_type: int = 1000
    
    def __post_init__(self):
        """Validate configuration."""
        # Validate weights sum to 1.0
        weight_sum = sum(self.dimension_weights.values())
        if abs(weight_sum - 1.0) > 0.001:
            raise ValueError(f"Dimension weights must sum to 1.0, got {weight_sum}")
        
        # Validate thresholds
        thresholds = [self.critical_threshold, self.high_threshold, self.medium_threshold, self.low_threshold]
        if not all(0.0 <= t <= 1.0 for t in thresholds):
            raise ValueError("All thresholds must be between 0 and 1")
        
        if not all(thresholds[i] <= thresholds[i+1] for i in range(len(thresholds)-1)):
            raise ValueError("Thresholds must be in ascending order")


@dataclass(frozen=True)
class DimensionAssessment:
    """Assessment for a single quality dimension."""
    dimension: QualityDimension
    score: float
    issue_count: int
    details: Dict[str, Any] = field(default_factory=dict)
    recommendations: List[str] = field(default_factory=list)
    
    def get_quality_level(self) -> str:
        """Get quality level based on score."""
        if self.score >= 0.9:
            return "excellent"
        elif self.score >= 0.8:
            return "good"
        elif self.score >= 0.7:
            return "fair"
        elif self.score >= 0.5:
            return "poor"
        else:
            return "critical"


class QualityAssessmentService:
    """Service for comprehensive quality assessment."""
    
    def __init__(self, config: QualityAssessmentConfig = None):
        """Initialize quality assessment service."""
        self.config = config or QualityAssessmentConfig()
        self.validation_engine = ValidationEngine()
        
        # Assessment statistics
        self._assessment_stats = {
            'total_assessments': 0,
            'average_overall_score': 0.0,
            'total_issues_detected': 0,
            'most_common_issue_type': None,
            'assessment_duration_seconds': 0.0
        }
    
    def assess_dataset_quality(self, 
                              df: pd.DataFrame,
                              validation_results: List[ValidationResult] = None,
                              dataset_id: DatasetId = None,
                              previous_profile: DataQualityProfile = None) -> DataQualityProfile:
        """Perform comprehensive quality assessment on dataset."""
        start_time = datetime.now()
        
        try:
            # Create dataset ID if not provided
            if not dataset_id:
                dataset_id = DatasetId(f"dataset_{int(datetime.now().timestamp())}")
            
            # Apply sampling if dataset is large
            if len(df) > self.config.sample_size_for_analysis:
                df_sample = df.sample(n=self.config.sample_size_for_analysis, random_state=42)
                logger.info(f"Applied sampling for assessment: {len(df_sample)} rows")
            else:
                df_sample = df
            
            # Assess individual quality dimensions
            dimension_assessments = self._assess_all_dimensions(df_sample)\n            \n            # Calculate overall quality scores\n            quality_scores = self._calculate_quality_scores(dimension_assessments)\n            \n            # Detect quality issues\n            quality_issues = self._detect_quality_issues(df_sample, dimension_assessments, validation_results)\n            \n            # Generate remediation suggestions\n            remediation_suggestions = self._generate_remediation_suggestions(quality_issues) if self.config.enable_remediation_suggestions else []\n            \n            # Analyze quality trends\n            quality_trends = self._analyze_quality_trends(quality_scores, previous_profile) if self.config.enable_trend_analysis else QualityTrends([])\n            \n            # Create quality profile\n            profile = DataQualityProfile(\n                profile_id=ProfileId(),\n                dataset_id=dataset_id,\n                quality_scores=quality_scores,\n                validation_results=validation_results or [],\n                quality_issues=quality_issues,\n                remediation_suggestions=remediation_suggestions,\n                quality_trends=quality_trends,\n                created_at=start_time,\n                last_assessed=datetime.now(),\n                version=ProfileVersion(),\n                record_count=len(df),\n                column_count=len(df.columns),\n                data_size_bytes=df.memory_usage(deep=True).sum()\n            )\n            \n            # Update statistics\n            self._update_assessment_stats(profile, start_time)\n            \n            logger.info(f\"Quality assessment completed for dataset {dataset_id}\")\n            return profile\n            \n        except Exception as e:\n            logger.error(f\"Quality assessment failed: {str(e)}\")\n            raise\n    \n    def compare_quality_profiles(self, \n                               profile1: DataQualityProfile,\n                               profile2: DataQualityProfile) -> Dict[str, Any]:\n        \"\"\"Compare two quality profiles.\"\"\"\n        comparison = {\n            'profile1_id': str(profile1.profile_id),\n            'profile2_id': str(profile2.profile_id),\n            'comparison_date': datetime.now().isoformat(),\n            'score_comparison': profile1.quality_scores.compare_with(profile2.quality_scores),\n            'issue_comparison': self._compare_issues(profile1.quality_issues, profile2.quality_issues),\n            'trend_analysis': self._compare_trends(profile1, profile2),\n            'recommendations': self._generate_comparison_recommendations(profile1, profile2)\n        }\n        \n        return comparison\n    \n    def calculate_business_impact(self, \n                                profile: DataQualityProfile,\n                                business_context: Dict[str, Any] = None) -> Dict[str, Any]:\n        \"\"\"Calculate business impact of quality issues.\"\"\"\n        if not self.config.enable_business_impact_analysis:\n            return {}\n        \n        business_context = business_context or {}\n        \n        # Calculate financial impact\n        financial_impact = self._calculate_financial_impact(profile, business_context)\n        \n        # Calculate operational impact\n        operational_impact = self._calculate_operational_impact(profile, business_context)\n        \n        # Calculate compliance impact\n        compliance_impact = self._calculate_compliance_impact(profile, business_context)\n        \n        # Calculate customer impact\n        customer_impact = self._calculate_customer_impact(profile, business_context)\n        \n        return {\n            'financial_impact': financial_impact,\n            'operational_impact': operational_impact,\n            'compliance_impact': compliance_impact,\n            'customer_impact': customer_impact,\n            'overall_business_impact': self._calculate_overall_business_impact(\n                financial_impact, operational_impact, compliance_impact, customer_impact\n            )\n        }\n    \n    def generate_quality_report(self, \n                              profile: DataQualityProfile,\n                              report_type: str = 'executive') -> Dict[str, Any]:\n        \"\"\"Generate comprehensive quality report.\"\"\"\n        if report_type == 'executive':\n            return self._generate_executive_report(profile)\n        elif report_type == 'detailed':\n            return self._generate_detailed_report(profile)\n        elif report_type == 'technical':\n            return self._generate_technical_report(profile)\n        else:\n            raise ValueError(f\"Unknown report type: {report_type}\")\n    \n    def _assess_all_dimensions(self, df: pd.DataFrame) -> Dict[QualityDimension, DimensionAssessment]:\n        \"\"\"Assess all quality dimensions.\"\"\"\n        assessments = {}\n        \n        # Completeness assessment\n        assessments[QualityDimension.COMPLETENESS] = self._assess_completeness(df)\n        \n        # Accuracy assessment\n        assessments[QualityDimension.ACCURACY] = self._assess_accuracy(df)\n        \n        # Consistency assessment\n        assessments[QualityDimension.CONSISTENCY] = self._assess_consistency(df)\n        \n        # Validity assessment\n        assessments[QualityDimension.VALIDITY] = self._assess_validity(df)\n        \n        # Uniqueness assessment\n        assessments[QualityDimension.UNIQUENESS] = self._assess_uniqueness(df)\n        \n        # Timeliness assessment\n        assessments[QualityDimension.TIMELINESS] = self._assess_timeliness(df)\n        \n        return assessments\n    \n    def _assess_completeness(self, df: pd.DataFrame) -> DimensionAssessment:\n        \"\"\"Assess data completeness.\"\"\"\n        total_cells = df.size\n        missing_cells = df.isnull().sum().sum()\n        completeness_score = (total_cells - missing_cells) / total_cells if total_cells > 0 else 0.0\n        \n        # Detailed analysis\n        column_completeness = {}\n        column_issues = []\n        \n        for column in df.columns:\n            col_completeness = df[column].notna().sum() / len(df)\n            column_completeness[column] = col_completeness\n            \n            if col_completeness < 0.9:\n                column_issues.append(f\"Column '{column}' has {col_completeness:.1%} completeness\")\n        \n        details = {\n            'total_cells': total_cells,\n            'missing_cells': missing_cells,\n            'completeness_rate': completeness_score,\n            'column_completeness': column_completeness,\n            'columns_with_issues': len(column_issues)\n        }\n        \n        recommendations = []\n        if completeness_score < 0.95:\n            recommendations.append(\"Investigate root causes of missing data\")\n            recommendations.append(\"Implement data validation at source systems\")\n        \n        if column_issues:\n            recommendations.append(\"Consider imputation strategies for incomplete columns\")\n        \n        return DimensionAssessment(\n            dimension=QualityDimension.COMPLETENESS,\n            score=completeness_score,\n            issue_count=len(column_issues),\n            details=details,\n            recommendations=recommendations\n        )\n    \n    def _assess_accuracy(self, df: pd.DataFrame) -> DimensionAssessment:\n        \"\"\"Assess data accuracy.\"\"\"\n        accuracy_issues = []\n        \n        # Check for impossible values\n        for column in df.select_dtypes(include=[np.number]).columns:\n            if 'age' in column.lower():\n                invalid_ages = ((df[column] < 0) | (df[column] > 150)).sum()\n                if invalid_ages > 0:\n                    accuracy_issues.append(f\"Column '{column}' has {invalid_ages} impossible age values\")\n            \n            elif 'year' in column.lower():\n                current_year = datetime.now().year\n                invalid_years = ((df[column] < 1900) | (df[column] > current_year + 10)).sum()\n                if invalid_years > 0:\n                    accuracy_issues.append(f\"Column '{column}' has {invalid_years} impossible year values\")\n        \n        # Check for format accuracy in text columns\n        for column in df.select_dtypes(include=['object']).columns:\n            if 'email' in column.lower():\n                email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n                invalid_emails = ~df[column].str.match(email_pattern, na=False)\n                if invalid_emails.sum() > 0:\n                    accuracy_issues.append(f\"Column '{column}' has {invalid_emails.sum()} invalid email formats\")\n            \n            elif 'phone' in column.lower():\n                # Simple phone validation\n                invalid_phones = ~df[column].str.match(r'^[\\d\\s\\-\\(\\)\\+]+$', na=False)\n                if invalid_phones.sum() > 0:\n                    accuracy_issues.append(f\"Column '{column}' has {invalid_phones.sum()} invalid phone formats\")\n        \n        # Calculate accuracy score\n        total_records = len(df)\n        accuracy_score = max(0.0, 1.0 - (len(accuracy_issues) / (total_records * 0.1))) if total_records > 0 else 0.0\n        \n        details = {\n            'total_records': total_records,\n            'accuracy_issues': accuracy_issues,\n            'accuracy_rate': accuracy_score\n        }\n        \n        recommendations = []\n        if accuracy_score < 0.9:\n            recommendations.append(\"Implement data validation rules at data entry points\")\n            recommendations.append(\"Set up automated data quality checks\")\n        \n        return DimensionAssessment(\n            dimension=QualityDimension.ACCURACY,\n            score=accuracy_score,\n            issue_count=len(accuracy_issues),\n            details=details,\n            recommendations=recommendations\n        )\n    \n    def _assess_consistency(self, df: pd.DataFrame) -> DimensionAssessment:\n        \"\"\"Assess data consistency.\"\"\"\n        consistency_issues = []\n        \n        # Check for inconsistent formats\n        for column in df.select_dtypes(include=['object']).columns:\n            unique_values = df[column].dropna().unique()\n            \n            # Check for mixed case inconsistencies\n            if len(unique_values) > 1:\n                lower_values = {str(v).lower() for v in unique_values}\n                if len(lower_values) < len(unique_values):\n                    consistency_issues.append(f\"Column '{column}' has mixed case inconsistencies\")\n            \n            # Check for date format inconsistencies\n            if 'date' in column.lower():\n                date_formats = set()\n                for value in unique_values[:100]:  # Sample first 100 values\n                    if pd.notna(value):\n                        str_value = str(value)\n                        if '/' in str_value:\n                            date_formats.add('slash')\n                        elif '-' in str_value:\n                            date_formats.add('dash')\n                        elif ' ' in str_value:\n                            date_formats.add('spaced')\n                \n                if len(date_formats) > 1:\n                    consistency_issues.append(f\"Column '{column}' has mixed date formats\")\n        \n        # Check for data type inconsistencies\n        for column in df.columns:\n            if df[column].dtype == 'object':\n                # Check if column could be numeric but has string values\n                non_null_values = df[column].dropna()\n                if len(non_null_values) > 0:\n                    numeric_count = 0\n                    for value in non_null_values:\n                        try:\n                            float(value)\n                            numeric_count += 1\n                        except (ValueError, TypeError):\n                            pass\n                    \n                    if 0 < numeric_count < len(non_null_values):\n                        consistency_issues.append(f\"Column '{column}' has mixed numeric/text values\")\n        \n        # Calculate consistency score\n        total_columns = len(df.columns)\n        consistency_score = max(0.0, 1.0 - (len(consistency_issues) / total_columns)) if total_columns > 0 else 0.0\n        \n        details = {\n            'total_columns': total_columns,\n            'consistency_issues': consistency_issues,\n            'consistency_rate': consistency_score\n        }\n        \n        recommendations = []\n        if consistency_score < 0.9:\n            recommendations.append(\"Standardize data formats across all sources\")\n            recommendations.append(\"Implement data transformation rules\")\n        \n        return DimensionAssessment(\n            dimension=QualityDimension.CONSISTENCY,\n            score=consistency_score,\n            issue_count=len(consistency_issues),\n            details=details,\n            recommendations=recommendations\n        )\n    \n    def _assess_validity(self, df: pd.DataFrame) -> DimensionAssessment:\n        \"\"\"Assess data validity.\"\"\"\n        validity_issues = []\n        \n        # Check for values outside valid ranges\n        for column in df.select_dtypes(include=[np.number]).columns:\n            # Check for outliers using IQR method\n            Q1 = df[column].quantile(0.25)\n            Q3 = df[column].quantile(0.75)\n            IQR = Q3 - Q1\n            \n            if IQR > 0:\n                lower_bound = Q1 - 1.5 * IQR\n                upper_bound = Q3 + 1.5 * IQR\n                \n                outliers = ((df[column] < lower_bound) | (df[column] > upper_bound)).sum()\n                if outliers > len(df) * 0.05:  # More than 5% outliers\n                    validity_issues.append(f\"Column '{column}' has {outliers} outlier values\")\n        \n        # Check for invalid categorical values\n        for column in df.select_dtypes(include=['object']).columns:\n            unique_count = df[column].nunique()\n            total_count = len(df)\n            \n            # If categorical column has too many unique values, it might be invalid\n            if unique_count > total_count * 0.5 and unique_count > 100:\n                validity_issues.append(f\"Column '{column}' may have invalid categorical values\")\n        \n        # Calculate validity score\n        total_columns = len(df.columns)\n        validity_score = max(0.0, 1.0 - (len(validity_issues) / total_columns)) if total_columns > 0 else 0.0\n        \n        details = {\n            'total_columns': total_columns,\n            'validity_issues': validity_issues,\n            'validity_rate': validity_score\n        }\n        \n        recommendations = []\n        if validity_score < 0.9:\n            recommendations.append(\"Define valid value ranges for all columns\")\n            recommendations.append(\"Implement business rule validation\")\n        \n        return DimensionAssessment(\n            dimension=QualityDimension.VALIDITY,\n            score=validity_score,\n            issue_count=len(validity_issues),\n            details=details,\n            recommendations=recommendations\n        )\n    \n    def _assess_uniqueness(self, df: pd.DataFrame) -> DimensionAssessment:\n        \"\"\"Assess data uniqueness.\"\"\"\n        uniqueness_issues = []\n        \n        # Check for duplicate rows\n        duplicate_rows = df.duplicated().sum()\n        if duplicate_rows > 0:\n            uniqueness_issues.append(f\"Dataset has {duplicate_rows} duplicate rows\")\n        \n        # Check for columns that should be unique\n        for column in df.columns:\n            if any(keyword in column.lower() for keyword in ['id', 'key', 'uuid', 'unique']):\n                duplicates = df[column].duplicated().sum()\n                if duplicates > 0:\n                    uniqueness_issues.append(f\"Column '{column}' has {duplicates} duplicate values\")\n        \n        # Calculate uniqueness score\n        total_records = len(df)\n        uniqueness_score = max(0.0, 1.0 - (duplicate_rows / total_records)) if total_records > 0 else 0.0\n        \n        details = {\n            'total_records': total_records,\n            'duplicate_rows': duplicate_rows,\n            'uniqueness_issues': uniqueness_issues,\n            'uniqueness_rate': uniqueness_score\n        }\n        \n        recommendations = []\n        if uniqueness_score < 0.95:\n            recommendations.append(\"Implement duplicate detection and removal processes\")\n            recommendations.append(\"Define unique constraints for key columns\")\n        \n        return DimensionAssessment(\n            dimension=QualityDimension.UNIQUENESS,\n            score=uniqueness_score,\n            issue_count=len(uniqueness_issues),\n            details=details,\n            recommendations=recommendations\n        )\n    \n    def _assess_timeliness(self, df: pd.DataFrame) -> DimensionAssessment:\n        \"\"\"Assess data timeliness.\"\"\"\n        timeliness_issues = []\n        \n        # Check for date/time columns\n        datetime_columns = df.select_dtypes(include=['datetime64']).columns\n        \n        for column in datetime_columns:\n            # Check for future dates\n            future_dates = (df[column] > datetime.now()).sum()\n            if future_dates > 0:\n                timeliness_issues.append(f\"Column '{column}' has {future_dates} future dates\")\n            \n            # Check for very old dates\n            very_old_dates = (df[column] < datetime.now() - timedelta(days=365*10)).sum()\n            if very_old_dates > len(df) * 0.1:  # More than 10% very old\n                timeliness_issues.append(f\"Column '{column}' has {very_old_dates} very old dates\")\n        \n        # Check for string columns that might contain dates\n        for column in df.select_dtypes(include=['object']).columns:\n            if 'date' in column.lower() or 'time' in column.lower():\n                # Try to parse as dates\n                try:\n                    parsed_dates = pd.to_datetime(df[column], errors='coerce')\n                    future_dates = (parsed_dates > datetime.now()).sum()\n                    if future_dates > 0:\n                        timeliness_issues.append(f\"Column '{column}' has {future_dates} future dates\")\n                except Exception:\n                    pass\n        \n        # Calculate timeliness score\n        total_records = len(df)\n        timeliness_score = max(0.0, 1.0 - (len(timeliness_issues) / 10)) if total_records > 0 else 1.0\n        \n        details = {\n            'datetime_columns': list(datetime_columns),\n            'timeliness_issues': timeliness_issues,\n            'timeliness_rate': timeliness_score\n        }\n        \n        recommendations = []\n        if timeliness_score < 0.9:\n            recommendations.append(\"Implement data freshness monitoring\")\n            recommendations.append(\"Set up automated data refresh schedules\")\n        \n        return DimensionAssessment(\n            dimension=QualityDimension.TIMELINESS,\n            score=timeliness_score,\n            issue_count=len(timeliness_issues),\n            details=details,\n            recommendations=recommendations\n        )\n    \n    def _calculate_quality_scores(self, assessments: Dict[QualityDimension, DimensionAssessment]) -> QualityScores:\n        \"\"\"Calculate overall quality scores.\"\"\"\n        dimension_scores = {}\n        \n        for dimension, assessment in assessments.items():\n            dimension_scores[dimension.value] = assessment.score\n        \n        # Calculate weighted overall score\n        overall_score = sum(\n            score * self.config.dimension_weights.get(dimension, 0.0)\n            for dimension, score in dimension_scores.items()\n        )\n        \n        return QualityScores(\n            overall_score=overall_score,\n            completeness_score=dimension_scores.get('completeness', 0.0),\n            accuracy_score=dimension_scores.get('accuracy', 0.0),\n            consistency_score=dimension_scores.get('consistency', 0.0),\n            validity_score=dimension_scores.get('validity', 0.0),\n            uniqueness_score=dimension_scores.get('uniqueness', 0.0),\n            timeliness_score=dimension_scores.get('timeliness', 0.0),\n            scoring_method=self.config.scoring_method,\n            weight_configuration=self.config.dimension_weights\n        )\n    \n    def _detect_quality_issues(self, \n                             df: pd.DataFrame,\n                             assessments: Dict[QualityDimension, DimensionAssessment],\n                             validation_results: List[ValidationResult] = None) -> List[QualityIssue]:\n        \"\"\"Detect quality issues based on assessments.\"\"\"\n        issues = []\n        \n        # Convert dimension assessments to quality issues\n        for dimension, assessment in assessments.items():\n            if assessment.score < self.config.medium_threshold:\n                severity = self._determine_severity(assessment.score)\n                issue_type = self._map_dimension_to_issue_type(dimension)\n                \n                # Create business impact assessment\n                business_impact = self._assess_business_impact(dimension, assessment.score)\n                \n                issue = QualityIssue(\n                    issue_id=IssueId(),\n                    issue_type=issue_type,\n                    severity=severity,\n                    description=f\"{dimension.value.title()} quality issue detected\",\n                    affected_records=int(len(df) * (1 - assessment.score)),\n                    affected_columns=self._get_affected_columns(df, dimension),\n                    root_cause=self._determine_root_cause(dimension, assessment),\n                    business_impact=business_impact,\n                    remediation_effort=self._estimate_remediation_effort(assessment.score),\n                    status=IssueStatus.OPEN,\n                    detected_at=datetime.now()\n                )\n                \n                issues.append(issue)\n        \n        # Convert validation results to quality issues\n        if validation_results:\n            for result in validation_results:\n                if result.status in [ValidationStatus.FAILED, ValidationStatus.ERROR]:\n                    issue = QualityIssue(\n                        issue_id=IssueId(),\n                        issue_type=QualityIssueType.BUSINESS_RULE_VIOLATION,\n                        severity=Severity.HIGH,\n                        description=f\"Validation rule failed\",\n                        affected_records=result.failed_records,\n                        affected_columns=[],\n                        root_cause=\"Business rule validation failure\",\n                        business_impact=BusinessImpact(\n                            impact_level=ImpactLevel.HIGH,\n                            affected_processes=[\"data_validation\"]\n                        ),\n                        remediation_effort=EffortEstimate.MODERATE,\n                        status=IssueStatus.OPEN,\n                        detected_at=datetime.now(),\n                        rule_id=result.rule_id\n                    )\n                    \n                    issues.append(issue)\n        \n        return issues[:self.config.max_issues_per_type]  # Limit number of issues\n    \n    def _determine_severity(self, score: float) -> Severity:\n        \"\"\"Determine severity based on score.\"\"\"\n        if score < self.config.critical_threshold:\n            return Severity.CRITICAL\n        elif score < self.config.high_threshold:\n            return Severity.HIGH\n        elif score < self.config.medium_threshold:\n            return Severity.MEDIUM\n        else:\n            return Severity.LOW\n    \n    def _map_dimension_to_issue_type(self, dimension: QualityDimension) -> QualityIssueType:\n        \"\"\"Map quality dimension to issue type.\"\"\"\n        mapping = {\n            QualityDimension.COMPLETENESS: QualityIssueType.COMPLETENESS_ISSUE,\n            QualityDimension.ACCURACY: QualityIssueType.ACCURACY_ISSUE,\n            QualityDimension.CONSISTENCY: QualityIssueType.INCONSISTENT_DATA,\n            QualityDimension.VALIDITY: QualityIssueType.INVALID_FORMAT,\n            QualityDimension.UNIQUENESS: QualityIssueType.UNIQUENESS_VIOLATION,\n            QualityDimension.TIMELINESS: QualityIssueType.TIMELINESS_ISSUE\n        }\n        return mapping.get(dimension, QualityIssueType.INCONSISTENT_DATA)\n    \n    def _assess_business_impact(self, dimension: QualityDimension, score: float) -> BusinessImpact:\n        \"\"\"Assess business impact based on dimension and score.\"\"\"\n        if score < self.config.critical_threshold:\n            impact_level = ImpactLevel.CRITICAL\n            compliance_risk = ComplianceRisk.SEVERE\n            customer_impact = CustomerImpact.SERVICE_DISRUPTION\n        elif score < self.config.high_threshold:\n            impact_level = ImpactLevel.HIGH\n            compliance_risk = ComplianceRisk.MODERATE\n            customer_impact = CustomerImpact.DEGRADED_EXPERIENCE\n        elif score < self.config.medium_threshold:\n            impact_level = ImpactLevel.MEDIUM\n            compliance_risk = ComplianceRisk.MINOR\n            customer_impact = CustomerImpact.MINOR_INCONVENIENCE\n        else:\n            impact_level = ImpactLevel.LOW\n            compliance_risk = ComplianceRisk.NONE\n            customer_impact = CustomerImpact.NO_IMPACT\n        \n        return BusinessImpact(\n            impact_level=impact_level,\n            affected_processes=[f\"{dimension.value}_dependent_processes\"],\n            compliance_risk=compliance_risk,\n            customer_impact=customer_impact,\n            operational_impact=OperationalImpact.PROCESS_DISRUPTION if impact_level == ImpactLevel.CRITICAL else OperationalImpact.EFFICIENCY_REDUCTION\n        )\n    \n    def _get_affected_columns(self, df: pd.DataFrame, dimension: QualityDimension) -> List[str]:\n        \"\"\"Get columns affected by dimension issue.\"\"\"\n        # This is a simplified implementation\n        # In practice, you'd analyze the specific issues in each dimension\n        if dimension == QualityDimension.COMPLETENESS:\n            return [col for col in df.columns if df[col].isnull().sum() > 0]\n        elif dimension == QualityDimension.UNIQUENESS:\n            return [col for col in df.columns if df[col].duplicated().sum() > 0]\n        else:\n            return list(df.columns)\n    \n    def _determine_root_cause(self, dimension: QualityDimension, assessment: DimensionAssessment) -> str:\n        \"\"\"Determine root cause of quality issue.\"\"\"\n        root_causes = {\n            QualityDimension.COMPLETENESS: \"Missing data at source or during ETL processing\",\n            QualityDimension.ACCURACY: \"Data entry errors or system integration issues\",\n            QualityDimension.CONSISTENCY: \"Lack of data standards or format inconsistencies\",\n            QualityDimension.VALIDITY: \"Invalid business rules or data constraints\",\n            QualityDimension.UNIQUENESS: \"Duplicate data sources or inadequate deduplication\",\n            QualityDimension.TIMELINESS: \"Data processing delays or outdated information\"\n        }\n        \n        return root_causes.get(dimension, \"Unknown root cause\")\n    \n    def _estimate_remediation_effort(self, score: float) -> EffortEstimate:\n        \"\"\"Estimate remediation effort based on score.\"\"\"\n        if score < 0.3:\n            return EffortEstimate.EXTENSIVE\n        elif score < 0.5:\n            return EffortEstimate.MAJOR\n        elif score < 0.7:\n            return EffortEstimate.MODERATE\n        elif score < 0.9:\n            return EffortEstimate.MINOR\n        else:\n            return EffortEstimate.TRIVIAL\n    \n    def _generate_remediation_suggestions(self, issues: List[QualityIssue]) -> List[RemediationSuggestion]:\n        \"\"\"Generate remediation suggestions for quality issues.\"\"\"\n        suggestions = []\n        \n        for issue in issues:\n            suggestion = RemediationSuggestion(\n                suggestion_id=SuggestionId(),\n                issue_id=issue.issue_id,\n                action_type=self._determine_remediation_action(issue),\n                description=self._generate_remediation_description(issue),\n                implementation_steps=self._generate_implementation_steps(issue),\n                effort_estimate=issue.remediation_effort,\n                success_probability=self._estimate_success_probability(issue),\n                side_effects=self._identify_side_effects(issue),\n                priority=self._determine_priority(issue)\n            )\n            \n            suggestions.append(suggestion)\n        \n        return suggestions\n    \n    def _determine_remediation_action(self, issue: QualityIssue) -> RemediationAction:\n        \"\"\"Determine appropriate remediation action.\"\"\"\n        action_mapping = {\n            QualityIssueType.COMPLETENESS_ISSUE: RemediationAction.DATA_CLEANSING,\n            QualityIssueType.ACCURACY_ISSUE: RemediationAction.DATA_CLEANSING,\n            QualityIssueType.INCONSISTENT_DATA: RemediationAction.DATA_CLEANSING,\n            QualityIssueType.INVALID_FORMAT: RemediationAction.DATA_CLEANSING,\n            QualityIssueType.UNIQUENESS_VIOLATION: RemediationAction.DATA_CLEANSING,\n            QualityIssueType.TIMELINESS_ISSUE: RemediationAction.PROCESS_CHANGE,\n            QualityIssueType.BUSINESS_RULE_VIOLATION: RemediationAction.RULE_MODIFICATION\n        }\n        \n        return action_mapping.get(issue.issue_type, RemediationAction.DATA_CLEANSING)\n    \n    def _generate_remediation_description(self, issue: QualityIssue) -> str:\n        \"\"\"Generate remediation description.\"\"\"\n        descriptions = {\n            QualityIssueType.COMPLETENESS_ISSUE: \"Implement data imputation or require complete data at source\",\n            QualityIssueType.ACCURACY_ISSUE: \"Validate and correct inaccurate data values\",\n            QualityIssueType.INCONSISTENT_DATA: \"Standardize data formats and values\",\n            QualityIssueType.INVALID_FORMAT: \"Apply format validation and transformation rules\",\n            QualityIssueType.UNIQUENESS_VIOLATION: \"Implement duplicate detection and removal\",\n            QualityIssueType.TIMELINESS_ISSUE: \"Establish data refresh schedules and monitoring\",\n            QualityIssueType.BUSINESS_RULE_VIOLATION: \"Review and update business rule definitions\"\n        }\n        \n        return descriptions.get(issue.issue_type, \"Apply appropriate data quality remediation\")\n    \n    def _generate_implementation_steps(self, issue: QualityIssue) -> List[str]:\n        \"\"\"Generate implementation steps for remediation.\"\"\"\n        base_steps = [\n            \"Analyze root cause of quality issue\",\n            \"Design remediation approach\",\n            \"Implement remediation solution\",\n            \"Test and validate improvements\",\n            \"Deploy to production\",\n            \"Monitor results and effectiveness\"\n        ]\n        \n        return base_steps\n    \n    def _estimate_success_probability(self, issue: QualityIssue) -> float:\n        \"\"\"Estimate success probability of remediation.\"\"\"\n        # Simple estimation based on issue severity and type\n        if issue.severity == Severity.LOW:\n            return 0.9\n        elif issue.severity == Severity.MEDIUM:\n            return 0.8\n        elif issue.severity == Severity.HIGH:\n            return 0.7\n        else:\n            return 0.6\n    \n    def _identify_side_effects(self, issue: QualityIssue) -> List[str]:\n        \"\"\"Identify potential side effects of remediation.\"\"\"\n        side_effects = [\n            \"Potential data loss during cleansing\",\n            \"Processing time increase\",\n            \"Temporary data inconsistency\"\n        ]\n        \n        return side_effects\n    \n    def _determine_priority(self, issue: QualityIssue) -> Priority:\n        \"\"\"Determine priority based on issue characteristics.\"\"\"\n        if issue.severity == Severity.CRITICAL:\n            return Priority.IMMEDIATE\n        elif issue.severity == Severity.HIGH:\n            return Priority.HIGH\n        elif issue.severity == Severity.MEDIUM:\n            return Priority.MEDIUM\n        else:\n            return Priority.LOW\n    \n    def _analyze_quality_trends(self, \n                              current_scores: QualityScores,\n                              previous_profile: DataQualityProfile = None) -> QualityTrends:\n        \"\"\"Analyze quality trends.\"\"\"\n        trend_points = []\n        \n        # Add current trend point\n        current_point = QualityTrendPoint(\n            timestamp=datetime.now(),\n            overall_score=current_scores.overall_score,\n            dimension_scores=current_scores.get_dimension_scores()\n        )\n        trend_points.append(current_point)\n        \n        # Add previous trend points if available\n        if previous_profile:\n            previous_point = QualityTrendPoint(\n                timestamp=previous_profile.last_assessed,\n                overall_score=previous_profile.quality_scores.overall_score,\n                dimension_scores=previous_profile.quality_scores.get_dimension_scores()\n            )\n            trend_points.append(previous_point)\n        \n        return QualityTrends(\n            trend_points=trend_points,\n            trend_period_days=self.config.trend_period_days\n        )\n    \n    def _compare_issues(self, issues1: List[QualityIssue], issues2: List[QualityIssue]) -> Dict[str, Any]:\n        \"\"\"Compare quality issues between two profiles.\"\"\"\n        issue_types1 = [issue.issue_type for issue in issues1]\n        issue_types2 = [issue.issue_type for issue in issues2]\n        \n        return {\n            'total_issues_1': len(issues1),\n            'total_issues_2': len(issues2),\n            'issue_change': len(issues2) - len(issues1),\n            'new_issue_types': list(set(issue_types2) - set(issue_types1)),\n            'resolved_issue_types': list(set(issue_types1) - set(issue_types2))\n        }\n    \n    def _compare_trends(self, profile1: DataQualityProfile, profile2: DataQualityProfile) -> Dict[str, Any]:\n        \"\"\"Compare quality trends between profiles.\"\"\"\n        return {\n            'trend_direction_1': profile1.quality_trends.trend_direction,\n            'trend_direction_2': profile2.quality_trends.trend_direction,\n            'trend_strength_1': profile1.quality_trends.trend_strength,\n            'trend_strength_2': profile2.quality_trends.trend_strength,\n            'quality_change': profile2.quality_scores.overall_score - profile1.quality_scores.overall_score\n        }\n    \n    def _generate_comparison_recommendations(self, \n                                          profile1: DataQualityProfile,\n                                          profile2: DataQualityProfile) -> List[str]:\n        \"\"\"Generate recommendations based on profile comparison.\"\"\"\n        recommendations = []\n        \n        score_diff = profile2.quality_scores.overall_score - profile1.quality_scores.overall_score\n        \n        if score_diff < -0.05:\n            recommendations.append(\"Quality has degraded significantly - investigate root causes\")\n        elif score_diff > 0.05:\n            recommendations.append(\"Quality has improved - consider replicating successful practices\")\n        \n        return recommendations\n    \n    def _calculate_financial_impact(self, profile: DataQualityProfile, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Calculate financial impact of quality issues.\"\"\"\n        # This is a simplified calculation - in practice, you'd use more sophisticated models\n        base_cost_per_record = context.get('cost_per_record', 1.0)\n        \n        total_affected_records = sum(issue.affected_records for issue in profile.quality_issues)\n        estimated_cost = total_affected_records * base_cost_per_record\n        \n        return {\n            'estimated_cost_usd': estimated_cost,\n            'affected_records': total_affected_records,\n            'cost_per_record': base_cost_per_record\n        }\n    \n    def _calculate_operational_impact(self, profile: DataQualityProfile, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Calculate operational impact of quality issues.\"\"\"\n        high_impact_issues = [issue for issue in profile.quality_issues \n                             if issue.business_impact.impact_level in [ImpactLevel.CRITICAL, ImpactLevel.HIGH]]\n        \n        return {\n            'high_impact_issues': len(high_impact_issues),\n            'affected_processes': len(set(process for issue in profile.quality_issues \n                                       for process in issue.business_impact.affected_processes)),\n            'estimated_downtime_hours': len(high_impact_issues) * 2  # Rough estimate\n        }\n    \n    def _calculate_compliance_impact(self, profile: DataQualityProfile, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Calculate compliance impact of quality issues.\"\"\"\n        severe_compliance_issues = [issue for issue in profile.quality_issues \n                                   if issue.business_impact.compliance_risk == ComplianceRisk.SEVERE]\n        \n        return {\n            'severe_compliance_issues': len(severe_compliance_issues),\n            'compliance_risk_level': 'high' if severe_compliance_issues else 'low',\n            'regulatory_exposure': context.get('regulatory_exposure', 'unknown')\n        }\n    \n    def _calculate_customer_impact(self, profile: DataQualityProfile, context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Calculate customer impact of quality issues.\"\"\"\n        customer_affecting_issues = [issue for issue in profile.quality_issues \n                                    if issue.business_impact.customer_impact != CustomerImpact.NO_IMPACT]\n        \n        return {\n            'customer_affecting_issues': len(customer_affecting_issues),\n            'potential_customer_churn': context.get('churn_risk_multiplier', 0.01) * len(customer_affecting_issues),\n            'customer_satisfaction_impact': 'high' if customer_affecting_issues else 'none'\n        }\n    \n    def _calculate_overall_business_impact(self, \n                                         financial: Dict[str, Any],\n                                         operational: Dict[str, Any],\n                                         compliance: Dict[str, Any],\n                                         customer: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Calculate overall business impact.\"\"\"\n        impact_score = 0.0\n        \n        # Weight different impact dimensions\n        if financial.get('estimated_cost_usd', 0) > 10000:\n            impact_score += 0.3\n        \n        if operational.get('high_impact_issues', 0) > 5:\n            impact_score += 0.3\n        \n        if compliance.get('compliance_risk_level') == 'high':\n            impact_score += 0.25\n        \n        if customer.get('customer_satisfaction_impact') == 'high':\n            impact_score += 0.15\n        \n        return {\n            'overall_impact_score': min(1.0, impact_score),\n            'impact_level': 'high' if impact_score > 0.7 else 'medium' if impact_score > 0.3 else 'low',\n            'requires_immediate_action': impact_score > 0.8\n        }\n    \n    def _generate_executive_report(self, profile: DataQualityProfile) -> Dict[str, Any]:\n        \"\"\"Generate executive-level quality report.\"\"\"\n        return {\n            'executive_summary': {\n                'overall_quality_score': profile.quality_scores.overall_score,\n                'quality_grade': profile.quality_scores.get_quality_grade(),\n                'quality_interpretation': profile.quality_scores.get_quality_interpretation(),\n                'total_issues': len(profile.quality_issues),\n                'critical_issues': len(profile.get_critical_issues()),\n                'trend_direction': profile.quality_trends.trend_direction\n            },\n            'key_findings': {\n                'lowest_dimension': profile.quality_scores.get_lowest_dimension(),\n                'highest_dimension': profile.quality_scores.get_highest_dimension(),\n                'main_concerns': [issue.description for issue in profile.get_critical_issues()]\n            },\n            'recommendations': {\n                'immediate_actions': [sugg.description for sugg in profile.remediation_suggestions \n                                    if sugg.priority == Priority.IMMEDIATE],\n                'strategic_improvements': [sugg.description for sugg in profile.remediation_suggestions \n                                         if sugg.priority in [Priority.HIGH, Priority.MEDIUM]]\n            }\n        }\n    \n    def _generate_detailed_report(self, profile: DataQualityProfile) -> Dict[str, Any]:\n        \"\"\"Generate detailed quality report.\"\"\"\n        return {\n            'profile_overview': profile.get_quality_summary(),\n            'quality_scores': {\n                'overall_score': profile.quality_scores.overall_score,\n                'dimension_scores': profile.quality_scores.get_dimension_scores(),\n                'scoring_method': profile.quality_scores.scoring_method.value,\n                'weights': profile.quality_scores.weight_configuration\n            },\n            'quality_issues': [issue.get_issue_summary() for issue in profile.quality_issues],\n            'remediation_suggestions': [sugg.get_suggestion_summary() for sugg in profile.remediation_suggestions],\n            'quality_trends': profile.quality_trends.get_trend_summary(),\n            'validation_results': [result.get_validation_summary() for result in profile.validation_results]\n        }\n    \n    def _generate_technical_report(self, profile: DataQualityProfile) -> Dict[str, Any]:\n        \"\"\"Generate technical quality report.\"\"\"\n        return {\n            'technical_details': {\n                'profile_id': str(profile.profile_id),\n                'dataset_id': str(profile.dataset_id),\n                'version': str(profile.version),\n                'created_at': profile.created_at.isoformat(),\n                'last_assessed': profile.last_assessed.isoformat(),\n                'record_count': profile.record_count,\n                'column_count': profile.column_count,\n                'data_size_bytes': profile.data_size_bytes\n            },\n            'assessment_configuration': {\n                'scoring_method': self.config.scoring_method.value,\n                'dimension_weights': self.config.dimension_weights,\n                'thresholds': {\n                    'critical': self.config.critical_threshold,\n                    'high': self.config.high_threshold,\n                    'medium': self.config.medium_threshold,\n                    'low': self.config.low_threshold\n                }\n            },\n            'detailed_analysis': {\n                'quality_scores': profile.quality_scores.get_dimension_scores(),\n                'issue_breakdown': self._get_issue_breakdown(profile.quality_issues),\n                'validation_summary': self._get_validation_summary(profile.validation_results)\n            }\n        }\n    \n    def _get_issue_breakdown(self, issues: List[QualityIssue]) -> Dict[str, Any]:\n        \"\"\"Get breakdown of quality issues.\"\"\"\n        breakdown = defaultdict(int)\n        \n        for issue in issues:\n            breakdown[issue.issue_type.value] += 1\n        \n        return dict(breakdown)\n    \n    def _get_validation_summary(self, results: List[ValidationResult]) -> Dict[str, Any]:\n        \"\"\"Get summary of validation results.\"\"\"\n        if not results:\n            return {}\n        \n        total_validations = len(results)\n        passed_validations = sum(1 for result in results if result.status == ValidationStatus.PASSED)\n        \n        return {\n            'total_validations': total_validations,\n            'passed_validations': passed_validations,\n            'success_rate': passed_validations / total_validations if total_validations > 0 else 0,\n            'total_records_validated': sum(result.passed_records + result.failed_records for result in results)\n        }\n    \n    def _update_assessment_stats(self, profile: DataQualityProfile, start_time: datetime) -> None:\n        \"\"\"Update assessment statistics.\"\"\"\n        self._assessment_stats['total_assessments'] += 1\n        \n        # Update average score\n        current_avg = self._assessment_stats['average_overall_score']\n        total_assessments = self._assessment_stats['total_assessments']\n        new_score = profile.quality_scores.overall_score\n        \n        self._assessment_stats['average_overall_score'] = (\n            (current_avg * (total_assessments - 1)) + new_score\n        ) / total_assessments\n        \n        # Update issue count\n        self._assessment_stats['total_issues_detected'] += len(profile.quality_issues)\n        \n        # Update most common issue type\n        if profile.quality_issues:\n            issue_types = [issue.issue_type.value for issue in profile.quality_issues]\n            most_common = max(set(issue_types), key=issue_types.count)\n            self._assessment_stats['most_common_issue_type'] = most_common\n        \n        # Update duration\n        duration = (datetime.now() - start_time).total_seconds()\n        self._assessment_stats['assessment_duration_seconds'] = duration\n    \n    def get_assessment_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get assessment service statistics.\"\"\"\n        return self._assessment_stats.copy()