"""Rule Management Service.

Service for managing quality rules including creation, validation, versioning,
testing, and deployment of data quality rules.
"""

import pandas as pd
from typing import List, Dict, Any, Optional, Union
from datetime import datetime
from dataclasses import dataclass, field
import logging
import json
import re
from enum import Enum

from ...domain.entities.validation_rule import (
    QualityRule, ValidationLogic, ValidationResult, ValidationError,
    RuleId, RuleType, LogicType, Severity, QualityCategory,
    SuccessCriteria, UserId
)
from ...domain.entities.quality_profile import DatasetId
from .validation_engine import ValidationEngine, ValidationEngineConfig

logger = logging.getLogger(__name__)


class RuleTestStatus(Enum):
    """Status of rule testing."""
    PENDING = "pending"
    RUNNING = "running"
    PASSED = "passed"
    FAILED = "failed"
    WARNING = "warning"
    ERROR = "error"


@dataclass(frozen=True)
class RuleTestResult:
    """Result of rule testing."""
    rule_id: RuleId
    test_status: RuleTestStatus
    test_dataset_size: int
    validation_result: Optional[ValidationResult] = None
    performance_metrics: Dict[str, Any] = field(default_factory=dict)
    recommendations: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    tested_at: datetime = field(default_factory=datetime.now)
    
    def is_successful(self) -> bool:
        """Check if test was successful."""
        return self.test_status == RuleTestStatus.PASSED
    
    def has_performance_issues(self) -> bool:
        """Check if test revealed performance issues."""
        return self.performance_metrics.get('execution_time_seconds', 0) > 30
    
    def get_test_summary(self) -> Dict[str, Any]:
        """Get test result summary."""
        return {
            'rule_id': str(self.rule_id),
            'test_status': self.test_status.value,
            'test_dataset_size': self.test_dataset_size,
            'is_successful': self.is_successful(),
            'has_performance_issues': self.has_performance_issues(),
            'execution_time_seconds': self.performance_metrics.get('execution_time_seconds', 0),
            'memory_usage_mb': self.performance_metrics.get('memory_usage_mb', 0),
            'recommendations_count': len(self.recommendations),
            'warnings_count': len(self.warnings),
            'tested_at': self.tested_at.isoformat()
        }


@dataclass(frozen=True)
class RuleTemplate:
    """Template for creating rules."""
    template_id: str
    template_name: str
    template_description: str
    rule_type: RuleType
    logic_type: LogicType
    expression_template: str
    parameter_definitions: Dict[str, Any] = field(default_factory=dict)
    examples: List[str] = field(default_factory=list)
    
    def create_rule(self, 
                   rule_name: str,
                   description: str,
                   parameters: Dict[str, Any],
                   created_by: UserId,
                   severity: Severity = Severity.MEDIUM,
                   category: QualityCategory = QualityCategory.DATA_INTEGRITY) -> QualityRule:
        """Create a rule from this template."""
        # Substitute parameters in expression template
        expression = self.expression_template
        for param_name, param_value in parameters.items():
            expression = expression.replace(f"{{{param_name}}}", str(param_value))
        
        # Create validation logic
        validation_logic = ValidationLogic(
            logic_type=self.logic_type,
            expression=expression,
            parameters=parameters,
            error_message=f"Rule validation failed: {rule_name}",
            success_criteria=SuccessCriteria()
        )
        
        return QualityRule(
            rule_id=RuleId(),
            rule_name=rule_name,
            rule_type=self.rule_type,
            description=description,
            validation_logic=validation_logic,
            severity=severity,
            category=category,
            is_active=True,
            created_by=created_by,
            created_at=datetime.now(),
            last_modified=datetime.now()
        )


class RuleManagementService:
    """Service for managing quality rules."""
    
    def __init__(self, validation_engine: ValidationEngine = None):
        """Initialize rule management service."""
        self.validation_engine = validation_engine or ValidationEngine()
        self._rule_templates = self._initialize_rule_templates()
        
        # Rule statistics
        self._rule_stats = {
            'total_rules_created': 0,
            'total_rules_tested': 0,
            'total_rules_deployed': 0,
            'average_test_time': 0.0,
            'test_success_rate': 0.0
        }
    
    def create_rule(self, 
                   rule_name: str,
                   rule_type: RuleType,
                   description: str,
                   validation_logic: ValidationLogic,
                   severity: Severity,
                   category: QualityCategory,
                   created_by: UserId,
                   target_tables: List[str] = None,
                   target_columns: List[str] = None,
                   tags: List[str] = None) -> QualityRule:
        """Create a new quality rule."""
        try:\n            # Validate rule inputs\n            self._validate_rule_inputs(rule_name, validation_logic)\n            \n            # Create rule entity\n            rule = QualityRule(\n                rule_id=RuleId(),\n                rule_name=rule_name,\n                rule_type=rule_type,\n                description=description,\n                validation_logic=validation_logic,\n                severity=severity,\n                category=category,\n                is_active=True,\n                created_by=created_by,\n                created_at=datetime.now(),\n                last_modified=datetime.now(),\n                target_tables=target_tables or [],\n                target_columns=target_columns or [],\n                tags=tags or []\n            )\n            \n            # Update statistics\n            self._rule_stats['total_rules_created'] += 1\n            \n            logger.info(f\"Created rule: {rule_name} ({rule.rule_id})\")\n            return rule\n            \n        except Exception as e:\n            logger.error(f\"Failed to create rule {rule_name}: {str(e)}\")\n            raise\n    \n    def create_rule_from_template(self, \n                                 template_id: str,\n                                 rule_name: str,\n                                 description: str,\n                                 parameters: Dict[str, Any],\n                                 created_by: UserId,\n                                 severity: Severity = Severity.MEDIUM,\n                                 category: QualityCategory = QualityCategory.DATA_INTEGRITY) -> QualityRule:\n        \"\"\"Create rule from template.\"\"\"\n        try:\n            template = self._rule_templates.get(template_id)\n            if not template:\n                raise ValueError(f\"Template not found: {template_id}\")\n            \n            # Validate parameters\n            self._validate_template_parameters(template, parameters)\n            \n            # Create rule from template\n            rule = template.create_rule(\n                rule_name=rule_name,\n                description=description,\n                parameters=parameters,\n                created_by=created_by,\n                severity=severity,\n                category=category\n            )\n            \n            self._rule_stats['total_rules_created'] += 1\n            \n            logger.info(f\"Created rule from template {template_id}: {rule_name}\")\n            return rule\n            \n        except Exception as e:\n            logger.error(f\"Failed to create rule from template {template_id}: {str(e)}\")\n            raise\n    \n    def test_rule(self, \n                 rule: QualityRule,\n                 test_dataset: pd.DataFrame,\n                 dataset_id: DatasetId = None) -> RuleTestResult:\n        \"\"\"Test a rule against a dataset.\"\"\"\n        try:\n            start_time = datetime.now()\n            \n            # Create dataset ID if not provided\n            if not dataset_id:\n                dataset_id = DatasetId(\"test_dataset\")\n            \n            # Execute validation\n            validation_result = self.validation_engine.validate_single_rule(\n                df=test_dataset,\n                rule=rule,\n                dataset_id=dataset_id\n            )\n            \n            # Calculate performance metrics\n            execution_time = (datetime.now() - start_time).total_seconds()\n            memory_usage = test_dataset.memory_usage(deep=True).sum() / 1024 / 1024  # MB\n            \n            performance_metrics = {\n                'execution_time_seconds': execution_time,\n                'memory_usage_mb': memory_usage,\n                'records_per_second': len(test_dataset) / execution_time if execution_time > 0 else 0\n            }\n            \n            # Determine test status\n            if validation_result.status.value == 'error':\n                test_status = RuleTestStatus.ERROR\n            elif validation_result.status.value == 'failed':\n                test_status = RuleTestStatus.FAILED\n            elif validation_result.status.value == 'warning':\n                test_status = RuleTestStatus.WARNING\n            else:\n                test_status = RuleTestStatus.PASSED\n            \n            # Generate recommendations\n            recommendations = self._generate_test_recommendations(rule, validation_result, performance_metrics)\n            \n            # Generate warnings\n            warnings = self._generate_test_warnings(rule, validation_result, performance_metrics)\n            \n            test_result = RuleTestResult(\n                rule_id=rule.rule_id,\n                test_status=test_status,\n                test_dataset_size=len(test_dataset),\n                validation_result=validation_result,\n                performance_metrics=performance_metrics,\n                recommendations=recommendations,\n                warnings=warnings\n            )\n            \n            # Update statistics\n            self._rule_stats['total_rules_tested'] += 1\n            self._update_test_statistics(test_result)\n            \n            logger.info(f\"Tested rule {rule.rule_name}: {test_status.value}\")\n            return test_result\n            \n        except Exception as e:\n            logger.error(f\"Failed to test rule {rule.rule_name}: {str(e)}\")\n            return RuleTestResult(\n                rule_id=rule.rule_id,\n                test_status=RuleTestStatus.ERROR,\n                test_dataset_size=len(test_dataset),\n                warnings=[f\"Test execution failed: {str(e)}\"]\n            )\n    \n    def batch_test_rules(self, \n                        rules: List[QualityRule],\n                        test_dataset: pd.DataFrame,\n                        dataset_id: DatasetId = None) -> List[RuleTestResult]:\n        \"\"\"Test multiple rules against a dataset.\"\"\"\n        results = []\n        \n        for rule in rules:\n            result = self.test_rule(rule, test_dataset, dataset_id)\n            results.append(result)\n        \n        return results\n    \n    def validate_rule_logic(self, validation_logic: ValidationLogic) -> Dict[str, Any]:\n        \"\"\"Validate rule logic without executing it.\"\"\"\n        validation_result = {\n            'is_valid': True,\n            'syntax_errors': [],\n            'warnings': [],\n            'recommendations': []\n        }\n        \n        try:\n            # Validate expression syntax\n            if validation_logic.logic_type == LogicType.PYTHON:\n                self._validate_python_syntax(validation_logic.expression, validation_result)\n            elif validation_logic.logic_type == LogicType.SQL:\n                self._validate_sql_syntax(validation_logic.expression, validation_result)\n            elif validation_logic.logic_type == LogicType.REGEX:\n                self._validate_regex_syntax(validation_logic.expression, validation_result)\n            \n            # Validate parameters\n            self._validate_logic_parameters(validation_logic, validation_result)\n            \n            # Generate recommendations\n            self._generate_logic_recommendations(validation_logic, validation_result)\n            \n        except Exception as e:\n            validation_result['is_valid'] = False\n            validation_result['syntax_errors'].append(f\"Validation failed: {str(e)}\")\n        \n        return validation_result\n    \n    def get_rule_templates(self) -> List[RuleTemplate]:\n        \"\"\"Get available rule templates.\"\"\"\n        return list(self._rule_templates.values())\n    \n    def get_rule_template(self, template_id: str) -> Optional[RuleTemplate]:\n        \"\"\"Get specific rule template.\"\"\"\n        return self._rule_templates.get(template_id)\n    \n    def simulate_rule_performance(self, \n                                 rule: QualityRule,\n                                 dataset_size: int,\n                                 column_count: int) -> Dict[str, Any]:\n        \"\"\"Simulate rule performance on different dataset sizes.\"\"\"\n        try:\n            # Basic performance estimation based on rule type and dataset characteristics\n            base_time_ms = self._estimate_base_execution_time(rule, dataset_size, column_count)\n            \n            # Estimate memory usage\n            memory_mb = self._estimate_memory_usage(rule, dataset_size, column_count)\n            \n            # Estimate scalability\n            scalability_factor = self._estimate_scalability_factor(rule)\n            \n            return {\n                'estimated_execution_time_ms': base_time_ms,\n                'estimated_memory_usage_mb': memory_mb,\n                'scalability_factor': scalability_factor,\n                'performance_category': self._categorize_performance(base_time_ms, memory_mb),\n                'recommendations': self._generate_performance_recommendations(rule, base_time_ms, memory_mb)\n            }\n            \n        except Exception as e:\n            logger.error(f\"Failed to simulate performance for rule {rule.rule_name}: {str(e)}\")\n            return {\n                'estimated_execution_time_ms': 0,\n                'estimated_memory_usage_mb': 0,\n                'scalability_factor': 1.0,\n                'performance_category': 'unknown',\n                'recommendations': [f\"Performance simulation failed: {str(e)}\"]\n            }\n    \n    def export_rules(self, rules: List[QualityRule], format: str = 'json') -> str:\n        \"\"\"Export rules to different formats.\"\"\"\n        try:\n            if format.lower() == 'json':\n                return self._export_rules_to_json(rules)\n            elif format.lower() == 'yaml':\n                return self._export_rules_to_yaml(rules)\n            elif format.lower() == 'csv':\n                return self._export_rules_to_csv(rules)\n            else:\n                raise ValueError(f\"Unsupported export format: {format}\")\n                \n        except Exception as e:\n            logger.error(f\"Failed to export rules: {str(e)}\")\n            raise\n    \n    def import_rules(self, rules_data: str, format: str = 'json', created_by: UserId = None) -> List[QualityRule]:\n        \"\"\"Import rules from different formats.\"\"\"\n        try:\n            if format.lower() == 'json':\n                return self._import_rules_from_json(rules_data, created_by)\n            elif format.lower() == 'yaml':\n                return self._import_rules_from_yaml(rules_data, created_by)\n            else:\n                raise ValueError(f\"Unsupported import format: {format}\")\n                \n        except Exception as e:\n            logger.error(f\"Failed to import rules: {str(e)}\")\n            raise\n    \n    def get_rule_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get rule management statistics.\"\"\"\n        return self._rule_stats.copy()\n    \n    def _validate_rule_inputs(self, rule_name: str, validation_logic: ValidationLogic) -> None:\n        \"\"\"Validate rule creation inputs.\"\"\"\n        if not rule_name.strip():\n            raise ValueError(\"Rule name cannot be empty\")\n        \n        if len(rule_name) > 100:\n            raise ValueError(\"Rule name cannot exceed 100 characters\")\n        \n        if not validation_logic.expression.strip():\n            raise ValueError(\"Validation expression cannot be empty\")\n        \n        if len(validation_logic.expression) > 10000:\n            raise ValueError(\"Validation expression cannot exceed 10000 characters\")\n    \n    def _validate_template_parameters(self, template: RuleTemplate, parameters: Dict[str, Any]) -> None:\n        \"\"\"Validate template parameters.\"\"\"\n        required_params = template.parameter_definitions.get('required', [])\n        \n        for param in required_params:\n            if param not in parameters:\n                raise ValueError(f\"Required parameter missing: {param}\")\n        \n        # Validate parameter types\n        param_types = template.parameter_definitions.get('types', {})\n        for param, value in parameters.items():\n            expected_type = param_types.get(param)\n            if expected_type and not isinstance(value, expected_type):\n                raise ValueError(f\"Parameter {param} must be of type {expected_type}\")\n    \n    def _validate_python_syntax(self, expression: str, result: Dict[str, Any]) -> None:\n        \"\"\"Validate Python expression syntax.\"\"\"\n        try:\n            compile(expression, '<string>', 'eval')\n        except SyntaxError as e:\n            result['is_valid'] = False\n            result['syntax_errors'].append(f\"Python syntax error: {str(e)}\")\n        except Exception as e:\n            result['warnings'].append(f\"Python validation warning: {str(e)}\")\n    \n    def _validate_sql_syntax(self, expression: str, result: Dict[str, Any]) -> None:\n        \"\"\"Validate SQL expression syntax.\"\"\"\n        # Basic SQL validation - in production, use proper SQL parser\n        sql_keywords = ['select', 'from', 'where', 'and', 'or', 'not', 'in', 'like']\n        \n        expr_lower = expression.lower()\n        \n        # Check for dangerous operations\n        dangerous_keywords = ['drop', 'delete', 'update', 'insert', 'create', 'alter']\n        for keyword in dangerous_keywords:\n            if keyword in expr_lower:\n                result['warnings'].append(f\"SQL contains potentially dangerous keyword: {keyword}\")\n        \n        # Basic syntax checks\n        if 'select' in expr_lower and 'from' not in expr_lower:\n            result['warnings'].append(\"SQL SELECT statement missing FROM clause\")\n    \n    def _validate_regex_syntax(self, expression: str, result: Dict[str, Any]) -> None:\n        \"\"\"Validate regex expression syntax.\"\"\"\n        try:\n            re.compile(expression)\n        except re.error as e:\n            result['is_valid'] = False\n            result['syntax_errors'].append(f\"Regex syntax error: {str(e)}\")\n    \n    def _validate_logic_parameters(self, logic: ValidationLogic, result: Dict[str, Any]) -> None:\n        \"\"\"Validate logic parameters.\"\"\"\n        # Check for required parameters based on logic type\n        if logic.logic_type == LogicType.REGEX:\n            if 'column_name' not in logic.parameters:\n                result['warnings'].append(\"Regex validation should specify column_name parameter\")\n        \n        elif logic.logic_type == LogicType.STATISTICAL:\n            if 'column_name' not in logic.parameters:\n                result['warnings'].append(\"Statistical validation should specify column_name parameter\")\n            \n            stat_type = logic.parameters.get('stat_type')\n            if stat_type == 'range':\n                if 'min_value' not in logic.parameters or 'max_value' not in logic.parameters:\n                    result['warnings'].append(\"Range validation should specify min_value and max_value parameters\")\n        \n        elif logic.logic_type == LogicType.COMPARISON:\n            if 'column1' not in logic.parameters or 'column2' not in logic.parameters:\n                result['warnings'].append(\"Comparison validation should specify column1 and column2 parameters\")\n    \n    def _generate_logic_recommendations(self, logic: ValidationLogic, result: Dict[str, Any]) -> None:\n        \"\"\"Generate recommendations for logic optimization.\"\"\"\n        # Performance recommendations\n        if logic.logic_type == LogicType.PYTHON and 'df.' in logic.expression:\n            result['recommendations'].append(\"Consider using vectorized operations instead of DataFrame operations for better performance\")\n        \n        if logic.logic_type == LogicType.SQL and 'like' in logic.expression.lower():\n            result['recommendations'].append(\"Consider using regex validation for better performance than SQL LIKE operations\")\n        \n        # Error message recommendations\n        if not logic.error_message or len(logic.error_message) < 10:\n            result['recommendations'].append(\"Consider providing more descriptive error messages\")\n    \n    def _generate_test_recommendations(self, \n                                     rule: QualityRule,\n                                     validation_result: ValidationResult,\n                                     performance_metrics: Dict[str, Any]) -> List[str]:\n        \"\"\"Generate recommendations based on test results.\"\"\"\n        recommendations = []\n        \n        # Performance recommendations\n        if performance_metrics.get('execution_time_seconds', 0) > 30:\n            recommendations.append(\"Consider optimizing rule logic for better performance\")\n        \n        # Failure rate recommendations\n        if validation_result.failure_rate > 0.5:\n            recommendations.append(\"High failure rate detected - consider adjusting rule criteria\")\n        \n        # Success criteria recommendations\n        if validation_result.failure_rate > 0.1 and rule.validation_logic.success_criteria.min_pass_rate == 1.0:\n            recommendations.append(\"Consider adjusting success criteria to allow for some acceptable failures\")\n        \n        return recommendations\n    \n    def _generate_test_warnings(self, \n                              rule: QualityRule,\n                              validation_result: ValidationResult,\n                              performance_metrics: Dict[str, Any]) -> List[str]:\n        \"\"\"Generate warnings based on test results.\"\"\"\n        warnings = []\n        \n        # Memory usage warnings\n        if performance_metrics.get('memory_usage_mb', 0) > 1000:\n            warnings.append(\"High memory usage detected - may cause issues with large datasets\")\n        \n        # Error count warnings\n        if len(validation_result.error_details) > 1000:\n            warnings.append(\"Large number of validation errors - consider optimizing rule logic\")\n        \n        return warnings\n    \n    def _estimate_base_execution_time(self, rule: QualityRule, dataset_size: int, column_count: int) -> float:\n        \"\"\"Estimate base execution time in milliseconds.\"\"\"\n        # Simple estimation based on rule type and dataset characteristics\n        base_time_per_record = {\n            LogicType.PYTHON: 0.01,\n            LogicType.SQL: 0.005,\n            LogicType.REGEX: 0.02,\n            LogicType.STATISTICAL: 0.001,\n            LogicType.COMPARISON: 0.001,\n            LogicType.AGGREGATION: 0.0001,\n            LogicType.LOOKUP: 0.001,\n            LogicType.CONDITIONAL: 0.005,\n            LogicType.EXPRESSION: 0.002\n        }\n        \n        base_time = base_time_per_record.get(rule.validation_logic.logic_type, 0.01)\n        \n        # Apply complexity factors\n        complexity_factor = 1.0\n        if len(rule.validation_logic.expression) > 1000:\n            complexity_factor *= 1.5\n        \n        if len(rule.target_columns) > 10:\n            complexity_factor *= 1.2\n        \n        return base_time * dataset_size * complexity_factor * 1000  # Convert to milliseconds\n    \n    def _estimate_memory_usage(self, rule: QualityRule, dataset_size: int, column_count: int) -> float:\n        \"\"\"Estimate memory usage in MB.\"\"\"\n        # Basic memory estimation\n        bytes_per_record = 100  # Average bytes per record\n        base_memory = (dataset_size * bytes_per_record) / (1024 * 1024)  # Convert to MB\n        \n        # Apply rule-specific factors\n        if rule.validation_logic.logic_type == LogicType.PYTHON:\n            base_memory *= 2.0  # Python operations typically use more memory\n        \n        return base_memory\n    \n    def _estimate_scalability_factor(self, rule: QualityRule) -> float:\n        \"\"\"Estimate scalability factor (1.0 = linear scaling).\"\"\"\n        if rule.validation_logic.logic_type == LogicType.AGGREGATION:\n            return 1.0  # Aggregations scale linearly\n        elif rule.validation_logic.logic_type == LogicType.PYTHON:\n            return 1.2  # Python operations may scale worse than linear\n        else:\n            return 1.0\n    \n    def _categorize_performance(self, execution_time_ms: float, memory_mb: float) -> str:\n        \"\"\"Categorize performance as fast, medium, or slow.\"\"\"\n        if execution_time_ms < 1000 and memory_mb < 100:\n            return 'fast'\n        elif execution_time_ms < 10000 and memory_mb < 500:\n            return 'medium'\n        else:\n            return 'slow'\n    \n    def _generate_performance_recommendations(self, \n                                            rule: QualityRule,\n                                            execution_time_ms: float,\n                                            memory_mb: float) -> List[str]:\n        \"\"\"Generate performance optimization recommendations.\"\"\"\n        recommendations = []\n        \n        if execution_time_ms > 30000:  # 30 seconds\n            recommendations.append(\"Consider breaking down complex rules into simpler components\")\n            recommendations.append(\"Enable sampling for large datasets to improve performance\")\n        \n        if memory_mb > 1000:  # 1 GB\n            recommendations.append(\"Consider processing data in chunks to reduce memory usage\")\n            recommendations.append(\"Optimize rule logic to avoid creating large intermediate results\")\n        \n        if rule.validation_logic.logic_type == LogicType.PYTHON:\n            recommendations.append(\"Consider using vectorized operations instead of row-wise processing\")\n        \n        return recommendations\n    \n    def _export_rules_to_json(self, rules: List[QualityRule]) -> str:\n        \"\"\"Export rules to JSON format.\"\"\"\n        rules_data = []\n        \n        for rule in rules:\n            rule_data = {\n                'rule_id': str(rule.rule_id),\n                'rule_name': rule.rule_name,\n                'rule_type': rule.rule_type.value,\n                'description': rule.description,\n                'validation_logic': {\n                    'logic_type': rule.validation_logic.logic_type.value,\n                    'expression': rule.validation_logic.expression,\n                    'parameters': rule.validation_logic.parameters,\n                    'error_message': rule.validation_logic.error_message,\n                    'success_criteria': {\n                        'min_pass_rate': rule.validation_logic.success_criteria.min_pass_rate,\n                        'max_failure_count': rule.validation_logic.success_criteria.max_failure_count,\n                        'warning_threshold': rule.validation_logic.success_criteria.warning_threshold\n                    }\n                },\n                'severity': rule.severity.value,\n                'category': rule.category.value,\n                'is_active': rule.is_active,\n                'target_tables': rule.target_tables,\n                'target_columns': rule.target_columns,\n                'tags': rule.tags,\n                'created_at': rule.created_at.isoformat(),\n                'last_modified': rule.last_modified.isoformat()\n            }\n            rules_data.append(rule_data)\n        \n        return json.dumps(rules_data, indent=2)\n    \n    def _export_rules_to_yaml(self, rules: List[QualityRule]) -> str:\n        \"\"\"Export rules to YAML format.\"\"\"\n        try:\n            import yaml\n            rules_data = json.loads(self._export_rules_to_json(rules))\n            return yaml.dump(rules_data, default_flow_style=False)\n        except ImportError:\n            raise ImportError(\"PyYAML is required for YAML export\")\n    \n    def _export_rules_to_csv(self, rules: List[QualityRule]) -> str:\n        \"\"\"Export rules to CSV format.\"\"\"\n        import csv\n        from io import StringIO\n        \n        output = StringIO()\n        writer = csv.writer(output)\n        \n        # Write header\n        writer.writerow([\n            'rule_id', 'rule_name', 'rule_type', 'description', 'logic_type',\n            'expression', 'severity', 'category', 'is_active', 'target_tables',\n            'target_columns', 'tags', 'created_at', 'last_modified'\n        ])\n        \n        # Write data\n        for rule in rules:\n            writer.writerow([\n                str(rule.rule_id),\n                rule.rule_name,\n                rule.rule_type.value,\n                rule.description,\n                rule.validation_logic.logic_type.value,\n                rule.validation_logic.expression,\n                rule.severity.value,\n                rule.category.value,\n                rule.is_active,\n                ';'.join(rule.target_tables),\n                ';'.join(rule.target_columns),\n                ';'.join(rule.tags),\n                rule.created_at.isoformat(),\n                rule.last_modified.isoformat()\n            ])\n        \n        return output.getvalue()\n    \n    def _import_rules_from_json(self, rules_data: str, created_by: UserId) -> List[QualityRule]:\n        \"\"\"Import rules from JSON format.\"\"\"\n        data = json.loads(rules_data)\n        rules = []\n        \n        for rule_data in data:\n            # Parse validation logic\n            logic_data = rule_data['validation_logic']\n            success_criteria = SuccessCriteria(\n                min_pass_rate=logic_data['success_criteria']['min_pass_rate'],\n                max_failure_count=logic_data['success_criteria'].get('max_failure_count'),\n                warning_threshold=logic_data['success_criteria']['warning_threshold']\n            )\n            \n            validation_logic = ValidationLogic(\n                logic_type=LogicType(logic_data['logic_type']),\n                expression=logic_data['expression'],\n                parameters=logic_data['parameters'],\n                error_message=logic_data['error_message'],\n                success_criteria=success_criteria\n            )\n            \n            # Create rule\n            rule = QualityRule(\n                rule_id=RuleId(rule_data['rule_id']),\n                rule_name=rule_data['rule_name'],\n                rule_type=RuleType(rule_data['rule_type']),\n                description=rule_data['description'],\n                validation_logic=validation_logic,\n                severity=Severity(rule_data['severity']),\n                category=QualityCategory(rule_data['category']),\n                is_active=rule_data['is_active'],\n                created_by=created_by or UserId(\"imported\"),\n                created_at=datetime.fromisoformat(rule_data['created_at']),\n                last_modified=datetime.fromisoformat(rule_data['last_modified']),\n                target_tables=rule_data.get('target_tables', []),\n                target_columns=rule_data.get('target_columns', []),\n                tags=rule_data.get('tags', [])\n            )\n            \n            rules.append(rule)\n        \n        return rules\n    \n    def _import_rules_from_yaml(self, rules_data: str, created_by: UserId) -> List[QualityRule]:\n        \"\"\"Import rules from YAML format.\"\"\"\n        try:\n            import yaml\n            data = yaml.safe_load(rules_data)\n            return self._import_rules_from_json(json.dumps(data), created_by)\n        except ImportError:\n            raise ImportError(\"PyYAML is required for YAML import\")\n    \n    def _initialize_rule_templates(self) -> Dict[str, RuleTemplate]:\n        \"\"\"Initialize built-in rule templates.\"\"\"\n        templates = {}\n        \n        # Completeness template\n        templates['completeness_check'] = RuleTemplate(\n            template_id='completeness_check',\n            template_name='Completeness Check',\n            template_description='Check for missing values in columns',\n            rule_type=RuleType.COMPLETENESS,\n            logic_type=LogicType.EXPRESSION,\n            expression_template='df[\\'{column_name}\\'].notna()',\n            parameter_definitions={\n                'required': ['column_name'],\n                'types': {'column_name': str}\n            },\n            examples=['Check if customer_id is not null', 'Validate email addresses are present']\n        )\n        \n        # Range validation template\n        templates['range_validation'] = RuleTemplate(\n            template_id='range_validation',\n            template_name='Range Validation',\n            template_description='Validate numeric values are within specified range',\n            rule_type=RuleType.VALIDITY,\n            logic_type=LogicType.STATISTICAL,\n            expression_template='range_check',\n            parameter_definitions={\n                'required': ['column_name', 'min_value', 'max_value'],\n                'types': {'column_name': str, 'min_value': (int, float), 'max_value': (int, float)}\n            },\n            examples=['Age between 0 and 150', 'Price between 0 and 10000']\n        )\n        \n        # Pattern validation template\n        templates['pattern_validation'] = RuleTemplate(\n            template_id='pattern_validation',\n            template_name='Pattern Validation',\n            template_description='Validate text values match specified pattern',\n            rule_type=RuleType.FORMAT,\n            logic_type=LogicType.REGEX,\n            expression_template='{pattern}',\n            parameter_definitions={\n                'required': ['column_name', 'pattern'],\n                'types': {'column_name': str, 'pattern': str}\n            },\n            examples=['Email format validation', 'Phone number format validation']\n        )\n        \n        # Uniqueness template\n        templates['uniqueness_check'] = RuleTemplate(\n            template_id='uniqueness_check',\n            template_name='Uniqueness Check',\n            template_description='Check for duplicate values in columns',\n            rule_type=RuleType.UNIQUENESS,\n            logic_type=LogicType.EXPRESSION,\n            expression_template='~df[\\'{column_name}\\'].duplicated()',\n            parameter_definitions={\n                'required': ['column_name'],\n                'types': {'column_name': str}\n            },\n            examples=['Check unique customer IDs', 'Validate unique email addresses']\n        )\n        \n        return templates\n    \n    def _update_test_statistics(self, test_result: RuleTestResult) -> None:\n        \"\"\"Update test statistics.\"\"\"\n        # Update average test time\n        current_avg = self._rule_stats['average_test_time']\n        total_tests = self._rule_stats['total_rules_tested']\n        \n        if test_result.performance_metrics.get('execution_time_seconds'):\n            new_time = test_result.performance_metrics['execution_time_seconds']\n            self._rule_stats['average_test_time'] = ((current_avg * (total_tests - 1)) + new_time) / total_tests\n        \n        # Update success rate\n        current_success_rate = self._rule_stats['test_success_rate']\n        success_count = int(current_success_rate * (total_tests - 1))\n        \n        if test_result.is_successful():\n            success_count += 1\n        \n        self._rule_stats['test_success_rate'] = success_count / total_tests