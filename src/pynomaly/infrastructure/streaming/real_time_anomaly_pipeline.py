"""Real-time anomaly detection pipeline with streaming data processing."""

import asyncio
import json
import logging
from abc import ABC, abstractmethod
from collections import deque
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, AsyncGenerator, Callable, Dict, List, Optional, Union
from uuid import UUID, uuid4

import numpy as np
import pandas as pd
from pydantic import BaseModel, Field

from pynomaly.domain.entities import AnomalyPoint, DetectionResult
from pynomaly.domain.value_objects import ContaminationRate


logger = logging.getLogger(__name__)


class StreamingMode(Enum):
    """Streaming processing modes."""
    BATCH = "batch"
    SLIDING_WINDOW = "sliding_window"
    TUMBLING_WINDOW = "tumbling_window"
    SESSION_WINDOW = "session_window"


class AlertSeverity(Enum):
    """Alert severity levels."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class StreamingMetrics(BaseModel):
    """Metrics for streaming pipeline performance."""
    processed_records: int = 0
    anomalies_detected: int = 0
    processing_rate: float = 0.0  # records per second
    average_latency: float = 0.0  # milliseconds
    error_count: int = 0
    backpressure_events: int = 0
    last_processed_at: Optional[datetime] = None
    pipeline_uptime: timedelta = timedelta()
    memory_usage_mb: float = 0.0
    cpu_usage_percent: float = 0.0


class StreamingConfig(BaseModel):
    """Configuration for streaming pipeline."""
    pipeline_id: str = Field(default_factory=lambda: str(uuid4()))
    batch_size: int = Field(100, description="Batch size for processing")
    window_size: int = Field(1000, description="Window size for sliding window")
    window_slide: int = Field(100, description="Slide interval for sliding window")
    session_timeout: int = Field(300, description="Session timeout in seconds")
    max_buffer_size: int = Field(10000, description="Maximum buffer size")
    processing_timeout: float = Field(30.0, description="Processing timeout in seconds")
    enable_backpressure: bool = Field(True, description="Enable backpressure control")
    enable_metrics: bool = Field(True, description="Enable metrics collection")
    alert_thresholds: Dict[str, float] = Field(
        default_factory=lambda: {
            "anomaly_rate": 0.1,
            "processing_latency": 1000.0,
            "error_rate": 0.05,
        }
    )


class DataPoint(BaseModel):
    """Individual data point in the stream."""
    timestamp: datetime
    data: Dict[str, Any]
    metadata: Dict[str, Any] = Field(default_factory=dict)
    source_id: Optional[str] = None
    partition_key: Optional[str] = None


class StreamingAlert(BaseModel):
    """Alert generated by the streaming pipeline."""
    alert_id: str = Field(default_factory=lambda: str(uuid4()))
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    severity: AlertSeverity
    alert_type: str
    message: str
    data_point: Optional[DataPoint] = None
    anomaly_score: Optional[float] = None
    detection_result: Optional[DetectionResult] = None
    metadata: Dict[str, Any] = Field(default_factory=dict)


class StreamingWindow:
    """Sliding window for streaming data."""
    
    def __init__(self, size: int, slide: int = 1):
        """Initialize streaming window.
        
        Args:
            size: Window size
            slide: Slide interval
        """
        self.size = size
        self.slide = slide
        self.buffer = deque(maxlen=size)
        self.slide_count = 0
    
    def add(self, data_point: DataPoint) -> bool:
        """Add data point to window.
        
        Args:
            data_point: Data point to add
            
        Returns:
            True if window should be processed
        """
        self.buffer.append(data_point)
        self.slide_count += 1
        
        # Check if window is ready for processing
        return len(self.buffer) == self.size and self.slide_count >= self.slide
    
    def get_window_data(self) -> List[DataPoint]:
        """Get current window data."""
        return list(self.buffer)
    
    def slide_window(self) -> None:
        """Slide the window."""
        if self.slide_count >= self.slide:
            # Remove old data points
            for _ in range(min(self.slide, len(self.buffer))):
                if self.buffer:
                    self.buffer.popleft()
            self.slide_count = 0


class DataSource(ABC):
    """Abstract base class for data sources."""
    
    @abstractmethod
    async def connect(self) -> None:
        """Connect to the data source."""
        pass
    
    @abstractmethod
    async def disconnect(self) -> None:
        """Disconnect from the data source."""
        pass
    
    @abstractmethod
    async def consume(self) -> AsyncGenerator[DataPoint, None]:
        """Consume data from the source."""
        pass


class KafkaDataSource(DataSource):
    """Kafka data source implementation."""
    
    def __init__(
        self,
        bootstrap_servers: str,
        topic: str,
        group_id: str,
        auto_offset_reset: str = "latest",
    ):
        """Initialize Kafka data source.
        
        Args:
            bootstrap_servers: Kafka bootstrap servers
            topic: Kafka topic to consume from
            group_id: Consumer group ID
            auto_offset_reset: Auto offset reset policy
        """
        self.bootstrap_servers = bootstrap_servers
        self.topic = topic
        self.group_id = group_id
        self.auto_offset_reset = auto_offset_reset
        self.consumer = None
    
    async def connect(self) -> None:
        """Connect to Kafka."""
        try:
            # In a real implementation, use aiokafka
            # from aiokafka import AIOKafkaConsumer
            # self.consumer = AIOKafkaConsumer(
            #     self.topic,
            #     bootstrap_servers=self.bootstrap_servers,
            #     group_id=self.group_id,
            #     auto_offset_reset=self.auto_offset_reset,
            # )
            # await self.consumer.start()
            logger.info(f"Connected to Kafka topic: {self.topic}")
        except Exception as e:
            logger.error(f"Failed to connect to Kafka: {e}")
            raise
    
    async def disconnect(self) -> None:
        """Disconnect from Kafka."""
        if self.consumer:
            # await self.consumer.stop()
            logger.info("Disconnected from Kafka")
    
    async def consume(self) -> AsyncGenerator[DataPoint, None]:
        """Consume data from Kafka topic."""
        # Mock implementation for demonstration
        # In real implementation, this would use aiokafka
        while True:
            # Simulate receiving a message
            await asyncio.sleep(0.1)
            
            # Generate mock data point
            data_point = DataPoint(
                timestamp=datetime.utcnow(),
                data={
                    "feature_1": np.random.normal(0, 1),
                    "feature_2": np.random.normal(0, 1),
                    "feature_3": np.random.normal(0, 1),
                },
                source_id="kafka_simulator",
            )
            
            yield data_point


class WebSocketDataSource(DataSource):
    """WebSocket data source implementation."""
    
    def __init__(self, websocket_url: str, headers: Optional[Dict[str, str]] = None):
        """Initialize WebSocket data source.
        
        Args:
            websocket_url: WebSocket URL
            headers: Optional headers
        """
        self.websocket_url = websocket_url
        self.headers = headers or {}
        self.websocket = None
    
    async def connect(self) -> None:
        """Connect to WebSocket."""
        try:
            # In a real implementation, use websockets library
            # import websockets
            # self.websocket = await websockets.connect(
            #     self.websocket_url, extra_headers=self.headers
            # )
            logger.info(f"Connected to WebSocket: {self.websocket_url}")
        except Exception as e:
            logger.error(f"Failed to connect to WebSocket: {e}")
            raise
    
    async def disconnect(self) -> None:
        """Disconnect from WebSocket."""
        if self.websocket:
            # await self.websocket.close()
            logger.info("Disconnected from WebSocket")
    
    async def consume(self) -> AsyncGenerator[DataPoint, None]:
        """Consume data from WebSocket."""
        # Mock implementation
        while True:
            await asyncio.sleep(0.1)
            
            # Generate mock data point
            data_point = DataPoint(
                timestamp=datetime.utcnow(),
                data={
                    "sensor_1": np.random.exponential(2),
                    "sensor_2": np.random.normal(100, 10),
                    "sensor_3": np.random.poisson(5),
                },
                source_id="websocket_simulator",
            )
            
            yield data_point


class StreamingAnomalyDetector:
    """Real-time anomaly detector for streaming data."""
    
    def __init__(
        self,
        detector_algorithm: str = "isolation_forest",
        contamination: float = 0.1,
        window_size: int = 1000,
        retraining_interval: int = 10000,
    ):
        """Initialize streaming anomaly detector.
        
        Args:
            detector_algorithm: Algorithm to use
            contamination: Expected contamination rate
            window_size: Training window size
            retraining_interval: Retraining interval in samples
        """
        self.detector_algorithm = detector_algorithm
        self.contamination = contamination
        self.window_size = window_size
        self.retraining_interval = retraining_interval
        
        self.detector = None
        self.training_buffer = deque(maxlen=window_size)
        self.samples_processed = 0
        self.is_trained = False
        
        self._initialize_detector()
    
    def _initialize_detector(self) -> None:
        """Initialize the anomaly detector."""
        if self.detector_algorithm == "isolation_forest":
            from sklearn.ensemble import IsolationForest
            self.detector = IsolationForest(
                contamination=self.contamination,
                random_state=42,
                n_jobs=-1,
            )
        elif self.detector_algorithm == "one_class_svm":
            from sklearn.svm import OneClassSVM
            self.detector = OneClassSVM(gamma="scale", nu=self.contamination)
        elif self.detector_algorithm == "local_outlier_factor":
            from sklearn.neighbors import LocalOutlierFactor
            self.detector = LocalOutlierFactor(
                contamination=self.contamination,
                novelty=True,
                n_jobs=-1,
            )
        else:
            raise ValueError(f"Unsupported algorithm: {self.detector_algorithm}")
    
    async def process_batch(self, data_points: List[DataPoint]) -> List[DetectionResult]:
        """Process a batch of data points.
        
        Args:
            data_points: List of data points to process
            
        Returns:
            List of detection results
        """
        if not data_points:
            return []
        
        # Extract features
        features = []
        for point in data_points:
            feature_vector = list(point.data.values())
            features.append(feature_vector)
        
        # Convert to numpy array
        X = np.array(features)
        
        # Add to training buffer
        for feature_vector in features:
            self.training_buffer.append(feature_vector)
        
        # Retrain if necessary
        if (
            self.samples_processed % self.retraining_interval == 0
            and len(self.training_buffer) >= self.window_size
        ):
            await self._retrain_detector()
        
        # Perform detection if trained
        results = []
        if self.is_trained:
            predictions = self.detector.predict(X)
            scores = self.detector.decision_function(X)
            
            for i, (point, pred, score) in enumerate(zip(data_points, predictions, scores)):
                is_anomaly = pred == -1
                anomaly_score = float(score)
                
                # Create anomaly point
                anomaly_point = AnomalyPoint(
                    index=i,
                    score=anomaly_score,
                    values=list(point.data.values()),
                    timestamp=point.timestamp,
                )
                
                # Create detection result
                result = DetectionResult(
                    id=uuid4(),
                    dataset_name="streaming_data",
                    algorithm_name=self.detector_algorithm,
                    contamination_rate=ContaminationRate(self.contamination),
                    anomalies=[anomaly_point] if is_anomaly else [],
                    scores=[anomaly_score],
                    timestamp=point.timestamp,
                    processing_time=0.0,  # Will be updated by pipeline
                    metadata={
                        "data_point_id": str(uuid4()),
                        "source_id": point.source_id,
                        "is_streaming": True,
                        "samples_processed": self.samples_processed,
                    },
                )
                
                results.append(result)
        
        self.samples_processed += len(data_points)
        return results
    
    async def _retrain_detector(self) -> None:
        """Retrain the detector with recent data."""
        if len(self.training_buffer) < self.window_size:
            return
        
        try:
            # Prepare training data
            X_train = np.array(list(self.training_buffer))
            
            # Retrain detector
            self.detector.fit(X_train)
            self.is_trained = True
            
            logger.info(
                f"Retrained {self.detector_algorithm} detector with "
                f"{len(X_train)} samples"
            )
        except Exception as e:
            logger.error(f"Failed to retrain detector: {e}")


class RealTimeAnomalyPipeline:
    """Real-time anomaly detection pipeline."""
    
    def __init__(
        self,
        config: StreamingConfig,
        data_source: DataSource,
        detector: StreamingAnomalyDetector,
        alert_handler: Optional[Callable[[StreamingAlert], None]] = None,
    ):
        """Initialize real-time anomaly pipeline.
        
        Args:
            config: Pipeline configuration
            data_source: Data source to consume from
            detector: Anomaly detector
            alert_handler: Optional alert handler
        """
        self.config = config
        self.data_source = data_source
        self.detector = detector
        self.alert_handler = alert_handler
        
        self.is_running = False
        self.metrics = StreamingMetrics()
        self.window = StreamingWindow(config.window_size, config.window_slide)
        self.buffer = deque(maxlen=config.max_buffer_size)
        self.start_time = None
        
        # Processing tasks
        self.consumer_task = None
        self.processor_task = None
        self.metrics_task = None
    
    async def start(self) -> None:
        """Start the streaming pipeline."""
        if self.is_running:
            logger.warning("Pipeline is already running")
            return
        
        logger.info(f"Starting real-time anomaly pipeline: {self.config.pipeline_id}")
        
        try:
            # Connect to data source
            await self.data_source.connect()
            
            # Initialize state
            self.is_running = True
            self.start_time = datetime.utcnow()
            
            # Start processing tasks
            self.consumer_task = asyncio.create_task(self._consume_data())
            self.processor_task = asyncio.create_task(self._process_data())
            
            if self.config.enable_metrics:
                self.metrics_task = asyncio.create_task(self._collect_metrics())
            
            logger.info("Real-time anomaly pipeline started successfully")
        except Exception as e:
            logger.error(f"Failed to start pipeline: {e}")
            await self.stop()
            raise
    
    async def stop(self) -> None:
        """Stop the streaming pipeline."""
        if not self.is_running:
            return
        
        logger.info("Stopping real-time anomaly pipeline")
        
        self.is_running = False
        
        # Cancel tasks
        for task in [self.consumer_task, self.processor_task, self.metrics_task]:
            if task and not task.done():
                task.cancel()
                try:
                    await task
                except asyncio.CancelledError:
                    pass
        
        # Disconnect from data source
        try:
            await self.data_source.disconnect()
        except Exception as e:
            logger.error(f"Error disconnecting from data source: {e}")
        
        logger.info("Real-time anomaly pipeline stopped")
    
    async def _consume_data(self) -> None:
        """Consume data from the data source."""
        try:
            async for data_point in self.data_source.consume():
                if not self.is_running:
                    break
                
                # Add to buffer
                self.buffer.append(data_point)
                
                # Check for backpressure
                if (
                    self.config.enable_backpressure
                    and len(self.buffer) > self.config.max_buffer_size * 0.8
                ):
                    self.metrics.backpressure_events += 1
                    await asyncio.sleep(0.01)  # Brief pause to reduce pressure
                
                # Update metrics
                self.metrics.processed_records += 1
                self.metrics.last_processed_at = datetime.utcnow()
        except asyncio.CancelledError:
            pass
        except Exception as e:
            logger.error(f"Error in data consumption: {e}")
            self.metrics.error_count += 1
    
    async def _process_data(self) -> None:
        """Process data for anomaly detection."""
        try:
            while self.is_running:
                if not self.buffer:
                    await asyncio.sleep(0.01)
                    continue
                
                # Collect batch
                batch = []
                for _ in range(min(self.config.batch_size, len(self.buffer))):
                    if self.buffer:
                        batch.append(self.buffer.popleft())
                
                if not batch:
                    continue
                
                # Process batch
                start_time = datetime.utcnow()
                
                try:
                    # Perform anomaly detection
                    detection_results = await asyncio.wait_for(
                        self.detector.process_batch(batch),
                        timeout=self.config.processing_timeout,
                    )
                    
                    # Calculate processing time
                    processing_time = (datetime.utcnow() - start_time).total_seconds() * 1000
                    
                    # Update results with processing time
                    for result in detection_results:
                        result.processing_time = processing_time
                    
                    # Handle alerts
                    await self._handle_detection_results(detection_results, batch)
                    
                    # Update metrics
                    self.metrics.anomalies_detected += sum(
                        len(result.anomalies) for result in detection_results
                    )
                    
                    # Update average latency
                    if self.metrics.processed_records > 0:
                        self.metrics.average_latency = (
                            (self.metrics.average_latency * (self.metrics.processed_records - len(batch)) + 
                             processing_time * len(batch)) / self.metrics.processed_records
                        )
                    else:
                        self.metrics.average_latency = processing_time
                
                except asyncio.TimeoutError:
                    logger.error("Processing timeout exceeded")
                    self.metrics.error_count += 1
                except Exception as e:
                    logger.error(f"Error in data processing: {e}")
                    self.metrics.error_count += 1
        
        except asyncio.CancelledError:
            pass
    
    async def _handle_detection_results(
        self, 
        results: List[DetectionResult], 
        batch: List[DataPoint]
    ) -> None:
        """Handle detection results and generate alerts."""
        for result, data_point in zip(results, batch):
            if result.anomalies:
                # Generate alert
                anomaly = result.anomalies[0]
                severity = self._determine_alert_severity(anomaly.score)
                
                alert = StreamingAlert(
                    severity=severity,
                    alert_type="anomaly_detected",
                    message=f"Anomaly detected with score {anomaly.score:.3f}",
                    data_point=data_point,
                    anomaly_score=anomaly.score,
                    detection_result=result,
                    metadata={
                        "algorithm": result.algorithm_name,
                        "pipeline_id": self.config.pipeline_id,
                    },
                )
                
                # Send alert
                if self.alert_handler:
                    try:
                        self.alert_handler(alert)
                    except Exception as e:
                        logger.error(f"Error in alert handler: {e}")
                
                logger.info(
                    f"Anomaly detected: score={anomaly.score:.3f}, "
                    f"severity={severity.value}"
                )
    
    def _determine_alert_severity(self, anomaly_score: float) -> AlertSeverity:
        """Determine alert severity based on anomaly score."""
        # This is a simple example - in practice, you'd use more sophisticated logic
        abs_score = abs(anomaly_score)
        
        if abs_score > 2.0:
            return AlertSeverity.CRITICAL
        elif abs_score > 1.5:
            return AlertSeverity.HIGH
        elif abs_score > 1.0:
            return AlertSeverity.MEDIUM
        else:
            return AlertSeverity.LOW
    
    async def _collect_metrics(self) -> None:
        """Collect pipeline metrics."""
        try:
            while self.is_running:
                await asyncio.sleep(10)  # Collect metrics every 10 seconds
                
                # Calculate processing rate
                if self.start_time:
                    uptime = datetime.utcnow() - self.start_time
                    self.metrics.pipeline_uptime = uptime
                    
                    if uptime.total_seconds() > 0:
                        self.metrics.processing_rate = (
                            self.metrics.processed_records / uptime.total_seconds()
                        )
                
                # Collect system metrics
                try:
                    import psutil
                    process = psutil.Process()
                    self.metrics.memory_usage_mb = process.memory_info().rss / 1024 / 1024
                    self.metrics.cpu_usage_percent = process.cpu_percent()
                except ImportError:
                    pass  # psutil not available
                
                # Check alert thresholds
                await self._check_alert_thresholds()
        
        except asyncio.CancelledError:
            pass
    
    async def _check_alert_thresholds(self) -> None:
        """Check if metrics exceed alert thresholds."""
        thresholds = self.config.alert_thresholds
        
        # Check anomaly rate
        if self.metrics.processed_records > 0:
            anomaly_rate = self.metrics.anomalies_detected / self.metrics.processed_records
            if anomaly_rate > thresholds.get("anomaly_rate", 0.1):
                alert = StreamingAlert(
                    severity=AlertSeverity.HIGH,
                    alert_type="high_anomaly_rate",
                    message=f"High anomaly rate detected: {anomaly_rate:.3f}",
                    metadata={"anomaly_rate": anomaly_rate},
                )
                if self.alert_handler:
                    self.alert_handler(alert)
        
        # Check processing latency
        if self.metrics.average_latency > thresholds.get("processing_latency", 1000.0):
            alert = StreamingAlert(
                severity=AlertSeverity.MEDIUM,
                alert_type="high_latency",
                message=f"High processing latency: {self.metrics.average_latency:.1f}ms",
                metadata={"latency_ms": self.metrics.average_latency},
            )
            if self.alert_handler:
                self.alert_handler(alert)
        
        # Check error rate
        if self.metrics.processed_records > 0:
            error_rate = self.metrics.error_count / self.metrics.processed_records
            if error_rate > thresholds.get("error_rate", 0.05):
                alert = StreamingAlert(
                    severity=AlertSeverity.HIGH,
                    alert_type="high_error_rate",
                    message=f"High error rate detected: {error_rate:.3f}",
                    metadata={"error_rate": error_rate},
                )
                if self.alert_handler:
                    self.alert_handler(alert)
    
    def get_metrics(self) -> StreamingMetrics:
        """Get current pipeline metrics."""
        return self.metrics
    
    def get_status(self) -> Dict[str, Any]:
        """Get pipeline status."""
        return {
            "pipeline_id": self.config.pipeline_id,
            "is_running": self.is_running,
            "start_time": self.start_time.isoformat() if self.start_time else None,
            "uptime_seconds": self.metrics.pipeline_uptime.total_seconds(),
            "buffer_size": len(self.buffer),
            "buffer_capacity": self.config.max_buffer_size,
            "detector_trained": self.detector.is_trained,
            "samples_processed": self.detector.samples_processed,
            "metrics": self.metrics.dict(),
        }