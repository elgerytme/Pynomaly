[tool:pytest]
# Pytest configuration for data intelligence comprehensive testing framework

# Test discovery
testpaths = 
    src/packages/ai/machine_learning/tests
    src/packages/data/anomaly_detection/tests  
    src/packages/enterprise/enterprise_auth/tests
    src/packages/system_tests/integration
    src/packages/data/quality/tests
    src/packages/data/observability/tests
    src/packages/ai/mlops/tests
    src/packages/enterprise/enterprise_governance/tests
    src/packages/data/statistics/tests
    src/packages/data/data_architecture/tests
    src/packages/enterprise/enterprise_scalability/tests
python_files = test_*.py *_test.py
python_classes = Test* *Tests
python_functions = test_*

# Minimum version
minversion = 8.0

# Default options
addopts = 
    --strict-markers
    --strict-config
    --tb=short
    --disable-warnings
    --color=yes
    --durations=10
    --show-capture=no
    --maxfail=10
    -v

# Async support
asyncio_mode = auto

# Markers for test categorization
markers =
    # Phase-based markers
    critical: Critical tests that must pass for any deployment
    domain: Domain-specific package tests  
    comprehensive: Full comprehensive testing
    
    # Package-specific markers
    machine_learning: Machine learning algorithm tests
    anomaly_detection: Statistical analytics and modeling tests
    enterprise_auth: Enterprise authentication tests
    integration: Cross-package integration tests
    data_quality: Data quality assessment tests
    observability: Monitoring and observability tests
    mlops: MLOps lifecycle tests
    enterprise_governance: Enterprise governance tests
    statistics: Statistical analysis tests
    architecture: Data architecture tests
    scalability: Enterprise scalability tests
    
    # Primary test categories
    unit: Unit tests for individual components
    e2e: End-to-end tests for complete workflows
    performance: Performance and benchmark tests
    security: Security-focused tests
    api: API endpoint tests
    ui: User interface tests
    platform: Cross-platform compatibility tests
    load: Load testing with multiple users
    regression: Regression tests against baselines
    contract: API contract validation tests
    
    # Test execution characteristics
    slow: Tests that take more than 1 second
    fast: Tests that complete in under 1 second
    benchmark: Performance benchmark tests
    memory: Memory-intensive tests
    cpu: CPU-intensive tests
    
    # External dependencies
    external: Tests requiring external services
    database: Tests requiring database connection
    redis: Tests requiring Redis connection
    network: Network-dependent tests
    filesystem: Filesystem-dependent tests
    
    # Test environments
    docker: Tests that require Docker
    kubernetes: Tests that require Kubernetes
    
    # Test type markers
    algorithm_validation: Algorithm accuracy validation
    stream_processing: Stream processing tests
    distributed_computing: Distributed computing tests
    auto_scaling: Auto-scaling tests
    pipeline: Data pipeline tests
    schema_validation: Schema validation tests
    data_transformation: Data transformation tests
    
    # ML-specific markers
    ml: Machine learning tests
    automl: AutoML tests
    torch: Tests requiring PyTorch
    tensorflow: Tests requiring TensorFlow
    jax: Tests requiring JAX
    model: Machine learning model tests
    
    # Data-related markers
    streaming: Real-time processing tests
    
    # Test types
    smoke: Smoke tests for basic functionality
    auth: Authentication/authorization tests
    
    # Test status
    xfail: Expected to fail
    skip: Skip these tests

# Filter warnings
filterwarnings =
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::FutureWarning
    ignore::UserWarning:pydantic.*
    ignore::UserWarning:dependency_injector.*
    ignore::UserWarning:sklearn.*
    ignore::ResourceWarning
    ignore::RuntimeWarning:numpy.*

# Logging configuration
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# Test timeout (disabled by default to avoid conflicts with benchmark tests)
# timeout = 300

# Coverage configuration (when using pytest-cov)
# These settings integrate with the comprehensive testing framework
# Coverage is handled by the framework itself for more control

# Parallel execution (when using pytest-xdist)
# Default number of workers can be overridden with -n option

# Custom test collection
collect_ignore = [
    "build",
    "dist", 
    "*.egg",
    ".git",
    ".tox",
    "__pycache__",
    "environments",
    ".env*",
    ".venv*",
    "test_env*",
    "venv*",
    "node_modules"

# Django settings (if Django tests are added)
# DJANGO_SETTINGS_MODULE = myproject.settings.test

# Test data paths
# These can be used by fixtures to locate test data
test_data_paths = 
    tests/data
    tests/fixtures
    tests/resources