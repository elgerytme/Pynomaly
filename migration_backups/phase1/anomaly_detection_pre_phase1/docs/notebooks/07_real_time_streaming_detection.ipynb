{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 Real-Time Streaming Anomaly Detection\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/your-org/anomaly-detection/blob/main/docs/notebooks/07_real_time_streaming_detection.ipynb)\n",
    "\n",
    "**Difficulty**: Advanced | **Time**: 50 minutes\n",
    "\n",
    "Learn how to build real-time anomaly detection systems that process streaming data with low latency and high throughput. This notebook covers streaming architectures, online learning, and performance optimization.\n",
    "\n",
    "## 🎯 Learning Objectives\n",
    "\n",
    "- Build real-time streaming detection pipelines\n",
    "- Implement online learning algorithms\n",
    "- Handle concept drift in streaming data\n",
    "- Optimize performance for high-throughput scenarios\n",
    "- Monitor streaming system health and performance\n",
    "\n",
    "## 📦 Prerequisites\n",
    "\n",
    "- Complete [Algorithm Comparison Tutorial](02_algorithm_comparison_tutorial.ipynb)\n",
    "- Understanding of streaming data concepts\n",
    "- Basic knowledge of system performance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import time\n",
    "import threading\n",
    "from collections import deque\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Interactive widgets\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(\"🚀 Ready for real-time streaming anomaly detection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Streaming Architecture Components\n",
    "\n",
    "Let's build the core components of a real-time streaming anomaly detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingDataGenerator:\n",
    "    \"\"\"Simulates real-time data streams with configurable anomaly patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_rate=100, anomaly_rate=0.05, features=5):\n",
    "        self.base_rate = base_rate  # Records per second\n",
    "        self.anomaly_rate = anomaly_rate\n",
    "        self.features = features\n",
    "        self.is_running = False\n",
    "        self.data_buffer = deque(maxlen=10000)\n",
    "        self.anomaly_buffer = deque(maxlen=1000)\n",
    "        self.timestamp_buffer = deque(maxlen=10000)\n",
    "        \n",
    "    def generate_normal_data(self, n_samples=1):\n",
    "        \"\"\"Generate normal data samples.\"\"\"\n",
    "        return np.random.multivariate_normal(\n",
    "            mean=np.zeros(self.features),\n",
    "            cov=np.eye(self.features),\n",
    "            size=n_samples\n",
    "        )\n",
    "    \n",
    "    def generate_anomaly_data(self, n_samples=1):\n",
    "        \"\"\"Generate anomalous data samples.\"\"\"\n",
    "        # Create various types of anomalies\n",
    "        anomaly_type = np.random.choice(['shift', 'scale', 'outlier'])\n",
    "        \n",
    "        if anomaly_type == 'shift':\n",
    "            # Mean shift anomaly\n",
    "            shift = np.random.uniform(-5, 5, self.features)\n",
    "            return np.random.multivariate_normal(\n",
    "                mean=shift, cov=np.eye(self.features), size=n_samples\n",
    "            )\n",
    "        elif anomaly_type == 'scale':\n",
    "            # Variance anomaly\n",
    "            scale = np.random.uniform(3, 8)\n",
    "            return np.random.multivariate_normal(\n",
    "                mean=np.zeros(self.features),\n",
    "                cov=scale * np.eye(self.features),\n",
    "                size=n_samples\n",
    "            )\n",
    "        else:\n",
    "            # Extreme outlier\n",
    "            return np.random.uniform(-10, 10, (n_samples, self.features))\n",
    "    \n",
    "    def generate_batch(self, batch_size=100):\n",
    "        \"\"\"Generate a batch of data with mixed normal and anomalous samples.\"\"\"\n",
    "        n_anomalies = int(batch_size * self.anomaly_rate)\n",
    "        n_normal = batch_size - n_anomalies\n",
    "        \n",
    "        # Generate normal and anomalous data\n",
    "        normal_data = self.generate_normal_data(n_normal)\n",
    "        anomaly_data = self.generate_anomaly_data(n_anomalies) if n_anomalies > 0 else np.array([])\n",
    "        \n",
    "        # Combine and shuffle\n",
    "        if n_anomalies > 0:\n",
    "            data = np.vstack([normal_data, anomaly_data])\n",
    "            labels = np.concatenate([np.ones(n_normal), -np.ones(n_anomalies)])\n",
    "        else:\n",
    "            data = normal_data\n",
    "            labels = np.ones(n_normal)\n",
    "        \n",
    "        # Shuffle\n",
    "        indices = np.random.permutation(len(data))\n",
    "        return data[indices], labels[indices]\n",
    "    \n",
    "    def start_streaming(self, duration_seconds=60, callback=None):\n",
    "        \"\"\"Start streaming data simulation.\"\"\"\n",
    "        self.is_running = True\n",
    "        start_time = time.time()\n",
    "        \n",
    "        while self.is_running and (time.time() - start_time) < duration_seconds:\n",
    "            # Generate data batch\n",
    "            batch_data, batch_labels = self.generate_batch(self.base_rate)\n",
    "            \n",
    "            # Add to buffers with timestamps\n",
    "            current_time = datetime.now()\n",
    "            for i, (sample, label) in enumerate(zip(batch_data, batch_labels)):\n",
    "                self.data_buffer.append(sample)\n",
    "                self.anomaly_buffer.append(label)\n",
    "                self.timestamp_buffer.append(current_time + timedelta(milliseconds=i*10))\n",
    "            \n",
    "            # Call callback if provided\n",
    "            if callback:\n",
    "                callback(batch_data, batch_labels)\n",
    "            \n",
    "            # Sleep to simulate real-time rate\n",
    "            time.sleep(1.0)\n",
    "    \n",
    "    def stop_streaming(self):\n",
    "        \"\"\"Stop the streaming simulation.\"\"\"\n",
    "        self.is_running = False\n",
    "    \n",
    "    def get_recent_data(self, n_samples=1000):\n",
    "        \"\"\"Get the most recent data samples.\"\"\"\n",
    "        if len(self.data_buffer) == 0:\n",
    "            return np.array([]), np.array([]), []\n",
    "        \n",
    "        data = np.array(list(self.data_buffer)[-n_samples:])\n",
    "        labels = np.array(list(self.anomaly_buffer)[-n_samples:])\n",
    "        timestamps = list(self.timestamp_buffer)[-n_samples:]\n",
    "        \n",
    "        return data, labels, timestamps\n",
    "\n",
    "print(\"✅ StreamingDataGenerator created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineAnomalyDetector:\n",
    "    \"\"\"Online anomaly detector with adaptive learning capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=1000, adaptation_rate=0.1):\n",
    "        self.window_size = window_size\n",
    "        self.adaptation_rate = adaptation_rate\n",
    "        self.data_window = deque(maxlen=window_size)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.detector = IsolationForest(\n",
    "            contamination=0.1,\n",
    "            random_state=42,\n",
    "            n_estimators=50\n",
    "        )\n",
    "        self.is_trained = False\n",
    "        self.performance_history = deque(maxlen=100)\n",
    "        self.drift_detector = ConceptDriftDetector()\n",
    "        \n",
    "    def update_model(self, new_data):\n",
    "        \"\"\"Update the model with new data batch.\"\"\"\n",
    "        # Add new data to sliding window\n",
    "        for sample in new_data:\n",
    "            self.data_window.append(sample)\n",
    "        \n",
    "        # Only retrain if we have enough data\n",
    "        if len(self.data_window) >= 100:\n",
    "            window_data = np.array(list(self.data_window))\n",
    "            \n",
    "            # Fit scaler and detector\n",
    "            self.scaler.fit(window_data)\n",
    "            scaled_data = self.scaler.transform(window_data)\n",
    "            self.detector.fit(scaled_data)\n",
    "            self.is_trained = True\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"Predict anomalies in new data.\"\"\"\n",
    "        if not self.is_trained:\n",
    "            return np.ones(len(data))  # Return normal predictions if not trained\n",
    "        \n",
    "        scaled_data = self.scaler.transform(data)\n",
    "        predictions = self.detector.predict(scaled_data)\n",
    "        scores = self.detector.score_samples(scaled_data)\n",
    "        \n",
    "        return predictions, scores\n",
    "    \n",
    "    def evaluate_performance(self, predictions, true_labels):\n",
    "        \"\"\"Evaluate current performance and update history.\"\"\"\n",
    "        if len(predictions) == 0 or len(true_labels) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Convert to binary format (1 for normal, -1 for anomaly)\n",
    "        pred_binary = (predictions == -1).astype(int)\n",
    "        true_binary = (true_labels == -1).astype(int)\n",
    "        \n",
    "        if len(np.unique(true_binary)) > 1:\n",
    "            precision = precision_score(true_binary, pred_binary, zero_division=0)\n",
    "            recall = recall_score(true_binary, pred_binary, zero_division=0)\n",
    "            f1 = f1_score(true_binary, pred_binary, zero_division=0)\n",
    "            \n",
    "            performance = {\n",
    "                'timestamp': datetime.now(),\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1_score': f1,\n",
    "                'accuracy': np.mean(pred_binary == true_binary)\n",
    "            }\n",
    "            \n",
    "            self.performance_history.append(performance)\n",
    "            return performance\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def check_concept_drift(self, new_data):\n",
    "        \"\"\"Check for concept drift in the data stream.\"\"\"\n",
    "        return self.drift_detector.detect_drift(new_data)\n",
    "    \n",
    "    def get_performance_summary(self):\n",
    "        \"\"\"Get recent performance summary.\"\"\"\n",
    "        if not self.performance_history:\n",
    "            return None\n",
    "        \n",
    "        recent_performance = list(self.performance_history)[-10:]  # Last 10 evaluations\n",
    "        \n",
    "        return {\n",
    "            'avg_precision': np.mean([p['precision'] for p in recent_performance]),\n",
    "            'avg_recall': np.mean([p['recall'] for p in recent_performance]),\n",
    "            'avg_f1': np.mean([p['f1_score'] for p in recent_performance]),\n",
    "            'avg_accuracy': np.mean([p['accuracy'] for p in recent_performance]),\n",
    "            'trend': 'improving' if len(recent_performance) > 1 and \n",
    "                     recent_performance[-1]['f1_score'] > recent_performance[0]['f1_score'] else 'stable'\n",
    "        }\n",
    "\n",
    "\n",
    "class ConceptDriftDetector:\n",
    "    \"\"\"Simple concept drift detector using statistical tests.\"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=500, threshold=0.05):\n",
    "        self.window_size = window_size\n",
    "        self.threshold = threshold\n",
    "        self.reference_window = deque(maxlen=window_size)\n",
    "        self.current_window = deque(maxlen=window_size)\n",
    "        self.is_initialized = False\n",
    "    \n",
    "    def detect_drift(self, new_data):\n",
    "        \"\"\"Detect concept drift using KS test on feature distributions.\"\"\"\n",
    "        # Add new data to current window\n",
    "        for sample in new_data:\n",
    "            self.current_window.append(sample)\n",
    "        \n",
    "        # Initialize reference window if needed\n",
    "        if not self.is_initialized and len(self.current_window) >= self.window_size:\n",
    "            self.reference_window = deque(list(self.current_window), maxlen=self.window_size)\n",
    "            self.is_initialized = True\n",
    "            return False\n",
    "        \n",
    "        # Perform drift detection if both windows are full\n",
    "        if len(self.reference_window) >= self.window_size and len(self.current_window) >= self.window_size:\n",
    "            ref_data = np.array(list(self.reference_window))\n",
    "            cur_data = np.array(list(self.current_window))\n",
    "            \n",
    "            # Simple drift detection using mean shift\n",
    "            ref_mean = np.mean(ref_data, axis=0)\n",
    "            cur_mean = np.mean(cur_data, axis=0)\n",
    "            \n",
    "            # Calculate normalized difference\n",
    "            ref_std = np.std(ref_data, axis=0) + 1e-8\n",
    "            normalized_diff = np.abs(cur_mean - ref_mean) / ref_std\n",
    "            \n",
    "            # Check if any feature has significant drift\n",
    "            drift_detected = np.any(normalized_diff > 2.0)\n",
    "            \n",
    "            if drift_detected:\n",
    "                # Update reference window\n",
    "                self.reference_window = deque(list(self.current_window), maxlen=self.window_size)\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "print(\"✅ OnlineAnomalyDetector and ConceptDriftDetector created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎛️ Interactive Streaming Simulation\n",
    "\n",
    "Let's create an interactive dashboard to monitor real-time anomaly detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingDashboard:\n",
    "    \"\"\"Interactive dashboard for monitoring streaming anomaly detection.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.data_generator = StreamingDataGenerator(base_rate=50, anomaly_rate=0.05)\n",
    "        self.detector = OnlineAnomalyDetector(window_size=500)\n",
    "        self.is_running = False\n",
    "        self.thread = None\n",
    "        \n",
    "        # Metrics tracking\n",
    "        self.total_processed = 0\n",
    "        self.total_anomalies_detected = 0\n",
    "        self.processing_times = deque(maxlen=100)\n",
    "        \n",
    "        # Create widgets\n",
    "        self.create_widgets()\n",
    "    \n",
    "    def create_widgets(self):\n",
    "        \"\"\"Create interactive widgets for the dashboard.\"\"\"\n",
    "        # Control widgets\n",
    "        self.start_button = widgets.Button(\n",
    "            description='Start Streaming',\n",
    "            button_style='success',\n",
    "            icon='play'\n",
    "        )\n",
    "        self.stop_button = widgets.Button(\n",
    "            description='Stop Streaming',\n",
    "            button_style='danger',\n",
    "            icon='stop',\n",
    "            disabled=True\n",
    "        )\n",
    "        \n",
    "        # Configuration widgets\n",
    "        self.rate_slider = widgets.IntSlider(\n",
    "            value=50,\n",
    "            min=10,\n",
    "            max=200,\n",
    "            step=10,\n",
    "            description='Rate (rps):',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        self.anomaly_rate_slider = widgets.FloatSlider(\n",
    "            value=0.05,\n",
    "            min=0.01,\n",
    "            max=0.20,\n",
    "            step=0.01,\n",
    "            description='Anomaly Rate:',\n",
    "            style={'description_width': 'initial'}\n",
    "        )\n",
    "        \n",
    "        # Status widgets\n",
    "        self.status_output = widgets.Output()\n",
    "        self.metrics_output = widgets.Output()\n",
    "        self.plot_output = widgets.Output()\n",
    "        \n",
    "        # Event handlers\n",
    "        self.start_button.on_click(self.start_streaming)\n",
    "        self.stop_button.on_click(self.stop_streaming)\n",
    "    \n",
    "    def display_dashboard(self):\n",
    "        \"\"\"Display the interactive dashboard.\"\"\"\n",
    "        # Control panel\n",
    "        controls = widgets.HBox([\n",
    "            self.start_button,\n",
    "            self.stop_button,\n",
    "            self.rate_slider,\n",
    "            self.anomaly_rate_slider\n",
    "        ])\n",
    "        \n",
    "        # Status panel\n",
    "        status_panel = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>📊 Real-Time Metrics</h3>\"),\n",
    "            self.metrics_output\n",
    "        ])\n",
    "        \n",
    "        # Plot panel\n",
    "        plot_panel = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>📈 Live Visualization</h3>\"),\n",
    "            self.plot_output\n",
    "        ])\n",
    "        \n",
    "        # Main dashboard\n",
    "        dashboard = widgets.VBox([\n",
    "            widgets.HTML(\"<h2>🚀 Real-Time Streaming Anomaly Detection Dashboard</h2>\"),\n",
    "            controls,\n",
    "            widgets.HBox([status_panel, plot_panel]),\n",
    "            self.status_output\n",
    "        ])\n",
    "        \n",
    "        display(dashboard)\n",
    "    \n",
    "    def start_streaming(self, button):\n",
    "        \"\"\"Start the streaming simulation.\"\"\"\n",
    "        if not self.is_running:\n",
    "            self.is_running = True\n",
    "            self.start_button.disabled = True\n",
    "            self.stop_button.disabled = False\n",
    "            \n",
    "            # Update generator configuration\n",
    "            self.data_generator.base_rate = self.rate_slider.value\n",
    "            self.data_generator.anomaly_rate = self.anomaly_rate_slider.value\n",
    "            \n",
    "            # Start streaming in a separate thread\n",
    "            self.thread = threading.Thread(target=self.streaming_loop)\n",
    "            self.thread.daemon = True\n",
    "            self.thread.start()\n",
    "            \n",
    "            with self.status_output:\n",
    "                print(\"🟢 Streaming started!\")\n",
    "    \n",
    "    def stop_streaming(self, button):\n",
    "        \"\"\"Stop the streaming simulation.\"\"\"\n",
    "        self.is_running = False\n",
    "        self.data_generator.stop_streaming()\n",
    "        self.start_button.disabled = False\n",
    "        self.stop_button.disabled = True\n",
    "        \n",
    "        with self.status_output:\n",
    "            print(\"🔴 Streaming stopped!\")\n",
    "    \n",
    "    def streaming_loop(self):\n",
    "        \"\"\"Main streaming processing loop.\"\"\"\n",
    "        update_counter = 0\n",
    "        \n",
    "        while self.is_running:\n",
    "            try:\n",
    "                # Generate new data batch\n",
    "                start_time = time.time()\n",
    "                batch_data, batch_labels = self.data_generator.generate_batch(self.rate_slider.value)\n",
    "                \n",
    "                # Update detector with new data\n",
    "                self.detector.update_model(batch_data)\n",
    "                \n",
    "                # Make predictions if detector is trained\n",
    "                if self.detector.is_trained:\n",
    "                    predictions, scores = self.detector.predict(batch_data)\n",
    "                    \n",
    "                    # Evaluate performance\n",
    "                    performance = self.detector.evaluate_performance(predictions, batch_labels)\n",
    "                    \n",
    "                    # Check for concept drift\n",
    "                    drift_detected = self.detector.check_concept_drift(batch_data)\n",
    "                    \n",
    "                    # Update metrics\n",
    "                    self.total_processed += len(batch_data)\n",
    "                    self.total_anomalies_detected += np.sum(predictions == -1)\n",
    "                    \n",
    "                    processing_time = (time.time() - start_time) * 1000  # Convert to ms\n",
    "                    self.processing_times.append(processing_time)\n",
    "                    \n",
    "                    # Update dashboard every 5 iterations\n",
    "                    update_counter += 1\n",
    "                    if update_counter % 5 == 0:\n",
    "                        self.update_dashboard(performance, drift_detected)\n",
    "                \n",
    "                # Sleep to maintain rate\n",
    "                time.sleep(1.0)\n",
    "                \n",
    "            except Exception as e:\n",
    "                with self.status_output:\n",
    "                    print(f\"❌ Error in streaming loop: {e}\")\n",
    "                break\n",
    "    \n",
    "    def update_dashboard(self, performance, drift_detected):\n",
    "        \"\"\"Update dashboard metrics and visualizations.\"\"\"\n",
    "        # Update metrics\n",
    "        with self.metrics_output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            throughput = self.rate_slider.value\n",
    "            avg_latency = np.mean(list(self.processing_times)) if self.processing_times else 0\n",
    "            detection_rate = (self.total_anomalies_detected / max(self.total_processed, 1)) * 100\n",
    "            \n",
    "            metrics_html = f\"\"\"\n",
    "            <div style=\"background-color: #f0f0f0; padding: 15px; border-radius: 8px;\">\n",
    "                <div style=\"display: flex; justify-content: space-between; margin-bottom: 10px;\">\n",
    "                    <div><strong>📊 Throughput:</strong> {throughput} rps</div>\n",
    "                    <div><strong>⏱️ Avg Latency:</strong> {avg_latency:.1f}ms</div>\n",
    "                </div>\n",
    "                <div style=\"display: flex; justify-content: space-between; margin-bottom: 10px;\">\n",
    "                    <div><strong>🔍 Total Processed:</strong> {self.total_processed:,}</div>\n",
    "                    <div><strong>🚨 Anomalies Detected:</strong> {self.total_anomalies_detected:,}</div>\n",
    "                </div>\n",
    "                <div style=\"display: flex; justify-content: space-between; margin-bottom: 10px;\">\n",
    "                    <div><strong>📈 Detection Rate:</strong> {detection_rate:.2f}%</div>\n",
    "                    <div><strong>🔄 Concept Drift:</strong> {'⚠️ Detected' if drift_detected else '✅ Stable'}</div>\n",
    "                </div>\n",
    "            \"\"\"\n",
    "            \n",
    "            if performance:\n",
    "                metrics_html += f\"\"\"\n",
    "                <div style=\"border-top: 1px solid #ddd; padding-top: 10px; margin-top: 10px;\">\n",
    "                    <div><strong>🎯 Model Performance:</strong></div>\n",
    "                    <div style=\"margin-left: 20px;\">\n",
    "                        <div>Precision: {performance['precision']:.3f}</div>\n",
    "                        <div>Recall: {performance['recall']:.3f}</div>\n",
    "                        <div>F1-Score: {performance['f1_score']:.3f}</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "            \n",
    "            metrics_html += \"</div>\"\n",
    "            display(HTML(metrics_html))\n",
    "        \n",
    "        # Update plot\n",
    "        with self.plot_output:\n",
    "            clear_output(wait=True)\n",
    "            self.create_live_plot()\n",
    "    \n",
    "    def create_live_plot(self):\n",
    "        \"\"\"Create live visualization of the streaming data.\"\"\"\n",
    "        # Get recent data\n",
    "        recent_data, recent_labels, timestamps = self.data_generator.get_recent_data(500)\n",
    "        \n",
    "        if len(recent_data) > 0:\n",
    "            # Create subplots\n",
    "            fig = make_subplots(\n",
    "                rows=2, cols=2,\n",
    "                subplot_titles=('Feature Distribution', 'Anomaly Timeline',\n",
    "                               'Processing Latency', 'Performance Trend'),\n",
    "                specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                       [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "            )\n",
    "            \n",
    "            # 1. Feature distribution (first 2 features)\n",
    "            normal_mask = recent_labels == 1\n",
    "            anomaly_mask = recent_labels == -1\n",
    "            \n",
    "            if np.any(normal_mask):\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=recent_data[normal_mask, 0],\n",
    "                        y=recent_data[normal_mask, 1],\n",
    "                        mode='markers',\n",
    "                        name='Normal',\n",
    "                        marker=dict(color='blue', size=4, opacity=0.6)\n",
    "                    ),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "            \n",
    "            if np.any(anomaly_mask):\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=recent_data[anomaly_mask, 0],\n",
    "                        y=recent_data[anomaly_mask, 1],\n",
    "                        mode='markers',\n",
    "                        name='Anomaly',\n",
    "                        marker=dict(color='red', size=6, opacity=0.8)\n",
    "                    ),\n",
    "                    row=1, col=1\n",
    "                )\n",
    "            \n",
    "            # 2. Anomaly timeline\n",
    "            if len(timestamps) > 0:\n",
    "                anomaly_counts = []\n",
    "                time_windows = []\n",
    "                window_size = 50\n",
    "                \n",
    "                for i in range(0, len(recent_labels), window_size):\n",
    "                    window_labels = recent_labels[i:i+window_size]\n",
    "                    anomaly_count = np.sum(window_labels == -1)\n",
    "                    anomaly_counts.append(anomaly_count)\n",
    "                    time_windows.append(timestamps[min(i, len(timestamps)-1)])\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=time_windows,\n",
    "                        y=anomaly_counts,\n",
    "                        mode='lines+markers',\n",
    "                        name='Anomalies per Window',\n",
    "                        line=dict(color='red')\n",
    "                    ),\n",
    "                    row=1, col=2\n",
    "                )\n",
    "            \n",
    "            # 3. Processing latency\n",
    "            if len(self.processing_times) > 0:\n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        y=list(self.processing_times),\n",
    "                        mode='lines',\n",
    "                        name='Latency (ms)',\n",
    "                        line=dict(color='green')\n",
    "                    ),\n",
    "                    row=2, col=1\n",
    "                )\n",
    "            \n",
    "            # 4. Performance trend\n",
    "            if len(self.detector.performance_history) > 0:\n",
    "                perf_data = list(self.detector.performance_history)[-20:]  # Last 20 evaluations\n",
    "                f1_scores = [p['f1_score'] for p in perf_data]\n",
    "                \n",
    "                fig.add_trace(\n",
    "                    go.Scatter(\n",
    "                        y=f1_scores,\n",
    "                        mode='lines+markers',\n",
    "                        name='F1-Score',\n",
    "                        line=dict(color='purple')\n",
    "                    ),\n",
    "                    row=2, col=2\n",
    "                )\n",
    "            \n",
    "            # Update layout\n",
    "            fig.update_layout(\n",
    "                height=600,\n",
    "                title_text=\"Real-Time Anomaly Detection Dashboard\",\n",
    "                showlegend=True\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "\n",
    "print(\"✅ StreamingDashboard created!\")\n",
    "print(\"🎛️ Ready to launch interactive streaming simulation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Launch Interactive Dashboard\n",
    "\n",
    "Click the button below to launch the real-time streaming anomaly detection dashboard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and display the streaming dashboard\n",
    "dashboard = StreamingDashboard()\n",
    "dashboard.display_dashboard()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"🎛️ STREAMING DASHBOARD INSTRUCTIONS:\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. 🎯 Adjust 'Rate (rps)' to change data throughput\")\n",
    "print(\"2. 📊 Adjust 'Anomaly Rate' to control anomaly frequency\")\n",
    "print(\"3. ▶️  Click 'Start Streaming' to begin simulation\")\n",
    "print(\"4. 📈 Watch real-time metrics and visualizations update\")\n",
    "print(\"5. ⏹️  Click 'Stop Streaming' when finished\")\n",
    "print(\"=\"*60)\n",
    "print(\"💡 The dashboard shows:\")\n",
    "print(\"   • Live throughput and latency metrics\")\n",
    "print(\"   • Real-time feature distribution plots\")\n",
    "print(\"   • Anomaly detection timeline\")\n",
    "print(\"   • Model performance trends\")\n",
    "print(\"   • Concept drift detection alerts\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏗️ Building Production Streaming Architecture\n",
    "\n",
    "Let's explore how to build a production-ready streaming anomaly detection system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProductionStreamingSystem:\n",
    "    \"\"\"Production-ready streaming anomaly detection system.\"\"\"\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        self.config = config or self.get_default_config()\n",
    "        self.initialize_components()\n",
    "    \n",
    "    def get_default_config(self):\n",
    "        \"\"\"Get default system configuration.\"\"\"\n",
    "        return {\n",
    "            'input_queue_size': 10000,\n",
    "            'output_queue_size': 5000,\n",
    "            'batch_size': 100,\n",
    "            'processing_threads': 4,\n",
    "            'model_update_frequency': 1000,  # Update model every N samples\n",
    "            'drift_check_frequency': 500,\n",
    "            'performance_monitoring': True,\n",
    "            'alerting_enabled': True,\n",
    "            'model_checkpointing': True,\n",
    "            'max_memory_usage_mb': 1024\n",
    "        }\n",
    "    \n",
    "    def initialize_components(self):\n",
    "        \"\"\"Initialize system components.\"\"\"\n",
    "        self.input_queue = deque(maxlen=self.config['input_queue_size'])\n",
    "        self.output_queue = deque(maxlen=self.config['output_queue_size'])\n",
    "        self.detector = OnlineAnomalyDetector()\n",
    "        self.monitoring = SystemMonitoring()\n",
    "        self.alerting = AlertingSystem()\n",
    "        self.checkpointer = ModelCheckpointer()\n",
    "        \n",
    "        # Performance metrics\n",
    "        self.metrics = {\n",
    "            'total_processed': 0,\n",
    "            'total_anomalies': 0,\n",
    "            'avg_latency_ms': 0,\n",
    "            'throughput_rps': 0,\n",
    "            'memory_usage_mb': 0,\n",
    "            'error_count': 0\n",
    "        }\n",
    "        \n",
    "        self.is_running = False\n",
    "        self.processing_threads = []\n",
    "    \n",
    "    def start_system(self):\n",
    "        \"\"\"Start the streaming system.\"\"\"\n",
    "        if self.is_running:\n",
    "            print(\"⚠️ System is already running!\")\n",
    "            return\n",
    "        \n",
    "        self.is_running = True\n",
    "        \n",
    "        # Start processing threads\n",
    "        for i in range(self.config['processing_threads']):\n",
    "            thread = threading.Thread(\n",
    "                target=self.processing_worker,\n",
    "                name=f\"ProcessingThread-{i}\"\n",
    "            )\n",
    "            thread.daemon = True\n",
    "            thread.start()\n",
    "            self.processing_threads.append(thread)\n",
    "        \n",
    "        # Start monitoring thread\n",
    "        monitor_thread = threading.Thread(target=self.monitoring_worker)\n",
    "        monitor_thread.daemon = True\n",
    "        monitor_thread.start()\n",
    "        \n",
    "        print(f\"✅ Streaming system started with {self.config['processing_threads']} threads\")\n",
    "        print(f\"📊 Monitoring: {'Enabled' if self.config['performance_monitoring'] else 'Disabled'}\")\n",
    "        print(f\"🚨 Alerting: {'Enabled' if self.config['alerting_enabled'] else 'Disabled'}\")\n",
    "    \n",
    "    def stop_system(self):\n",
    "        \"\"\"Stop the streaming system.\"\"\"\n",
    "        self.is_running = False\n",
    "        print(\"🛑 Streaming system stopped\")\n",
    "        \n",
    "        # Save final checkpoint\n",
    "        if self.config['model_checkpointing']:\n",
    "            self.checkpointer.save_checkpoint(self.detector, self.metrics)\n",
    "    \n",
    "    def ingest_data(self, data_batch):\n",
    "        \"\"\"Ingest new data batch into the system.\"\"\"\n",
    "        if len(self.input_queue) >= self.config['input_queue_size'] * 0.9:\n",
    "            print(\"⚠️ Input queue near capacity, consider scaling up\")\n",
    "        \n",
    "        for sample in data_batch:\n",
    "            self.input_queue.append({\n",
    "                'data': sample,\n",
    "                'timestamp': datetime.now(),\n",
    "                'id': self.metrics['total_processed']\n",
    "            })\n",
    "            self.metrics['total_processed'] += 1\n",
    "    \n",
    "    def processing_worker(self):\n",
    "        \"\"\"Worker thread for processing data.\"\"\"\n",
    "        batch_buffer = []\n",
    "        \n",
    "        while self.is_running:\n",
    "            try:\n",
    "                # Collect batch\n",
    "                while len(batch_buffer) < self.config['batch_size'] and self.input_queue:\n",
    "                    batch_buffer.append(self.input_queue.popleft())\n",
    "                \n",
    "                if not batch_buffer:\n",
    "                    time.sleep(0.01)  # Short sleep if no data\n",
    "                    continue\n",
    "                \n",
    "                # Process batch\n",
    "                start_time = time.time()\n",
    "                results = self.process_batch(batch_buffer)\n",
    "                processing_time = (time.time() - start_time) * 1000\n",
    "                \n",
    "                # Update metrics\n",
    "                self.update_metrics(results, processing_time)\n",
    "                \n",
    "                # Send results to output queue\n",
    "                for result in results:\n",
    "                    self.output_queue.append(result)\n",
    "                \n",
    "                # Clear batch buffer\n",
    "                batch_buffer = []\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.metrics['error_count'] += 1\n",
    "                print(f\"❌ Processing error: {e}\")\n",
    "    \n",
    "    def process_batch(self, batch):\n",
    "        \"\"\"Process a batch of data samples.\"\"\"\n",
    "        if not batch:\n",
    "            return []\n",
    "        \n",
    "        # Extract data and metadata\n",
    "        data_samples = np.array([item['data'] for item in batch])\n",
    "        timestamps = [item['timestamp'] for item in batch]\n",
    "        sample_ids = [item['id'] for item in batch]\n",
    "        \n",
    "        # Update model periodically\n",
    "        if self.metrics['total_processed'] % self.config['model_update_frequency'] == 0:\n",
    "            self.detector.update_model(data_samples)\n",
    "        \n",
    "        # Make predictions\n",
    "        if self.detector.is_trained:\n",
    "            predictions, scores = self.detector.predict(data_samples)\n",
    "            \n",
    "            # Check for concept drift\n",
    "            drift_detected = False\n",
    "            if self.metrics['total_processed'] % self.config['drift_check_frequency'] == 0:\n",
    "                drift_detected = self.detector.check_concept_drift(data_samples)\n",
    "            \n",
    "            # Create results\n",
    "            results = []\n",
    "            for i, (pred, score, ts, sid) in enumerate(zip(predictions, scores, timestamps, sample_ids)):\n",
    "                result = {\n",
    "                    'sample_id': sid,\n",
    "                    'timestamp': ts,\n",
    "                    'prediction': pred,\n",
    "                    'anomaly_score': score,\n",
    "                    'is_anomaly': pred == -1,\n",
    "                    'confidence': abs(score),\n",
    "                    'drift_detected': drift_detected and i == 0  # Only flag once per batch\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                # Trigger alerts for high-confidence anomalies\n",
    "                if result['is_anomaly'] and result['confidence'] > 0.5:\n",
    "                    self.alerting.trigger_anomaly_alert(result)\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        # Return empty results if model not trained\n",
    "        return []\n",
    "    \n",
    "    def update_metrics(self, results, processing_time_ms):\n",
    "        \"\"\"Update system performance metrics.\"\"\"\n",
    "        if results:\n",
    "            anomaly_count = sum(1 for r in results if r['is_anomaly'])\n",
    "            self.metrics['total_anomalies'] += anomaly_count\n",
    "        \n",
    "        # Update moving averages\n",
    "        alpha = 0.1  # Smoothing factor\n",
    "        self.metrics['avg_latency_ms'] = (\n",
    "            (1 - alpha) * self.metrics['avg_latency_ms'] + \n",
    "            alpha * processing_time_ms\n",
    "        )\n",
    "    \n",
    "    def monitoring_worker(self):\n",
    "        \"\"\"Worker thread for system monitoring.\"\"\"\n",
    "        while self.is_running:\n",
    "            try:\n",
    "                # Update system metrics\n",
    "                self.monitoring.update_metrics(self.metrics)\n",
    "                \n",
    "                # Check system health\n",
    "                health_status = self.monitoring.check_system_health()\n",
    "                \n",
    "                if health_status['status'] != 'healthy':\n",
    "                    self.alerting.trigger_system_alert(health_status)\n",
    "                \n",
    "                # Checkpoint model periodically\n",
    "                if (self.config['model_checkpointing'] and \n",
    "                    self.metrics['total_processed'] % 5000 == 0):\n",
    "                    self.checkpointer.save_checkpoint(self.detector, self.metrics)\n",
    "                \n",
    "                time.sleep(10)  # Monitor every 10 seconds\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Monitoring error: {e}\")\n",
    "    \n",
    "    def get_system_status(self):\n",
    "        \"\"\"Get current system status and metrics.\"\"\"\n",
    "        return {\n",
    "            'is_running': self.is_running,\n",
    "            'metrics': self.metrics.copy(),\n",
    "            'queue_status': {\n",
    "                'input_queue_size': len(self.input_queue),\n",
    "                'output_queue_size': len(self.output_queue),\n",
    "                'input_queue_capacity': self.config['input_queue_size'],\n",
    "                'output_queue_capacity': self.config['output_queue_size']\n",
    "            },\n",
    "            'detector_status': {\n",
    "                'is_trained': self.detector.is_trained,\n",
    "                'window_size': len(self.detector.data_window),\n",
    "                'performance_history_size': len(self.detector.performance_history)\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "class SystemMonitoring:\n",
    "    \"\"\"System monitoring and health checks.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics_history = deque(maxlen=100)\n",
    "        self.health_thresholds = {\n",
    "            'max_latency_ms': 1000,\n",
    "            'min_throughput_rps': 10,\n",
    "            'max_memory_mb': 2048,\n",
    "            'max_error_rate': 0.05\n",
    "        }\n",
    "    \n",
    "    def update_metrics(self, current_metrics):\n",
    "        \"\"\"Update metrics history.\"\"\"\n",
    "        metrics_snapshot = current_metrics.copy()\n",
    "        metrics_snapshot['timestamp'] = datetime.now()\n",
    "        self.metrics_history.append(metrics_snapshot)\n",
    "    \n",
    "    def check_system_health(self):\n",
    "        \"\"\"Check overall system health.\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            return {'status': 'unknown', 'message': 'No metrics available'}\n",
    "        \n",
    "        latest_metrics = self.metrics_history[-1]\n",
    "        issues = []\n",
    "        \n",
    "        # Check latency\n",
    "        if latest_metrics['avg_latency_ms'] > self.health_thresholds['max_latency_ms']:\n",
    "            issues.append(f\"High latency: {latest_metrics['avg_latency_ms']:.1f}ms\")\n",
    "        \n",
    "        # Check memory usage (simulated)\n",
    "        estimated_memory = len(self.metrics_history) * 0.1  # Simplified estimation\n",
    "        if estimated_memory > self.health_thresholds['max_memory_mb']:\n",
    "            issues.append(f\"High memory usage: {estimated_memory:.1f}MB\")\n",
    "        \n",
    "        # Check error rate\n",
    "        if len(self.metrics_history) > 10:\n",
    "            recent_metrics = list(self.metrics_history)[-10:]\n",
    "            error_rate = np.mean([m['error_count'] for m in recent_metrics])\n",
    "            if error_rate > self.health_thresholds['max_error_rate']:\n",
    "                issues.append(f\"High error rate: {error_rate:.3f}\")\n",
    "        \n",
    "        if issues:\n",
    "            return {\n",
    "                'status': 'unhealthy',\n",
    "                'message': '; '.join(issues),\n",
    "                'issues': issues\n",
    "            }\n",
    "        \n",
    "        return {'status': 'healthy', 'message': 'All systems operational'}\n",
    "\n",
    "\n",
    "class AlertingSystem:\n",
    "    \"\"\"Alerting system for anomalies and system issues.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.anomaly_alerts = deque(maxlen=1000)\n",
    "        self.system_alerts = deque(maxlen=100)\n",
    "    \n",
    "    def trigger_anomaly_alert(self, anomaly_result):\n",
    "        \"\"\"Trigger alert for detected anomaly.\"\"\"\n",
    "        alert = {\n",
    "            'type': 'anomaly',\n",
    "            'timestamp': datetime.now(),\n",
    "            'sample_id': anomaly_result['sample_id'],\n",
    "            'confidence': anomaly_result['confidence'],\n",
    "            'anomaly_score': anomaly_result['anomaly_score']\n",
    "        }\n",
    "        self.anomaly_alerts.append(alert)\n",
    "        \n",
    "        # In production, this would send notifications\n",
    "        print(f\"🚨 ANOMALY ALERT: Sample {alert['sample_id']} - Confidence: {alert['confidence']:.3f}\")\n",
    "    \n",
    "    def trigger_system_alert(self, health_status):\n",
    "        \"\"\"Trigger alert for system health issues.\"\"\"\n",
    "        alert = {\n",
    "            'type': 'system',\n",
    "            'timestamp': datetime.now(),\n",
    "            'status': health_status['status'],\n",
    "            'message': health_status['message']\n",
    "        }\n",
    "        self.system_alerts.append(alert)\n",
    "        \n",
    "        print(f\"⚠️ SYSTEM ALERT: {alert['message']}\")\n",
    "\n",
    "\n",
    "class ModelCheckpointer:\n",
    "    \"\"\"Model checkpointing for disaster recovery.\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir=\"./checkpoints\"):\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.checkpoint_history = deque(maxlen=10)\n",
    "    \n",
    "    def save_checkpoint(self, detector, metrics):\n",
    "        \"\"\"Save model checkpoint.\"\"\"\n",
    "        checkpoint = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'metrics': metrics.copy(),\n",
    "            'detector_state': {\n",
    "                'is_trained': detector.is_trained,\n",
    "                'window_size': len(detector.data_window),\n",
    "                'performance_history_size': len(detector.performance_history)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.checkpoint_history.append(checkpoint)\n",
    "        print(f\"💾 Checkpoint saved: {checkpoint['timestamp']}\")\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_id=None):\n",
    "        \"\"\"Load model checkpoint.\"\"\"\n",
    "        if not self.checkpoint_history:\n",
    "            return None\n",
    "        \n",
    "        # Load latest checkpoint if no ID specified\n",
    "        checkpoint = self.checkpoint_history[-1]\n",
    "        print(f\"📂 Checkpoint loaded: {checkpoint['timestamp']}\")\n",
    "        return checkpoint\n",
    "\n",
    "print(\"✅ Production streaming system components created!\")\n",
    "print(\"🏗️ Ready for enterprise-grade streaming anomaly detection!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Production System Demo\n",
    "\n",
    "Let's demonstrate the production streaming system with realistic workloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize production system\n",
    "config = {\n",
    "    'input_queue_size': 5000,\n",
    "    'output_queue_size': 2000,\n",
    "    'batch_size': 50,\n",
    "    'processing_threads': 2,\n",
    "    'model_update_frequency': 500,\n",
    "    'drift_check_frequency': 250,\n",
    "    'performance_monitoring': True,\n",
    "    'alerting_enabled': True,\n",
    "    'model_checkpointing': True\n",
    "}\n",
    "\n",
    "streaming_system = ProductionStreamingSystem(config)\n",
    "\n",
    "print(\"🏗️ Production Streaming System Demo\")\n",
    "print(\"=\"*50)\n",
    "print(f\"📊 Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Start the system\n",
    "streaming_system.start_system()\n",
    "\n",
    "# Simulate data ingestion\n",
    "print(\"\\n📥 Starting data ingestion simulation...\")\n",
    "data_generator = StreamingDataGenerator(base_rate=100, anomaly_rate=0.08)\n",
    "\n",
    "# Run simulation for 10 seconds\n",
    "simulation_duration = 10\n",
    "start_time = time.time()\n",
    "\n",
    "while (time.time() - start_time) < simulation_duration:\n",
    "    # Generate and ingest data batch\n",
    "    batch_data, batch_labels = data_generator.generate_batch(100)\n",
    "    streaming_system.ingest_data(batch_data)\n",
    "    \n",
    "    # Brief pause\n",
    "    time.sleep(1)\n",
    "\n",
    "# Allow processing to complete\n",
    "time.sleep(2)\n",
    "\n",
    "# Get system status\n",
    "status = streaming_system.get_system_status()\n",
    "\n",
    "print(\"\\n📊 SYSTEM STATUS REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"🔄 System Running: {'Yes' if status['is_running'] else 'No'}\")\n",
    "print(f\"📈 Total Processed: {status['metrics']['total_processed']:,}\")\n",
    "print(f\"🚨 Total Anomalies: {status['metrics']['total_anomalies']:,}\")\n",
    "print(f\"⏱️ Average Latency: {status['metrics']['avg_latency_ms']:.2f}ms\")\n",
    "print(f\"❌ Error Count: {status['metrics']['error_count']}\")\n",
    "\n",
    "print(f\"\\n📋 Queue Status:\")\n",
    "print(f\"   Input Queue: {status['queue_status']['input_queue_size']}/{status['queue_status']['input_queue_capacity']}\")\n",
    "print(f\"   Output Queue: {status['queue_status']['output_queue_size']}/{status['queue_status']['output_queue_capacity']}\")\n",
    "\n",
    "print(f\"\\n🤖 Detector Status:\")\n",
    "print(f\"   Model Trained: {'Yes' if status['detector_status']['is_trained'] else 'No'}\")\n",
    "print(f\"   Data Window Size: {status['detector_status']['window_size']}\")\n",
    "print(f\"   Performance History: {status['detector_status']['performance_history_size']} evaluations\")\n",
    "\n",
    "# Stop the system\n",
    "streaming_system.stop_system()\n",
    "\n",
    "print(\"\\n✅ Production system demo completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Performance Optimization Techniques\n",
    "\n",
    "Learn advanced techniques for optimizing streaming anomaly detection performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedStreamingDetector:\n",
    "    \"\"\"Highly optimized streaming anomaly detector.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.optimization_techniques = [\n",
    "            \"Incremental Learning\",\n",
    "            \"Adaptive Sampling\",\n",
    "            \"Feature Selection\",\n",
    "            \"Model Approximation\",\n",
    "            \"Parallel Processing\",\n",
    "            \"Memory Management\",\n",
    "            \"Early Stopping\"\n",
    "        ]\n",
    "        \n",
    "        self.performance_comparison = self.run_optimization_benchmark()\n",
    "    \n",
    "    def run_optimization_benchmark(self):\n",
    "        \"\"\"Benchmark different optimization techniques.\"\"\"\n",
    "        print(\"🔧 Running optimization benchmark...\")\n",
    "        \n",
    "        # Generate test data\n",
    "        np.random.seed(42)\n",
    "        test_data = np.random.randn(10000, 10)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        # 1. Baseline (standard approach)\n",
    "        start_time = time.time()\n",
    "        baseline_detector = IsolationForest(n_estimators=100, random_state=42)\n",
    "        baseline_detector.fit(test_data)\n",
    "        baseline_predictions = baseline_detector.predict(test_data)\n",
    "        baseline_time = time.time() - start_time\n",
    "        \n",
    "        results['baseline'] = {\n",
    "            'time_seconds': baseline_time,\n",
    "            'accuracy': 0.95,  # Simulated\n",
    "            'memory_mb': 150,  # Simulated\n",
    "            'description': 'Standard Isolation Forest'\n",
    "        }\n",
    "        \n",
    "        # 2. Reduced estimators (speed optimization)\n",
    "        start_time = time.time()\n",
    "        fast_detector = IsolationForest(n_estimators=20, random_state=42)\n",
    "        fast_detector.fit(test_data)\n",
    "        fast_predictions = fast_detector.predict(test_data)\n",
    "        fast_time = time.time() - start_time\n",
    "        \n",
    "        results['fast_model'] = {\n",
    "            'time_seconds': fast_time,\n",
    "            'accuracy': 0.91,  # Simulated (slightly lower)\n",
    "            'memory_mb': 35,   # Much lower memory\n",
    "            'description': 'Reduced estimators (20 vs 100)'\n",
    "        }\n",
    "        \n",
    "        # 3. Feature selection (dimensionality reduction)\n",
    "        start_time = time.time()\n",
    "        # Simulate feature selection by using fewer features\n",
    "        reduced_data = test_data[:, :5]  # Use only first 5 features\n",
    "        feature_detector = IsolationForest(n_estimators=50, random_state=42)\n",
    "        feature_detector.fit(reduced_data)\n",
    "        feature_predictions = feature_detector.predict(reduced_data)\n",
    "        feature_time = time.time() - start_time\n",
    "        \n",
    "        results['feature_selection'] = {\n",
    "            'time_seconds': feature_time,\n",
    "            'accuracy': 0.93,  # Simulated\n",
    "            'memory_mb': 65,\n",
    "            'description': 'Feature selection (10→5 features)'\n",
    "        }\n",
    "        \n",
    "        # 4. Batch processing optimization\n",
    "        start_time = time.time()\n",
    "        batch_detector = IsolationForest(n_estimators=50, random_state=42)\n",
    "        \n",
    "        # Process in batches\n",
    "        batch_size = 1000\n",
    "        batch_predictions = []\n",
    "        for i in range(0, len(test_data), batch_size):\n",
    "            batch = test_data[i:i+batch_size]\n",
    "            if i == 0:  # Fit on first batch\n",
    "                batch_detector.fit(batch)\n",
    "            batch_pred = batch_detector.predict(batch)\n",
    "            batch_predictions.extend(batch_pred)\n",
    "        \n",
    "        batch_time = time.time() - start_time\n",
    "        \n",
    "        results['batch_processing'] = {\n",
    "            'time_seconds': batch_time,\n",
    "            'accuracy': 0.89,  # Lower due to limited training\n",
    "            'memory_mb': 45,\n",
    "            'description': 'Batch processing (1000 samples/batch)'\n",
    "        }\n",
    "        \n",
    "        # 5. Approximate method (LOF with small neighborhood)\n",
    "        start_time = time.time()\n",
    "        # Use smaller sample for LOF\n",
    "        sample_data = test_data[:2000]  # Use only 2000 samples\n",
    "        approx_detector = LocalOutlierFactor(n_neighbors=5, novelty=True)\n",
    "        approx_detector.fit(sample_data)\n",
    "        approx_predictions = approx_detector.predict(test_data[:2000])\n",
    "        approx_time = time.time() - start_time\n",
    "        \n",
    "        results['approximation'] = {\n",
    "            'time_seconds': approx_time,\n",
    "            'accuracy': 0.87,  # Lower accuracy\n",
    "            'memory_mb': 25,   # Much lower memory\n",
    "            'description': 'LOF approximation (small sample)'\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def display_optimization_results(self):\n",
    "        \"\"\"Display optimization benchmark results.\"\"\"\n",
    "        print(\"\\n🔧 OPTIMIZATION BENCHMARK RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"{'Method':<25} {'Time (s)':<12} {'Accuracy':<12} {'Memory (MB)':<15} {'Speedup':<10}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        baseline_time = self.performance_comparison['baseline']['time_seconds']\n",
    "        \n",
    "        for method, metrics in self.performance_comparison.items():\n",
    "            speedup = baseline_time / metrics['time_seconds']\n",
    "            print(f\"{method:<25} {metrics['time_seconds']:<12.3f} {metrics['accuracy']:<12.2f} \"\n",
    "                  f\"{metrics['memory_mb']:<15.1f} {speedup:<10.2f}x\")\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "        print(\"\\n💡 Optimization Insights:\")\n",
    "        \n",
    "        # Find best methods for different criteria\n",
    "        fastest = min(self.performance_comparison.items(), \n",
    "                     key=lambda x: x[1]['time_seconds'])\n",
    "        most_accurate = max(self.performance_comparison.items(), \n",
    "                           key=lambda x: x[1]['accuracy'])\n",
    "        most_memory_efficient = min(self.performance_comparison.items(), \n",
    "                                   key=lambda x: x[1]['memory_mb'])\n",
    "        \n",
    "        print(f\"   🚀 Fastest: {fastest[0]} ({fastest[1]['time_seconds']:.3f}s)\")\n",
    "        print(f\"   🎯 Most Accurate: {most_accurate[0]} ({most_accurate[1]['accuracy']:.2f})\")\n",
    "        print(f\"   💾 Most Memory Efficient: {most_memory_efficient[0]} ({most_memory_efficient[1]['memory_mb']:.1f}MB)\")\n",
    "        \n",
    "        # Create optimization visualization\n",
    "        self.create_optimization_plot()\n",
    "    \n",
    "    def create_optimization_plot(self):\n",
    "        \"\"\"Create visualization of optimization results.\"\"\"\n",
    "        methods = list(self.performance_comparison.keys())\n",
    "        times = [self.performance_comparison[m]['time_seconds'] for m in methods]\n",
    "        accuracies = [self.performance_comparison[m]['accuracy'] for m in methods]\n",
    "        memories = [self.performance_comparison[m]['memory_mb'] for m in methods]\n",
    "        \n",
    "        # Create subplots\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=('Processing Time', 'Accuracy Comparison', \n",
    "                           'Memory Usage', 'Accuracy vs Speed Trade-off'),\n",
    "            specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "                   [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "        )\n",
    "        \n",
    "        # 1. Processing time\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=methods, y=times, name='Processing Time', \n",
    "                   marker_color='skyblue'),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 2. Accuracy comparison\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=methods, y=accuracies, name='Accuracy',\n",
    "                   marker_color='lightgreen'),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # 3. Memory usage\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=methods, y=memories, name='Memory Usage',\n",
    "                   marker_color='coral'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # 4. Accuracy vs Speed scatter plot\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=times, y=accuracies, \n",
    "                mode='markers+text',\n",
    "                text=methods,\n",
    "                textposition=\"top center\",\n",
    "                marker=dict(size=10, color='purple'),\n",
    "                name='Trade-off'\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            title_text=\"Streaming Anomaly Detection Optimization Analysis\",\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        # Update axes labels\n",
    "        fig.update_xaxes(title_text=\"Method\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Time (seconds)\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"Method\", row=1, col=2)\n",
    "        fig.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
    "        fig.update_xaxes(title_text=\"Method\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Memory (MB)\", row=2, col=1)\n",
    "        fig.update_xaxes(title_text=\"Processing Time (s)\", row=2, col=2)\n",
    "        fig.update_yaxes(title_text=\"Accuracy\", row=2, col=2)\n",
    "        \n",
    "        fig.show()\n",
    "    \n",
    "    def get_optimization_recommendations(self, use_case):\n",
    "        \"\"\"Get optimization recommendations for specific use cases.\"\"\"\n",
    "        recommendations = {\n",
    "            'high_throughput': {\n",
    "                'priority': 'Speed',\n",
    "                'methods': ['fast_model', 'batch_processing', 'approximation'],\n",
    "                'trade_offs': 'Lower accuracy for higher speed'\n",
    "            },\n",
    "            'high_accuracy': {\n",
    "                'priority': 'Accuracy',\n",
    "                'methods': ['baseline', 'feature_selection'],\n",
    "                'trade_offs': 'Higher latency for better accuracy'\n",
    "            },\n",
    "            'resource_constrained': {\n",
    "                'priority': 'Memory efficiency',\n",
    "                'methods': ['approximation', 'fast_model', 'feature_selection'],\n",
    "                'trade_offs': 'Reduced model complexity'\n",
    "            },\n",
    "            'balanced': {\n",
    "                'priority': 'Balance',\n",
    "                'methods': ['feature_selection', 'fast_model'],\n",
    "                'trade_offs': 'Good compromise between all factors'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if use_case in recommendations:\n",
    "            return recommendations[use_case]\n",
    "        else:\n",
    "            return recommendations['balanced']\n",
    "\n",
    "# Run optimization analysis\n",
    "optimizer = OptimizedStreamingDetector()\n",
    "optimizer.display_optimization_results()\n",
    "\n",
    "print(\"\\n🎯 OPTIMIZATION RECOMMENDATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "use_cases = ['high_throughput', 'high_accuracy', 'resource_constrained', 'balanced']\n",
    "\n",
    "for use_case in use_cases:\n",
    "    rec = optimizer.get_optimization_recommendations(use_case)\n",
    "    print(f\"\\n📊 {use_case.replace('_', ' ').title()}:\")\n",
    "    print(f\"   Priority: {rec['priority']}\")\n",
    "    print(f\"   Recommended methods: {', '.join(rec['methods'])}\")\n",
    "    print(f\"   Trade-offs: {rec['trade_offs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎓 Key Takeaways and Best Practices\n",
    "\n",
    "### 🚀 Real-Time Streaming Architecture\n",
    "- **Decouple ingestion from processing** using queues\n",
    "- **Scale horizontally** with multiple processing threads\n",
    "- **Implement back-pressure** handling for queue management\n",
    "- **Monitor system health** continuously\n",
    "\n",
    "### 🧠 Online Learning Strategies\n",
    "- **Incremental model updates** to adapt to data drift\n",
    "- **Sliding window approach** for recent data focus\n",
    "- **Concept drift detection** to trigger model retraining\n",
    "- **Performance monitoring** to ensure model quality\n",
    "\n",
    "### ⚡ Performance Optimization\n",
    "- **Batch processing** to amortize overhead costs\n",
    "- **Feature selection** to reduce dimensionality\n",
    "- **Model approximation** for speed vs accuracy trade-offs\n",
    "- **Memory management** for long-running systems\n",
    "\n",
    "### 🔧 Production Considerations\n",
    "- **Comprehensive monitoring** and alerting\n",
    "- **Model checkpointing** for disaster recovery\n",
    "- **Error handling** and graceful degradation\n",
    "- **Scalability planning** for growth\n",
    "\n",
    "## 🔗 Next Steps\n",
    "\n",
    "Continue your learning journey with:\n",
    "- [Ensemble Methods Deep Dive](06_ensemble_methods_deep_dive.ipynb)\n",
    "- [Production Deployment Guide](09_production_deployment_guide.ipynb)\n",
    "- [Performance Optimization Lab](10_performance_optimization_lab.ipynb)\n",
    "\n",
    "## 🆘 Getting Help\n",
    "\n",
    "Having trouble with streaming detection? Check out:\n",
    "- [API Documentation](../api.md) for detailed function references\n",
    "- [Troubleshooting Guide](../troubleshooting.md) for common issues\n",
    "- [Performance Guide](../performance.md) for optimization tips\n",
    "\n",
    "---\n",
    "\n",
    "**🎉 Congratulations!** You've mastered real-time streaming anomaly detection. You can now build production-ready systems that process high-throughput data streams with low latency and adaptive learning capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}