{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection Experiment Template\n",
    "\n",
    "**Purpose**: Comprehensive template for anomaly detection experiments using Pynomaly\n",
    "\n",
    "**Version**: 1.0.0\n",
    "\n",
    "**Created**: 2025-06-24\n",
    "\n",
    "**Author**: [Your Name]\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Experiment Overview\n",
    "\n",
    "This notebook provides a standardized framework for conducting anomaly detection experiments. It includes:\n",
    "\n",
    "- üìä **Data exploration and quality assessment**\n",
    "- üîß **Data preprocessing and feature engineering**\n",
    "- ü§ñ **Multiple algorithm comparison**\n",
    "- üìà **Performance evaluation and statistical testing**\n",
    "- üìä **Comprehensive visualization and reporting**\n",
    "- üîÑ **Reproducible experiment workflow**\n",
    "\n",
    "### üéØ Experiment Objectives\n",
    "\n",
    "1. [Define your primary objective here]\n",
    "2. [Define your secondary objective here]\n",
    "3. [Define success criteria here]\n",
    "\n",
    "### üìù Experiment Metadata\n",
    "\n",
    "- **Dataset**: [Dataset name and description]\n",
    "- **Target**: [What you're trying to detect]\n",
    "- **Algorithms**: [List of algorithms to compare]\n",
    "- **Evaluation**: [Primary evaluation metric]\n",
    "- **Timeline**: [Expected duration]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scientific computing\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Pynomaly imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'src'))\n",
    "\n",
    "from pynomaly.infrastructure.adapters.sklearn_adapter import SklearnAdapter\n",
    "from pynomaly.infrastructure.adapters.pyod_adapter import PyODAdapter\n",
    "from pynomaly.domain.entities.dataset import Dataset\n",
    "from pynomaly.domain.value_objects.contamination_rate import ContaminationRate\n",
    "\n",
    "# Visualization setup\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ Setup complete\")\n",
    "print(f\"üìÖ Experiment started: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìÇ Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = \"data/your_dataset.csv\"  # Update this path\n",
    "TARGET_COLUMN = \"is_anomaly\"  # Update if different\n",
    "ID_COLUMNS = [\"id\"]  # Columns to exclude from features\n",
    "\n",
    "# Load data\n",
    "try:\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    print(f\"‚úÖ Data loaded successfully: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå File not found: {DATA_PATH}\")\n",
    "    print(\"Please update DATA_PATH with the correct file path\")\n",
    "    # Create sample data for demonstration\n",
    "    from sklearn.datasets import make_classification\n",
    "    X, y = make_classification(n_samples=1000, n_features=10, n_clusters_per_class=1, \n",
    "                             n_redundant=0, random_state=RANDOM_STATE)\n",
    "    df = pd.DataFrame(X, columns=[f'feature_{i}' for i in range(10)])\n",
    "    df[TARGET_COLUMN] = y\n",
    "    print(f\"üìä Using sample data: {df.shape}\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nüìä Dataset shape: {df.shape}\")\n",
    "print(f\"üìã Columns: {list(df.columns)}\")\n",
    "print(f\"üéØ Target column: {TARGET_COLUMN}\")\n",
    "\n",
    "# Check if target exists\n",
    "if TARGET_COLUMN in df.columns:\n",
    "    print(f\"\\nüéØ Target distribution:\")\n",
    "    print(df[TARGET_COLUMN].value_counts())\n",
    "    print(f\"üìä Anomaly rate: {df[TARGET_COLUMN].mean():.1%}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Target column '{TARGET_COLUMN}' not found - assuming unsupervised learning\")\n",
    "    TARGET_COLUMN = None\n",
    "\n",
    "# Display first few rows\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df):\n",
    "    \"\"\"Comprehensive data quality assessment.\"\"\"\n",
    "    print(\"=== DATA QUALITY ASSESSMENT ===\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(f\"\\nüìä Basic Statistics:\")\n",
    "    print(f\"  ‚Ä¢ Total rows: {len(df):,}\")\n",
    "    print(f\"  ‚Ä¢ Total columns: {len(df.columns)}\")\n",
    "    print(f\"  ‚Ä¢ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_summary = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Missing %': missing_pct\n",
    "    }).sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    print(f\"\\n‚ùì Missing Values:\")\n",
    "    if missing.sum() > 0:\n",
    "        print(missing_summary[missing_summary['Missing Count'] > 0])\n",
    "    else:\n",
    "        print(\"  ‚úÖ No missing values found\")\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nüîÑ Duplicate rows: {duplicates} ({duplicates/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Data types\n",
    "    print(f\"\\nüìã Data Types:\")\n",
    "    dtype_summary = df.dtypes.value_counts()\n",
    "    for dtype, count in dtype_summary.items():\n",
    "        print(f\"  ‚Ä¢ {dtype}: {count} columns\")\n",
    "    \n",
    "    # Numeric columns analysis\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"\\nüìà Numeric Columns Summary:\")\n",
    "        display(df[numeric_cols].describe().round(3))\n",
    "    \n",
    "    # Categorical columns analysis\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"\\nüìù Categorical Columns:\")\n",
    "        for col in categorical_cols:\n",
    "            unique_count = df[col].nunique()\n",
    "            print(f\"  ‚Ä¢ {col}: {unique_count} unique values\")\n",
    "            if unique_count <= 10:\n",
    "                print(f\"    Values: {list(df[col].unique())}\")\n",
    "    \n",
    "    return missing_summary, duplicates\n",
    "\n",
    "# Perform assessment\n",
    "missing_summary, duplicates = assess_data_quality(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "if TARGET_COLUMN:\n",
    "    numeric_cols = [col for col in numeric_cols if col != TARGET_COLUMN]\n",
    "\n",
    "if len(numeric_cols) > 1:\n",
    "    # Correlation matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    # Create heatmap\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    plt.title('Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # High correlation pairs\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.8:\n",
    "                high_corr_pairs.append((\n",
    "                    correlation_matrix.columns[i], \n",
    "                    correlation_matrix.columns[j], \n",
    "                    corr_val\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(\"üîó High correlation pairs (|r| > 0.8):\")\n",
    "        for col1, col2, corr in high_corr_pairs:\n",
    "            print(f\"  ‚Ä¢ {col1} ‚Üî {col2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"‚úÖ No highly correlated feature pairs found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution analysis\n",
    "if len(numeric_cols) > 0:\n",
    "    # Select up to 6 most important features for visualization\n",
    "    viz_cols = numeric_cols[:6]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle('Feature Distributions', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(viz_cols):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            \n",
    "            # Plot distribution\n",
    "            if TARGET_COLUMN and TARGET_COLUMN in df.columns:\n",
    "                # Separate by target class\n",
    "                normal_data = df[df[TARGET_COLUMN] == 0][col].dropna()\n",
    "                anomaly_data = df[df[TARGET_COLUMN] == 1][col].dropna()\n",
    "                \n",
    "                ax.hist(normal_data, bins=30, alpha=0.7, label='Normal', density=True)\n",
    "                ax.hist(anomaly_data, bins=30, alpha=0.7, label='Anomaly', density=True)\n",
    "                ax.legend()\n",
    "            else:\n",
    "                # Single distribution\n",
    "                ax.hist(df[col].dropna(), bins=30, alpha=0.7, density=True)\n",
    "            \n",
    "            ax.set_title(f'{col}', fontweight='bold')\n",
    "            ax.set_ylabel('Density')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(viz_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, target_column=None, id_columns=None):\n",
    "    \"\"\"Comprehensive data preprocessing pipeline.\"\"\"\n",
    "    print(\"=== DATA PREPROCESSING ===\")\n",
    "    \n",
    "    # Make a copy\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Remove ID columns\n",
    "    if id_columns:\n",
    "        id_cols_present = [col for col in id_columns if col in df_processed.columns]\n",
    "        if id_cols_present:\n",
    "            df_processed = df_processed.drop(columns=id_cols_present)\n",
    "            print(f\"üóëÔ∏è Removed ID columns: {id_cols_present}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    if target_column and target_column in df_processed.columns:\n",
    "        y = df_processed[target_column]\n",
    "        X = df_processed.drop(columns=[target_column])\n",
    "        print(f\"üéØ Target separated: {target_column}\")\n",
    "    else:\n",
    "        y = None\n",
    "        X = df_processed\n",
    "        print(\"üìä Unsupervised mode: no target column\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    missing_before = X.isnull().sum().sum()\n",
    "    if missing_before > 0:\n",
    "        print(f\"‚ùì Handling {missing_before} missing values...\")\n",
    "        \n",
    "        # Numeric columns: fill with median\n",
    "        numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if X[col].isnull().any():\n",
    "                median_val = X[col].median()\n",
    "                X[col].fillna(median_val, inplace=True)\n",
    "                print(f\"  ‚Ä¢ {col}: filled with median ({median_val:.3f})\")\n",
    "        \n",
    "        # Categorical columns: fill with mode\n",
    "        categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_cols:\n",
    "            if X[col].isnull().any():\n",
    "                mode_val = X[col].mode().iloc[0] if not X[col].mode().empty else 'Unknown'\n",
    "                X[col].fillna(mode_val, inplace=True)\n",
    "                print(f\"  ‚Ä¢ {col}: filled with mode ({mode_val})\")\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"üè∑Ô∏è Encoding {len(categorical_cols)} categorical columns...\")\n",
    "        \n",
    "        for col in categorical_cols:\n",
    "            unique_count = X[col].nunique()\n",
    "            if unique_count <= 10:  # One-hot encode low cardinality\n",
    "                dummies = pd.get_dummies(X[col], prefix=col, drop_first=True)\n",
    "                X = pd.concat([X.drop(columns=[col]), dummies], axis=1)\n",
    "                print(f\"  ‚Ä¢ {col}: one-hot encoded ({unique_count} categories)\")\n",
    "            else:  # Label encode high cardinality\n",
    "                le = LabelEncoder()\n",
    "                X[col] = le.fit_transform(X[col])\n",
    "                print(f\"  ‚Ä¢ {col}: label encoded ({unique_count} categories)\")\n",
    "    \n",
    "    # Scale numeric features\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(f\"üìè Scaling {len(numeric_cols)} numeric features...\")\n",
    "        scaler = StandardScaler()\n",
    "        X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
    "        print(f\"  ‚úÖ StandardScaler applied\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Preprocessing complete\")\n",
    "    print(f\"  ‚Ä¢ Final shape: {X.shape}\")\n",
    "    print(f\"  ‚Ä¢ Feature columns: {len(X.columns)}\")\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Apply preprocessing\n",
    "X, y = preprocess_data(df, TARGET_COLUMN, ID_COLUMNS)\n",
    "\n",
    "# Display processed data info\n",
    "print(f\"\\nüìä Processed data summary:\")\n",
    "print(f\"  ‚Ä¢ Features shape: {X.shape}\")\n",
    "if y is not None:\n",
    "    print(f\"  ‚Ä¢ Target shape: {y.shape}\")\n",
    "    print(f\"  ‚Ä¢ Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Show sample of processed features\n",
    "display(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Algorithm Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm configuration\n",
    "CONTAMINATION_RATE = 0.1  # Expected proportion of anomalies\n",
    "ALGORITHMS = {\n",
    "    'IsolationForest': {\n",
    "        'adapter': SklearnAdapter,\n",
    "        'params': {\n",
    "            'contamination': CONTAMINATION_RATE,\n",
    "            'n_estimators': 100,\n",
    "            'random_state': RANDOM_STATE\n",
    "        }\n",
    "    },\n",
    "    'LocalOutlierFactor': {\n",
    "        'adapter': SklearnAdapter,\n",
    "        'params': {\n",
    "            'contamination': CONTAMINATION_RATE,\n",
    "            'n_neighbors': 20,\n",
    "            'novelty': True\n",
    "        }\n",
    "    },\n",
    "    'OneClassSVM': {\n",
    "        'adapter': SklearnAdapter,\n",
    "        'params': {\n",
    "            'nu': CONTAMINATION_RATE,\n",
    "            'kernel': 'rbf',\n",
    "            'gamma': 'scale'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"ü§ñ Configured {len(ALGORITHMS)} algorithms:\")\n",
    "for name, config in ALGORITHMS.items():\n",
    "    print(f\"  ‚Ä¢ {name}: {config['params']}\")\n",
    "\n",
    "# Create Pynomaly dataset\n",
    "dataset = Dataset(\n",
    "    name=\"experiment_dataset\",\n",
    "    data=X,\n",
    "    feature_names=list(X.columns)\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Dataset created: {dataset.summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî¨ Experiment Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "results = {}\n",
    "execution_times = {}\n",
    "\n",
    "print(\"=== EXPERIMENT EXECUTION ===\")\n",
    "print(f\"üöÄ Starting experiment with {len(ALGORITHMS)} algorithms...\\n\")\n",
    "\n",
    "for alg_name, config in ALGORITHMS.items():\n",
    "    print(f\"üîÑ Running {alg_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Start timing\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Create contamination rate object\n",
    "        contamination_rate = ContaminationRate(CONTAMINATION_RATE)\n",
    "        \n",
    "        # Initialize adapter\n",
    "        if config['adapter'] == SklearnAdapter:\n",
    "            adapter = SklearnAdapter(alg_name, contamination_rate=contamination_rate)\n",
    "        else:\n",
    "            adapter = config['adapter'](alg_name, contamination_rate=contamination_rate)\n",
    "        \n",
    "        # Fit and detect\n",
    "        result = adapter.fit_detect(dataset)\n",
    "        \n",
    "        # Record execution time\n",
    "        end_time = datetime.now()\n",
    "        execution_time = (end_time - start_time).total_seconds()\n",
    "        execution_times[alg_name] = execution_time\n",
    "        \n",
    "        # Extract scores and predictions\n",
    "        scores = np.array([score.value for score in result.scores])\n",
    "        predictions = (scores > np.percentile(scores, (1 - CONTAMINATION_RATE) * 100)).astype(int)\n",
    "        \n",
    "        # Store results\n",
    "        results[alg_name] = {\n",
    "            'scores': scores,\n",
    "            'predictions': predictions,\n",
    "            'result_object': result,\n",
    "            'execution_time': execution_time\n",
    "        }\n",
    "        \n",
    "        print(f\"  ‚úÖ {alg_name} completed in {execution_time:.2f}s\")\n",
    "        print(f\"     Detected {predictions.sum()} anomalies ({predictions.mean():.1%})\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå {alg_name} failed: {str(e)}\")\n",
    "        results[alg_name] = {'error': str(e)}\n",
    "\n",
    "print(f\"\\nüéØ Experiment completed! {len([r for r in results.values() if 'error' not in r])} algorithms successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance evaluation (if ground truth is available)\n",
    "if y is not None:\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "    \n",
    "    print(\"=== PERFORMANCE EVALUATION ===\")\n",
    "    \n",
    "    # Create performance summary\n",
    "    performance_df = pd.DataFrame()\n",
    "    \n",
    "    for alg_name, result in results.items():\n",
    "        if 'error' in result:\n",
    "            continue\n",
    "            \n",
    "        predictions = result['predictions']\n",
    "        scores = result['scores']\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'Algorithm': alg_name,\n",
    "            'Precision': precision_score(y, predictions),\n",
    "            'Recall': recall_score(y, predictions),\n",
    "            'F1-Score': f1_score(y, predictions),\n",
    "            'ROC-AUC': roc_auc_score(y, scores),\n",
    "            'PR-AUC': average_precision_score(y, scores),\n",
    "            'Execution Time (s)': result['execution_time']\n",
    "        }\n",
    "        \n",
    "        performance_df = pd.concat([performance_df, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    \n",
    "    # Display performance table\n",
    "    if not performance_df.empty:\n",
    "        print(\"üìä Performance Summary:\")\n",
    "        display(performance_df.round(4))\n",
    "        \n",
    "        # Identify best algorithms\n",
    "        best_f1 = performance_df.loc[performance_df['F1-Score'].idxmax(), 'Algorithm']\n",
    "        best_auc = performance_df.loc[performance_df['ROC-AUC'].idxmax(), 'Algorithm']\n",
    "        fastest = performance_df.loc[performance_df['Execution Time (s)'].idxmin(), 'Algorithm']\n",
    "        \n",
    "        print(f\"\\nüèÜ Best Performers:\")\n",
    "        print(f\"  ‚Ä¢ Best F1-Score: {best_f1} ({performance_df[performance_df['Algorithm'] == best_f1]['F1-Score'].iloc[0]:.4f})\")\n",
    "        print(f\"  ‚Ä¢ Best ROC-AUC: {best_auc} ({performance_df[performance_df['Algorithm'] == best_auc]['ROC-AUC'].iloc[0]:.4f})\")\n",
    "        print(f\"  ‚Ä¢ Fastest: {fastest} ({performance_df[performance_df['Algorithm'] == fastest]['Execution Time (s)'].iloc[0]:.2f}s)\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No ground truth available - skipping supervised evaluation\")\n",
    "    \n",
    "    # Unsupervised evaluation metrics\n",
    "    print(\"=== UNSUPERVISED EVALUATION ===\")\n",
    "    \n",
    "    unsupervised_df = pd.DataFrame()\n",
    "    \n",
    "    for alg_name, result in results.items():\n",
    "        if 'error' in result:\n",
    "            continue\n",
    "            \n",
    "        scores = result['scores']\n",
    "        predictions = result['predictions']\n",
    "        \n",
    "        metrics = {\n",
    "            'Algorithm': alg_name,\n",
    "            'Anomalies Detected': predictions.sum(),\n",
    "            'Anomaly Rate': predictions.mean(),\n",
    "            'Score Mean': scores.mean(),\n",
    "            'Score Std': scores.std(),\n",
    "            'Execution Time (s)': result['execution_time']\n",
    "        }\n",
    "        \n",
    "        unsupervised_df = pd.concat([unsupervised_df, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    \n",
    "    if not unsupervised_df.empty:\n",
    "        print(\"üìä Unsupervised Metrics Summary:\")\n",
    "        display(unsupervised_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves (if ground truth available)\n",
    "if y is not None and len([r for r in results.values() if 'error' not in r]) > 0:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for i, (alg_name, result) in enumerate(results.items()):\n",
    "        if 'error' in result:\n",
    "            continue\n",
    "            \n",
    "        scores = result['scores']\n",
    "        \n",
    "        # Calculate ROC curve\n",
    "        fpr, tpr, _ = roc_curve(y, scores)\n",
    "        auc_score = roc_auc_score(y, scores)\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        plt.plot(fpr, tpr, color=colors[i % len(colors)], linewidth=2,\n",
    "                label=f'{alg_name} (AUC = {auc_score:.3f})')\n",
    "    \n",
    "    # Plot diagonal line\n",
    "    plt.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "    plt.title('ROC Curves Comparison', fontsize=16, fontweight='bold')\n",
    "    plt.legend(loc='lower right', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score distributions comparison\n",
    "successful_results = {name: result for name, result in results.items() if 'error' not in result}\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Algorithm Comparison Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Score distributions\n",
    "    ax1 = axes[0, 0]\n",
    "    for alg_name, result in successful_results.items():\n",
    "        scores = result['scores']\n",
    "        ax1.hist(scores, bins=30, alpha=0.6, label=alg_name, density=True)\n",
    "    ax1.set_xlabel('Anomaly Score')\n",
    "    ax1.set_ylabel('Density')\n",
    "    ax1.set_title('Score Distributions')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Execution time comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    alg_names = list(successful_results.keys())\n",
    "    exec_times = [successful_results[name]['execution_time'] for name in alg_names]\n",
    "    bars = ax2.bar(alg_names, exec_times, alpha=0.7)\n",
    "    ax2.set_ylabel('Execution Time (seconds)')\n",
    "    ax2.set_title('Execution Time Comparison')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, time_val in zip(bars, exec_times):\n",
    "        height = bar.get_height()\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{time_val:.2f}s', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Anomaly detection rates\n",
    "    ax3 = axes[1, 0]\n",
    "    detection_rates = [successful_results[name]['predictions'].mean() for name in alg_names]\n",
    "    bars = ax3.bar(alg_names, detection_rates, alpha=0.7, color='coral')\n",
    "    ax3.set_ylabel('Anomaly Detection Rate')\n",
    "    ax3.set_title('Detection Rate Comparison')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    ax3.axhline(y=CONTAMINATION_RATE, color='red', linestyle='--', \n",
    "                label=f'Expected Rate ({CONTAMINATION_RATE:.1%})')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, rate in zip(bars, detection_rates):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                f'{rate:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Performance radar chart (if ground truth available)\n",
    "    ax4 = axes[1, 1]\n",
    "    if y is not None and not performance_df.empty:\n",
    "        # Create radar chart\n",
    "        metrics = ['Precision', 'Recall', 'F1-Score', 'ROC-AUC', 'PR-AUC']\n",
    "        \n",
    "        # Select up to 3 algorithms for clarity\n",
    "        top_algorithms = performance_df.nlargest(3, 'F1-Score')['Algorithm'].tolist()\n",
    "        \n",
    "        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "        angles += angles[:1]  # Complete the circle\n",
    "        \n",
    "        for i, alg in enumerate(top_algorithms):\n",
    "            values = performance_df[performance_df['Algorithm'] == alg][metrics].iloc[0].tolist()\n",
    "            values += values[:1]  # Complete the circle\n",
    "            \n",
    "            ax4.plot(angles, values, 'o-', linewidth=2, label=alg)\n",
    "            ax4.fill(angles, values, alpha=0.25)\n",
    "        \n",
    "        ax4.set_xticks(angles[:-1])\n",
    "        ax4.set_xticklabels(metrics)\n",
    "        ax4.set_ylim(0, 1)\n",
    "        ax4.set_title('Performance Comparison (Top 3)')\n",
    "        ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "        ax4.grid(True)\n",
    "    else:\n",
    "        ax4.text(0.5, 0.5, 'Performance Radar\\n(Ground Truth Required)', \n",
    "                ha='center', va='center', transform=ax4.transAxes,\n",
    "                fontsize=12, style='italic')\n",
    "        ax4.set_title('Performance Radar Chart')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Statistical Analysis and Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical significance testing (if ground truth available)\n",
    "if y is not None and len(successful_results) > 1:\n",
    "    print(\"=== STATISTICAL SIGNIFICANCE TESTING ===\")\n",
    "    \n",
    "    # Cross-validation for statistical testing\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from scipy.stats import wilcoxon, friedmanchisquare\n",
    "    \n",
    "    # Collect cross-validation scores\n",
    "    cv_scores = {}\n",
    "    \n",
    "    print(\"üîÑ Performing 5-fold cross-validation...\")\n",
    "    \n",
    "    for alg_name in successful_results.keys():\n",
    "        try:\n",
    "            # Create sklearn-compatible estimator for CV\n",
    "            if alg_name == 'IsolationForest':\n",
    "                from sklearn.ensemble import IsolationForest\n",
    "                estimator = IsolationForest(contamination=CONTAMINATION_RATE, random_state=RANDOM_STATE)\n",
    "            elif alg_name == 'LocalOutlierFactor':\n",
    "                from sklearn.neighbors import LocalOutlierFactor\n",
    "                estimator = LocalOutlierFactor(contamination=CONTAMINATION_RATE, novelty=True)\n",
    "            elif alg_name == 'OneClassSVM':\n",
    "                from sklearn.svm import OneClassSVM\n",
    "                estimator = OneClassSVM(nu=CONTAMINATION_RATE)\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            # Perform cross-validation\n",
    "            scores = cross_val_score(estimator, X, y, cv=5, scoring='f1')\n",
    "            cv_scores[alg_name] = scores\n",
    "            \n",
    "            print(f\"  ‚Ä¢ {alg_name}: {scores.mean():.4f} ¬± {scores.std():.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå {alg_name}: CV failed ({str(e)})\")\n",
    "    \n",
    "    # Pairwise statistical tests\n",
    "    if len(cv_scores) > 1:\n",
    "        print(f\"\\nüìä Pairwise Wilcoxon Signed-Rank Tests:\")\n",
    "        \n",
    "        algorithm_names = list(cv_scores.keys())\n",
    "        significance_results = []\n",
    "        \n",
    "        for i, alg1 in enumerate(algorithm_names):\n",
    "            for alg2 in algorithm_names[i+1:]:\n",
    "                try:\n",
    "                    statistic, p_value = wilcoxon(cv_scores[alg1], cv_scores[alg2])\n",
    "                    is_significant = p_value < 0.05\n",
    "                    \n",
    "                    better_alg = alg1 if np.mean(cv_scores[alg1]) > np.mean(cv_scores[alg2]) else alg2\n",
    "                    \n",
    "                    result = {\n",
    "                        'Comparison': f'{alg1} vs {alg2}',\n",
    "                        'P-value': p_value,\n",
    "                        'Significant': '‚úÖ' if is_significant else '‚ùå',\n",
    "                        'Better Algorithm': better_alg\n",
    "                    }\n",
    "                    \n",
    "                    significance_results.append(result)\n",
    "                    \n",
    "                    significance_symbol = '***' if p_value < 0.001 else '**' if p_value < 0.01 else '*' if p_value < 0.05 else 'ns'\n",
    "                    print(f\"  ‚Ä¢ {alg1} vs {alg2}: p = {p_value:.4f} ({significance_symbol}) - {better_alg} performs better\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ùå {alg1} vs {alg2}: Test failed ({str(e)})\")\n",
    "        \n",
    "        # Overall significance test\n",
    "        if len(cv_scores) > 2:\n",
    "            try:\n",
    "                scores_matrix = [cv_scores[alg] for alg in algorithm_names]\n",
    "                statistic, p_value = friedmanchisquare(*scores_matrix)\n",
    "                \n",
    "                print(f\"\\nüî¨ Friedman Test (Overall):\")\n",
    "                print(f\"  ‚Ä¢ Statistic: {statistic:.4f}\")\n",
    "                print(f\"  ‚Ä¢ P-value: {p_value:.4f}\")\n",
    "                \n",
    "                if p_value < 0.05:\n",
    "                    print(f\"  ‚Ä¢ ‚úÖ Significant differences detected between algorithms\")\n",
    "                else:\n",
    "                    print(f\"  ‚Ä¢ ‚ùå No significant differences between algorithms\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"  ‚ùå Friedman test failed: {str(e)}\")\n",
    "        \n",
    "        # Create significance summary table\n",
    "        if significance_results:\n",
    "            significance_df = pd.DataFrame(significance_results)\n",
    "            print(f\"\\nüìã Statistical Significance Summary:\")\n",
    "            display(significance_df)\n",
    "\nelse:\n",
    "    print(\"‚ö†Ô∏è Statistical testing requires ground truth and multiple algorithms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Final Results Summary and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== EXPERIMENT SUMMARY AND RECOMMENDATIONS ===\")\n",
    "print(f\"üìÖ Experiment completed: {datetime.now()}\")\n",
    "print(f\"üìä Dataset: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
    "print(f\"ü§ñ Algorithms tested: {len(ALGORITHMS)}\")\n",
    "print(f\"‚úÖ Successful runs: {len(successful_results)}\")\n",
    "\n",
    "if y is not None and not performance_df.empty:\n",
    "    print(f\"\\nüèÜ BEST PERFORMING ALGORITHMS:\")\n",
    "    \n",
    "    # Sort by F1-score\n",
    "    top_performers = performance_df.nlargest(3, 'F1-Score')\n",
    "    \n",
    "    for i, (_, row) in enumerate(top_performers.iterrows(), 1):\n",
    "        print(f\"  {i}. {row['Algorithm']}:\")\n",
    "        print(f\"     ‚Ä¢ F1-Score: {row['F1-Score']:.4f}\")\n",
    "        print(f\"     ‚Ä¢ Precision: {row['Precision']:.4f}\")\n",
    "        print(f\"     ‚Ä¢ Recall: {row['Recall']:.4f}\")\n",
    "        print(f\"     ‚Ä¢ ROC-AUC: {row['ROC-AUC']:.4f}\")\n",
    "        print(f\"     ‚Ä¢ Execution Time: {row['Execution Time (s)']:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "    \n",
    "    best_algorithm = top_performers.iloc[0]['Algorithm']\n",
    "    best_f1 = top_performers.iloc[0]['F1-Score']\n",
    "    \n",
    "    print(f\"  ‚Ä¢ ü•á Primary recommendation: {best_algorithm}\")\n",
    "    print(f\"    - Achieved best F1-score of {best_f1:.4f}\")\n",
    "    \n",
    "    # Performance-based recommendations\n",
    "    if best_f1 > 0.8:\n",
    "        print(f\"    - ‚úÖ Excellent performance (F1 > 0.8) - ready for production\")\n",
    "    elif best_f1 > 0.6:\n",
    "        print(f\"    - ‚ö†Ô∏è Good performance (F1 > 0.6) - consider hyperparameter tuning\")\n",
    "    else:\n",
    "        print(f\"    - üîß Moderate performance (F1 < 0.6) - requires further optimization\")\n",
    "    \n",
    "    # Speed recommendations\n",
    "    fastest_alg = performance_df.loc[performance_df['Execution Time (s)'].idxmin()]\n",
    "    if fastest_alg['Algorithm'] != best_algorithm:\n",
    "        print(f\"  ‚Ä¢ ‚ö° For speed-critical applications: {fastest_alg['Algorithm']}\")\n",
    "        print(f\"    - Fastest execution time: {fastest_alg['Execution Time (s)']:.2f}s\")\n",
    "        print(f\"    - F1-Score: {fastest_alg['F1-Score']:.4f}\")\n",
    "    \n",
    "    # Balance recommendation\n",
    "    performance_df['efficiency_score'] = performance_df['F1-Score'] / performance_df['Execution Time (s)']\n",
    "    most_efficient = performance_df.loc[performance_df['efficiency_score'].idxmax()]\n",
    "    \n",
    "    print(f\"  ‚Ä¢ ‚öñÔ∏è Best performance/speed balance: {most_efficient['Algorithm']}\")\n",
    "    print(f\"    - Efficiency score: {most_efficient['efficiency_score']:.4f}\")\n",
    "\nelse:\n",
    "    print(f\"\\nüìä UNSUPERVISED ANALYSIS SUMMARY:\")\n",
    "    if not unsupervised_df.empty:\n",
    "        for _, row in unsupervised_df.iterrows():\n",
    "            print(f\"  ‚Ä¢ {row['Algorithm']}: {row['Anomalies Detected']} anomalies ({row['Anomaly Rate']:.1%})\")\n",
    "    \n",
    "    print(f\"\\nüí° GENERAL RECOMMENDATIONS:\")\n",
    "    print(f\"  ‚Ä¢ Collect ground truth labels for supervised evaluation\")\n",
    "    print(f\"  ‚Ä¢ Consider domain expert validation of detected anomalies\")\n",
    "    print(f\"  ‚Ä¢ Implement ensemble methods for improved robustness\")\n",
    "\n",
    "print(f\"\\nüîÑ NEXT STEPS:\")\n",
    "print(f\"  1. üéØ Hyperparameter tuning on best-performing algorithm\")\n",
    "print(f\"  2. üß™ Feature engineering and selection\")\n",
    "print(f\"  3. üîó Ensemble method development\")\n",
    "print(f\"  4. üìà Cross-validation with larger datasets\")\n",
    "print(f\"  5. üöÄ Production deployment planning\")\n",
    "\n",
    "print(f\"\\nüìã EXPERIMENT METADATA:\")\n",
    "print(f\"  ‚Ä¢ Random seed: {RANDOM_STATE}\")\n",
    "print(f\"  ‚Ä¢ Contamination rate: {CONTAMINATION_RATE}\")\n",
    "print(f\"  ‚Ä¢ Total execution time: {sum(execution_times.values()):.2f}s\")\n",
    "print(f\"  ‚Ä¢ Reproducible: ‚úÖ (random seed set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Save Results and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment results\n",
    "experiment_results = {\n",
    "    'metadata': {\n",
    "        'experiment_date': datetime.now().isoformat(),\n",
    "        'dataset_shape': X.shape,\n",
    "        'algorithms_tested': list(ALGORITHMS.keys()),\n",
    "        'contamination_rate': CONTAMINATION_RATE,\n",
    "        'random_state': RANDOM_STATE\n",
    "    },\n",
    "    'performance_metrics': performance_df.to_dict('records') if y is not None and not performance_df.empty else None,\n",
    "    'execution_times': execution_times,\n",
    "    'successful_algorithms': list(successful_results.keys()),\n",
    "    'failed_algorithms': [name for name, result in results.items() if 'error' in result]\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "output_file = f\"experiment_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "try:\n",
    "    import json\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(experiment_results, f, indent=2, default=str)\n",
    "    print(f\"üíæ Results saved to: {output_file}\")\nexcept Exception as e:\n",
    "    print(f\"‚ùå Failed to save results: {str(e)}\")\n",
    "\n",
    "# Export performance summary to CSV\n",
    "if y is not None and not performance_df.empty:\n",
    "    csv_file = f\"performance_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    try:\n",
    "        performance_df.to_csv(csv_file, index=False)\n",
    "        print(f\"üìä Performance summary saved to: {csv_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to save CSV: {str(e)}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Experiment completed successfully!\")\n",
    "print(f\"üìã Check the output files for detailed results and further analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìù Experiment Notes\n",
    "\n",
    "**Add your observations, insights, and future work ideas here:**\n",
    "\n",
    "### Key Findings\n",
    "- [Add your key findings here]\n",
    "- [Any unexpected results or patterns]\n",
    "- [Algorithm-specific observations]\n",
    "\n",
    "### Lessons Learned\n",
    "- [What worked well]\n",
    "- [What could be improved]\n",
    "- [Data quality insights]\n",
    "\n",
    "### Future Work\n",
    "- [Planned improvements]\n",
    "- [Additional algorithms to test]\n",
    "- [Feature engineering ideas]\n",
    "- [Production deployment considerations]\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Template Usage**: This template provides a comprehensive framework for anomaly detection experiments. Customize the sections based on your specific needs and domain requirements.\n",
    "\n",
    "**üìö Documentation**: For more information about Pynomaly, visit the [documentation](https://github.com/your-org/pynomaly) or check the examples directory.\n",
    "\n",
    "**ü§ù Support**: If you encounter issues or have questions, please create an issue in the GitHub repository or contact the development team."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}