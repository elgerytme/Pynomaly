#!/usr/bin/env python3
"""Validate performance regression testing infrastructure."""

import subprocess
import sys
from pathlib import Path


def validate_file_structure():
    """Validate that all required performance testing files exist."""
    print("=== Performance Testing File Structure ===")
    
    required_files = [
        # GitHub Actions workflow
        ".github/workflows/performance-testing.yml",
        
        # Performance test scripts
        "tests/performance/memory_analysis.py",
        "tests/performance/performance_gate.py",
        "tests/performance/generate_summary.py",
        "tests/performance/regression/comprehensive_analysis.py",
        "tests/performance/regression/update_baselines.py",
        
        # Existing performance infrastructure
        "tests/performance/test_performance_benchmarks.py",
        "tests/performance/test_algorithm_performance.py",
        "tests/performance/load_test.js",
        "tests/load/locustfile.py",
        "tests/performance/regression/performance_regression_detector.py",
        
        # Configuration
        "pyproject.toml"
    ]
    
    missing_files = []
    for file_path in required_files:
        if Path(file_path).exists():
            print(f"‚úì {file_path}")
        else:
            print(f"‚úó {file_path}")
            missing_files.append(file_path)
    
    return len(missing_files) == 0, missing_files


def validate_dependencies():
    """Validate performance testing dependencies in pyproject.toml."""
    print("\n=== Performance Testing Dependencies ===")
    
    pyproject_path = Path("pyproject.toml")
    if not pyproject_path.exists():
        print("‚úó pyproject.toml not found")
        return False
    
    content = pyproject_path.read_text()
    
    # Check for performance-test section
    if "performance-test" in content:
        print("‚úì performance-test section exists")
        
        # Check for specific dependencies
        perf_deps = [
            "pytest-benchmark>=4.0.0",
            "memray>=1.8.0", 
            "memory-profiler>=0.61.0",
            "matplotlib>=3.7.0",
            "seaborn>=0.12.0",
            "pandas>=2.2.3",
            "locust>=2.14.0",
            "psutil>=6.1.1"
        ]
        
        for dep in perf_deps:
            if dep in content:
                print(f"‚úì {dep}")
            else:
                print(f"‚ö† {dep} not found")
    else:
        print("‚úó performance-test section missing")
        return False
    
    return True


def validate_github_workflow():
    """Validate GitHub Actions workflow for performance testing."""
    print("\n=== GitHub Actions Workflow ===")
    
    workflow_path = Path(".github/workflows/performance-testing.yml")
    if not workflow_path.exists():
        print("‚úó Performance testing workflow not found")
        return False
    
    content = workflow_path.read_text()
    
    # Check for required jobs
    required_jobs = [
        "performance-benchmarks",
        "load-testing", 
        "memory-profiling",
        "performance-regression",
        "performance-summary"
    ]
    
    for job in required_jobs:
        if job in content:
            print(f"‚úì {job} job defined")
        else:
            print(f"‚úó {job} job missing")
    
    # Check for key features
    features = [
        "pytest -v -m \"benchmark",
        "performance_regression_detector.py",
        "k6 run",
        "locust -f",
        "memray",
        "comprehensive_analysis.py"
    ]
    
    print("\n--- Workflow Features ---")
    for feature in features:
        if feature in content:
            print(f"‚úì {feature}")
        else:
            print(f"‚ö† {feature} not found")
    
    return True


def validate_performance_scripts():
    """Validate performance testing scripts are executable."""
    print("\n=== Performance Scripts Validation ===")
    
    scripts = [
        "tests/performance/memory_analysis.py",
        "tests/performance/performance_gate.py", 
        "tests/performance/generate_summary.py",
        "tests/performance/regression/comprehensive_analysis.py",
        "tests/performance/regression/update_baselines.py"
    ]
    
    all_valid = True
    
    for script_path in scripts:
        script = Path(script_path)
        if script.exists():
            # Check if script has main function or is executable
            content = script.read_text()
            if "def main()" in content and "__name__ == \"__main__\"" in content:
                print(f"‚úì {script_path} - properly structured")
            else:
                print(f"‚ö† {script_path} - may lack proper main structure")
        else:
            print(f"‚úó {script_path} - not found")
            all_valid = False
    
    return all_valid


def validate_existing_infrastructure():
    """Validate existing performance testing infrastructure."""
    print("\n=== Existing Infrastructure ===")
    
    # Check for existing test files
    test_files = [
        "tests/performance/test_performance_benchmarks.py",
        "tests/performance/test_algorithm_performance.py",
        "tests/performance/test_comprehensive_performance.py"
    ]
    
    for test_file in test_files:
        if Path(test_file).exists():
            print(f"‚úì {test_file}")
        else:
            print(f"‚ö† {test_file} not found")
    
    # Check for load testing files
    load_files = [
        "tests/load/locustfile.py",
        "tests/performance/load_test.js"
    ]
    
    for load_file in load_files:
        if Path(load_file).exists():
            print(f"‚úì {load_file}")
        else:
            print(f"‚ö† {load_file} not found")
    
    # Check for regression detection
    regression_files = [
        "tests/performance/regression/performance_regression_detector.py"
    ]
    
    for reg_file in regression_files:
        if Path(reg_file).exists():
            print(f"‚úì {reg_file}")
        else:
            print(f"‚ö† {reg_file} not found")
    
    return True


def test_script_imports():
    """Test that new scripts can be imported without errors."""
    print("\n=== Script Import Tests ===")
    
    scripts_to_test = [
        ("tests.performance.memory_analysis", "MemoryAnalyzer"),
        ("tests.performance.performance_gate", "PerformanceGate"),
        ("tests.performance.generate_summary", "PerformanceSummaryGenerator")
    ]
    
    sys.path.insert(0, str(Path.cwd()))
    
    for module_path, class_name in scripts_to_test:
        try:
            # Convert path to module import
            module_name = module_path.replace(".", "/") + ".py"
            if Path(module_name).exists():
                print(f"‚úì {module_path} - file exists and should be importable")
            else:
                print(f"‚úó {module_path} - file not found")
        except Exception as e:
            print(f"‚ö† {module_path} - import issue: {e}")
    
    return True


def run_basic_functionality_test():
    """Run basic functionality tests on the scripts."""
    print("\n=== Basic Functionality Tests ===")
    
    # Test memory analysis
    print("Testing memory analysis script...")
    try:
        result = subprocess.run([
            sys.executable, "tests/performance/memory_analysis.py"
        ], capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0:
            print("‚úì Memory analysis script runs successfully")
        else:
            print(f"‚ö† Memory analysis script had issues: {result.stderr[:100]}")
    except subprocess.TimeoutExpired:
        print("‚ö† Memory analysis script timed out")
    except FileNotFoundError:
        print("‚úó Memory analysis script not found")
    
    # Test performance gate with help
    print("Testing performance gate script...")
    try:
        result = subprocess.run([
            sys.executable, "tests/performance/performance_gate.py", "--help"
        ], capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0 and "Performance gate checker" in result.stdout:
            print("‚úì Performance gate script help works")
        else:
            print("‚ö† Performance gate script help had issues")
    except subprocess.TimeoutExpired:
        print("‚ö† Performance gate script timed out")
    except FileNotFoundError:
        print("‚úó Performance gate script not found")
    
    # Test summary generator
    print("Testing summary generator script...")
    try:
        result = subprocess.run([
            sys.executable, "tests/performance/generate_summary.py", "--help"
        ], capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0 and "Generate performance test summary" in result.stdout:
            print("‚úì Summary generator script help works")
        else:
            print("‚ö† Summary generator script help had issues")
    except subprocess.TimeoutExpired:
        print("‚ö† Summary generator script timed out")
    except FileNotFoundError:
        print("‚úó Summary generator script not found")
    
    return True


def main():
    """Run comprehensive performance testing validation."""
    print("üöÄ Pynomaly Performance Regression Testing Validation\n")
    
    validation_results = []
    
    # Run all validations
    validations = [
        ("File Structure", validate_file_structure),
        ("Dependencies", validate_dependencies),
        ("GitHub Workflow", validate_github_workflow),
        ("Performance Scripts", validate_performance_scripts),
        ("Existing Infrastructure", validate_existing_infrastructure),
        ("Script Imports", test_script_imports),
        ("Basic Functionality", run_basic_functionality_test)
    ]
    
    for name, validation_func in validations:
        try:
            result = validation_func()
            if isinstance(result, tuple):
                success, details = result
                validation_results.append((name, success))
                if not success:
                    print(f"   Missing: {details}")
            else:
                validation_results.append((name, result))
        except Exception as e:
            print(f"   Error in {name}: {e}")
            validation_results.append((name, False))
    
    # Summary
    print("\n" + "="*60)
    print("üìä Validation Summary:")
    print("="*60)
    
    passed = sum(1 for name, result in validation_results if result)
    total = len(validation_results)
    
    for name, result in validation_results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status:8} {name}")
    
    print(f"\nOverall: {passed}/{total} validations passed")
    
    if passed == total:
        print("\nüéâ Performance Regression Testing Infrastructure COMPLETE!")
        print("\nüìã Implementation Summary:")
        print("- ‚úÖ GitHub Actions workflow for automated performance testing")
        print("- ‚úÖ Memory profiling and analysis tools")
        print("- ‚úÖ Performance gate checking for CI/CD")
        print("- ‚úÖ Comprehensive performance reporting")
        print("- ‚úÖ Baseline management and regression detection")
        print("- ‚úÖ Load testing integration (k6 + Locust)")
        print("- ‚úÖ Performance visualization and plotting")
        print("- ‚úÖ Cross-suite performance analysis")
        
        print("\nüöÄ Ready for Production:")
        print("- Run performance tests: pytest -m benchmark")
        print("- Trigger workflow: GitHub Actions on push/PR")
        print("- Generate reports: python tests/performance/generate_summary.py")
        print("- Check gates: python tests/performance/performance_gate.py")
        
        return True
    else:
        print(f"\n‚ö†Ô∏è {total - passed} validations failed - review the output above")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)