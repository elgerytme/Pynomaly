# Logstash pipeline for Pynomaly logs
input {
  # Receive logs from Filebeat or direct JSON input
  beats {
    port => 5044
  }
  
  # HTTP input for direct log ingestion
  http {
    port => 5000
    codec => json
  }
  
  # TCP input for structured logs
  tcp {
    port => 5001
    codec => json_lines
  }
  
  # UDP input for high-volume logs
  udp {
    port => 5002
    codec => json
  }
}

filter {
  # Parse timestamp
  date {
    match => [ "timestamp", "ISO8601" ]
    target => "@timestamp"
  }
  
  # Add index template
  mutate {
    add_field => { "[@metadata][index]" => "pynomaly-logs-%{+YYYY.MM.dd}" }
  }
  
  # Enrich with geolocation if IP is present
  if [ip] {
    geoip {
      source => "ip"
      target => "geoip"
    }
  }
  
  # Parse user agent if present
  if [user_agent] {
    useragent {
      source => "user_agent"
      target => "ua"
    }
  }
  
  # Extract error details for ERROR and CRITICAL levels
  if [level] in ["ERROR", "CRITICAL"] {
    # Extract stack trace
    if [exception] {
      mutate {
        add_field => { "has_exception" => "true" }
      }
    }
    
    # Categorize error types
    if [message] =~ /(?i)(connection|timeout|network)/ {
      mutate { add_field => { "error_category" => "connectivity" } }
    } else if [message] =~ /(?i)(permission|unauthorized|forbidden)/ {
      mutate { add_field => { "error_category" => "authorization" } }
    } else if [message] =~ /(?i)(validation|invalid|format)/ {
      mutate { add_field => { "error_category" => "validation" } }
    } else if [message] =~ /(?i)(database|sql|query)/ {
      mutate { add_field => { "error_category" => "database" } }
    } else if [message] =~ /(?i)(memory|out of memory|oom)/ {
      mutate { add_field => { "error_category" => "memory" } }
    } else {
      mutate { add_field => { "error_category" => "unknown" } }
    }
  }
  
  # Extract performance metrics from messages
  if [message] =~ /duration|response_time|processing_time/ {
    grok {
      match => { "message" => "%{NUMBER:duration_ms:float}\s*(ms|milliseconds?)" }
      tag_on_failure => ["_grokparsefailure_duration_ms"]
    }
    
    grok {
      match => { "message" => "%{NUMBER:duration_seconds:float}\s*(s|seconds?)" }
      tag_on_failure => ["_grokparsefailure_duration_seconds"]
    }
    
    # Convert seconds to milliseconds
    if [duration_seconds] {
      ruby {
        code => "event.set('duration_ms', event.get('duration_seconds') * 1000)"
      }
    }
    
    # Categorize performance
    if [duration_ms] {
      if [duration_ms] < 100 {
        mutate { add_field => { "performance_category" => "fast" } }
      } else if [duration_ms] < 1000 {
        mutate { add_field => { "performance_category" => "normal" } }
      } else if [duration_ms] < 5000 {
        mutate { add_field => { "performance_category" => "slow" } }
      } else {
        mutate { add_field => { "performance_category" => "very_slow" } }
      }
    }
    
    mutate { add_field => { "has_performance_data" => "true" } }
  }
  
  # Extract operation type from function name
  if [function] {
    if [function] =~ /(?i)(detect|predict)/ {
      mutate { add_field => { "operation_type" => "detection" } }
    } else if [function] =~ /(?i)(train|fit)/ {
      mutate { add_field => { "operation_type" => "training" } }
    } else if [function] =~ /(?i)(load|save|store)/ {
      mutate { add_field => { "operation_type" => "data_io" } }
    } else if [function] =~ /(?i)(auth|login|logout)/ {
      mutate { add_field => { "operation_type" => "authentication" } }
    } else {
      mutate { add_field => { "operation_type" => "general" } }
    }
  }
  
  # Add service metadata
  mutate {
    add_field => { "service" => "pynomaly" }
    add_field => { "environment" => "${ENVIRONMENT:production}" }
    add_field => { "version" => "${PYNOMALY_VERSION:latest}" }
  }
  
  # Remove sensitive fields
  mutate {
    remove_field => [ "password", "token", "secret", "key", "authorization" ]
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "%{[@metadata][index]}"
    template_name => "pynomaly-logs"
    template_pattern => "pynomaly-logs-*"
    template_overwrite => true
    template => "/usr/share/logstash/templates/pynomaly-logs.json"
    manage_template => true
  }
  
  # Debug output (comment out in production)
  # stdout {
  #   codec => rubydebug
  # }
  
  # Output errors to separate index
  if [level] in ["ERROR", "CRITICAL"] {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "pynomaly-errors-%{+YYYY.MM.dd}"
    }
  }
  
  # Output performance data to separate index
  if [has_performance_data] {
    elasticsearch {
      hosts => ["http://elasticsearch:9200"]
      index => "pynomaly-performance-%{+YYYY.MM.dd}"
    }
  }
}