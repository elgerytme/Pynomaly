name: Validation Suite

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write
  checks: write
  actions: read

env:
  PYTHON_VERSION: "3.11"
  HATCH_VERBOSE: 1
  COVERAGE_THRESHOLD: 95
  COVERAGE_THRESHOLD_DOMAIN: 98
  COVERAGE_THRESHOLD_APPLICATION: 95
  COVERAGE_THRESHOLD_INFRASTRUCTURE: 90
  COVERAGE_THRESHOLD_PRESENTATION: 85

jobs:
  # Job 1: Fast Quality Checks
  quality-checks:
    name: Quality Checks
    runs-on: ubuntu-latest
    outputs:
      quality-status: ${{ steps.quality-summary.outputs.status }}
      quality-report: ${{ steps.quality-summary.outputs.report }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Hatch
      run: |
        python -m pip install --upgrade pip
        pip install hatch

    - name: Cache Hatch environments
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/hatch
          ~/.local/share/hatch
        key: ${{ runner.os }}-hatch-${{ hashFiles('pyproject.toml') }}

    - name: Run code style checks
      id: style-check
      run: |
        echo "::group::Code Style Check"
        hatch env run lint:style
        echo "::endgroup::"
      continue-on-error: true

    - name: Run type checking
      id: type-check
      run: |
        echo "::group::Type Checking"
        hatch env run lint:typing
        echo "::endgroup::"
      continue-on-error: true

    - name: Run formatting check
      id: format-check
      run: |
        echo "::group::Format Check"
        hatch env run lint:fmt
        echo "::endgroup::"
      continue-on-error: true

    - name: Check duplicate configs
      run: python scripts/validation/assert_no_duplicate_configs.py
      continue-on-error: false

    - name: Quality summary
      id: quality-summary
      run: |
        STYLE_STATUS="${{ steps.style-check.outcome }}"
        TYPE_STATUS="${{ steps.type-check.outcome }}"
        FORMAT_STATUS="${{ steps.format-check.outcome }}"
        
        if [[ "$STYLE_STATUS" == "success" && "$TYPE_STATUS" == "success" && "$FORMAT_STATUS" == "success" ]]; then
          echo "status=success" >> $GITHUB_OUTPUT
          echo "report=✅ All quality checks passed" >> $GITHUB_OUTPUT
        else
          echo "status=failure" >> $GITHUB_OUTPUT
          echo "report=❌ Quality checks failed: Style($STYLE_STATUS), Type($TYPE_STATUS), Format($FORMAT_STATUS)" >> $GITHUB_OUTPUT
        fi

  # Job 2: Build and Package
  build-validation:
    name: Build Validation
    runs-on: ubuntu-latest
    outputs:
      build-status: ${{ steps.build-summary.outputs.status }}
      build-report: ${{ steps.build-summary.outputs.report }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Hatch
      run: |
        python -m pip install --upgrade pip
        pip install hatch

    - name: Build package
      id: build-package
      run: |
        echo "::group::Building Package"
        hatch build --clean
        echo "::endgroup::"
      continue-on-error: true

    - name: Test package installation
      id: test-install
      run: |
        echo "::group::Testing Installation"
        pip install dist/*.whl
        python -c "import pynomaly; print('✅ Package installation successful')"
        echo "::endgroup::"
      continue-on-error: true

    - name: Build summary
      id: build-summary
      run: |
        BUILD_STATUS="${{ steps.build-package.outcome }}"
        INSTALL_STATUS="${{ steps.test-install.outcome }}"
        
        if [[ "$BUILD_STATUS" == "success" && "$INSTALL_STATUS" == "success" ]]; then
          echo "status=success" >> $GITHUB_OUTPUT
          echo "report=✅ Build and installation successful" >> $GITHUB_OUTPUT
        else
          echo "status=failure" >> $GITHUB_OUTPUT
          echo "report=❌ Build failed: Build($BUILD_STATUS), Install($INSTALL_STATUS)" >> $GITHUB_OUTPUT
        fi

    - name: Upload build artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: build-artifacts
        path: dist/
        retention-days: 7

  # Job 3: Test Suite with Coverage
  test-validation:
    name: Test Validation
    runs-on: ubuntu-latest
    strategy:
      matrix:
        test-type: [unit, integration]
      fail-fast: false
    outputs:
      test-status: ${{ steps.test-summary.outputs.status }}
      coverage-status: ${{ steps.coverage-check.outputs.status }}
      test-report: ${{ steps.test-summary.outputs.report }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install Hatch
      run: |
        python -m pip install --upgrade pip
        pip install hatch

    - name: Cache Hatch environments
      uses: actions/cache@v4
      with:
        path: |
          ~/.cache/hatch
          ~/.local/share/hatch
        key: ${{ runner.os }}-hatch-test-${{ hashFiles('pyproject.toml') }}

    - name: Run unit tests
      if: matrix.test-type == 'unit'
      id: unit-tests
      run: |
        echo "::group::Unit Tests"
        hatch run test:run tests/domain/ tests/application/ -v --tb=short \
      --cov=src/pynomaly --cov-report=json:coverage.json --cov-report=term \
          --cov-report=html --cov-fail-under=${{ env.COVERAGE_THRESHOLD }}
        echo "::endgroup::"
      continue-on-error: true

    - name: Run integration tests
      if: matrix.test-type == 'integration'
      id: integration-tests
      run: |
        echo "::group::Integration Tests"
        hatch run test:run tests/infrastructure/ tests/presentation/ -v --tb=short \
          --cov=src/pynomaly --cov-report=json:coverage.json --cov-report=term \
          --cov-append
        echo "::endgroup::"
      continue-on-error: true

    - name: Check coverage thresholds
      id: coverage-check
      run: |
        echo "::group::Coverage Analysis"
        
        # Extract coverage information from JSON
        if [ -f coverage.json ]; then
          # Parse overall coverage
          OVERALL_COVERAGE=$(jq -r '.totals.percent_covered' coverage.json)
          echo "Overall coverage: ${OVERALL_COVERAGE}%"
          
          # Initialize failure flag
          COVERAGE_FAILED=0
          
          # Check main threshold
          if (( $(echo "$OVERALL_COVERAGE >= ${{ env.COVERAGE_THRESHOLD }}" | bc -l) )); then
            echo "✅ Overall coverage meets threshold: ${OVERALL_COVERAGE}% >= ${{ env.COVERAGE_THRESHOLD }}%"
          else
            echo "❌ Overall coverage below threshold: ${OVERALL_COVERAGE}% < ${{ env.COVERAGE_THRESHOLD }}%"
            COVERAGE_FAILED=1
          fi

          # Extract and check per-package coverage
          DOMAIN_COVERAGE=$(jq -r '.files | to_entries | map(select(.key | startswith("src/pynomaly/domain"))) | map(.value.summary.percent_covered) | add / length' coverage.json)
          echo "Domain coverage: ${DOMAIN_COVERAGE}%"
          if (( $(echo "$DOMAIN_COVERAGE >= ${{ env.COVERAGE_THRESHOLD_DOMAIN }}" | bc -l) )); then
            echo "✅ Domain coverage meets threshold: ${DOMAIN_COVERAGE}% >= ${{ env.COVERAGE_THRESHOLD_DOMAIN }}%"
          else
            echo "❌ Domain coverage below threshold: ${DOMAIN_COVERAGE}% < ${{ env.COVERAGE_THRESHOLD_DOMAIN }}%"
            COVERAGE_FAILED=1
          fi

          # Application layer
          APPLICATION_COVERAGE=$(jq -r '.files | to_entries | map(select(.key | startswith("src/pynomaly/application"))) | map(.value.summary.percent_covered) | add / length' coverage.json)
          echo "Application coverage: ${APPLICATION_COVERAGE}%"
          if (( $(echo "$APPLICATION_COVERAGE >= ${{ env.COVERAGE_THRESHOLD_APPLICATION }}" | bc -l) )); then
            echo "✅ Application coverage meets threshold: ${APPLICATION_COVERAGE}% >= ${{ env.COVERAGE_THRESHOLD_APPLICATION }}%"
          else
            echo "❌ Application coverage below threshold: ${APPLICATION_COVERAGE}% < ${{ env.COVERAGE_THRESHOLD_APPLICATION }}%"
            COVERAGE_FAILED=1
          fi

          # Infrastructure layer
          INFRA_COVERAGE=$(jq -r '.files | to_entries | map(select(.key | startswith("src/pynomaly/infrastructure"))) | map(.value.summary.percent_covered) | add / length' coverage.json)
          echo "Infrastructure coverage: ${INFRA_COVERAGE}%"
          if (( $(echo "$INFRA_COVERAGE >= ${{ env.COVERAGE_THRESHOLD_INFRASTRUCTURE }}" | bc -l) )); then
            echo "✅ Infrastructure coverage meets threshold: ${INFRA_COVERAGE}% >= ${{ env.COVERAGE_THRESHOLD_INFRASTRUCTURE }}%"
          else
            echo "❌ Infrastructure coverage below threshold: ${INFRA_COVERAGE}% < ${{ env.COVERAGE_THRESHOLD_INFRASTRUCTURE }}%"
            COVERAGE_FAILED=1
          fi

          # Presentation layer
          PRESENTATION_COVERAGE=$(jq -r '.files | to_entries | map(select(.key | startswith("src/pynomaly/presentation"))) | map(.value.summary.percent_covered) | add / length' coverage.json)
          echo "Presentation coverage: ${PRESENTATION_COVERAGE}%"
          if (( $(echo "$PRESENTATION_COVERAGE >= ${{ env.COVERAGE_THRESHOLD_PRESENTATION }}" | bc -l) )); then
            echo "✅ Presentation coverage meets threshold: ${PRESENTATION_COVERAGE}% >= ${{ env.COVERAGE_THRESHOLD_PRESENTATION }}%"
          else
            echo "❌ Presentation coverage below threshold: ${PRESENTATION_COVERAGE}% < ${{ env.COVERAGE_THRESHOLD_PRESENTATION }}%"
            COVERAGE_FAILED=1
          fi

          # Check final status
          if [ $COVERAGE_FAILED -eq 1 ]; then
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "❌ Some coverage thresholds not met"
            exit 1
          else
            echo "status=success" >> $GITHUB_OUTPUT
            echo "✅ All coverage thresholds met"
          fi

        else
          echo "status=failure" >> $GITHUB_OUTPUT
          echo "❌ No coverage report found"
          exit 1
        fi
        echo "::endgroup::"
      continue-on-error: false

    - name: Test summary
      id: test-summary
      run: |
        TEST_STATUS="${{ matrix.test-type == 'unit' && steps.unit-tests.outcome || steps.integration-tests.outcome }}"
        COVERAGE_STATUS="${{ steps.coverage-check.outcome }}"
        
        if [[ "$TEST_STATUS" == "success" && "$COVERAGE_STATUS" == "success" ]]; then
          echo "status=success" >> $GITHUB_OUTPUT
          echo "report=✅ ${{ matrix.test-type }} tests passed with adequate coverage" >> $GITHUB_OUTPUT
        else
          echo "status=failure" >> $GITHUB_OUTPUT
          echo "report=❌ ${{ matrix.test-type }} tests failed: Tests($TEST_STATUS), Coverage($COVERAGE_STATUS)" >> $GITHUB_OUTPUT
        fi

    - name: Upload test artifacts
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: test-results-${{ matrix.test-type }}
        path: |
          htmlcov/
          coverage.json
          coverage.xml
          .coverage
        retention-days: 7

  # Job 4: Security Validation
  security-validation:
    name: Security Validation
    runs-on: ubuntu-latest
    outputs:
      security-status: ${{ steps.security-summary.outputs.status }}
      security-report: ${{ steps.security-summary.outputs.report }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety

    - name: Run Bandit security scan
      id: bandit-scan
      run: |
        echo "::group::Bandit Security Scan"
        bandit -r src/ -f json -o bandit-report.json
        bandit -r src/ -f txt
        echo "::endgroup::"
      continue-on-error: true

    - name: Run Safety dependency check
      id: safety-check
      run: |
        echo "::group::Safety Dependency Check"
        safety check --json --output safety-report.json
        safety check
        echo "::endgroup::"
      continue-on-error: true

    - name: Security summary
      id: security-summary
      run: |
        BANDIT_STATUS="${{ steps.bandit-scan.outcome }}"
        SAFETY_STATUS="${{ steps.safety-check.outcome }}"
        
        if [[ "$BANDIT_STATUS" == "success" && "$SAFETY_STATUS" == "success" ]]; then
          echo "status=success" >> $GITHUB_OUTPUT
          echo "report=✅ Security scans passed" >> $GITHUB_OUTPUT
        else
          echo "status=failure" >> $GITHUB_OUTPUT
          echo "report=❌ Security issues found: Bandit($BANDIT_STATUS), Safety($SAFETY_STATUS)" >> $GITHUB_OUTPUT
        fi

    - name: Upload security reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
        retention-days: 30

  # Job 5: Documentation Validation
  documentation-validation:
    name: Documentation Validation
    runs-on: ubuntu-latest
    outputs:
      docs-status: ${{ steps.docs-summary.outputs.status }}
      docs-report: ${{ steps.docs-summary.outputs.report }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install documentation dependencies
      run: |
        python -m pip install --upgrade pip
        pip install mkdocs mkdocs-material mkdocstrings[python] pymdown-extensions

    - name: Build documentation
      id: build-docs
      run: |
        echo "::group::Building Documentation"
        mkdocs build --config-file=config/docs/mkdocs.yml --strict
        echo "::endgroup::"
      continue-on-error: true

    - name: Check documentation links
      id: link-check
      run: |
        echo "::group::Checking Documentation Links"
        if [ -f scripts/analysis/check_documentation_links.py ]; then
          python scripts/analysis/check_documentation_links.py
        else
          echo "Link checker not found, skipping"
        fi
        echo "::endgroup::"
      continue-on-error: true

    - name: Documentation summary
      id: docs-summary
      run: |
        BUILD_STATUS="${{ steps.build-docs.outcome }}"
        LINK_STATUS="${{ steps.link-check.outcome }}"
        
        if [[ "$BUILD_STATUS" == "success" ]]; then
          echo "status=success" >> $GITHUB_OUTPUT
          echo "report=✅ Documentation builds successfully" >> $GITHUB_OUTPUT
        else
          echo "status=failure" >> $GITHUB_OUTPUT
          echo "report=❌ Documentation build failed" >> $GITHUB_OUTPUT
        fi

    - name: Upload documentation
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: documentation-build
        path: site/
        retention-days: 7

  # Job 6: Enhanced Validation with Rich Output
  enhanced-validation:
    name: Enhanced Validation
    runs-on: ubuntu-latest
    outputs:
      enhanced-status: ${{ steps.enhanced-summary.outputs.status }}
      enhanced-report: ${{ steps.enhanced-summary.outputs.report }}
      violations-count: ${{ steps.enhanced-summary.outputs.violations-count }}

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install hatch
        pip install typer[all] rich requests

    - name: Install project in development mode
      run: |
        hatch env create dev
        hatch env run dev:pip install -e ".[cli]"

    - name: Run enhanced validation
      id: enhanced-validation
      run: |
        # Set environment variables for GitHub integration
        export GITHUB_TOKEN="${{ secrets.GITHUB_TOKEN }}"
        export GITHUB_REPOSITORY="${{ github.repository }}"
        export GITHUB_PR_NUMBER="${{ github.event.number }}"
        export CI=true
        
        # Run validation with rich output and GitHub comment
        hatch env run dev:python -m pynomaly.presentation.cli.app validate run \
          --github-comment \
          --format rich \
          --save enhanced-validation-report.json || echo "validation_failed=true" >> $GITHUB_OUTPUT
      continue-on-error: true

    - name: Enhanced validation summary
      id: enhanced-summary
      run: |
        if [ -f enhanced-validation-report.json ]; then
          PASSED=$(jq -r '.passed' enhanced-validation-report.json)
          VIOLATION_COUNT=$(jq -r '.violations | length' enhanced-validation-report.json)
          
          echo "violations-count=$VIOLATION_COUNT" >> $GITHUB_OUTPUT
          
          if [ "$PASSED" = "true" ]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "report=✅ Enhanced validation passed" >> $GITHUB_OUTPUT
          else
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "report=❌ Enhanced validation failed with $VIOLATION_COUNT violations" >> $GITHUB_OUTPUT
          fi
        else
          echo "status=failure" >> $GITHUB_OUTPUT
          echo "report=❌ Enhanced validation report not generated" >> $GITHUB_OUTPUT
          echo "violations-count=0" >> $GITHUB_OUTPUT
        fi

    - name: Upload enhanced validation report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: enhanced-validation-report
        path: enhanced-validation-report.json
        retention-days: 30

  # Job 7: Validation Summary and PR Comment
  validation-summary:
    name: Validation Summary
    runs-on: ubuntu-latest
    needs: [quality-checks, build-validation, test-validation, security-validation, documentation-validation, enhanced-validation]
    if: always()

    steps:
    - name: Generate validation summary
      id: generate-summary
      run: |
        echo "::group::Generating Validation Summary"
        
        # Collect all job statuses
        QUALITY_STATUS="${{ needs.quality-checks.outputs.quality-status }}"
        BUILD_STATUS="${{ needs.build-validation.outputs.build-status }}"
        SECURITY_STATUS="${{ needs.security-validation.outputs.security-status }}"
        DOCS_STATUS="${{ needs.documentation-validation.outputs.docs-status }}"
        ENHANCED_STATUS="${{ needs.enhanced-validation.outputs.enhanced-status }}"
        
        # Check test results (matrix job)
        TEST_RESULTS="${{ needs.test-validation.result }}"
        
        # Determine overall status
        if [[ "$QUALITY_STATUS" == "success" && "$BUILD_STATUS" == "success" && "$TEST_RESULTS" == "success" && "$SECURITY_STATUS" == "success" && "$DOCS_STATUS" == "success" && "$ENHANCED_STATUS" == "success" ]]; then
          OVERALL_STATUS="✅ PASSED"
          MERGE_READY="true"
        else
          OVERALL_STATUS="❌ FAILED"
          MERGE_READY="false"
        fi
        
        # Create summary report
        cat > validation-summary.md << 'EOF'
        # 🔍 Validation Suite Results
        
        **Overall Status: $OVERALL_STATUS**
        
        **Commit:** ${{ github.sha }}
        **Branch:** ${{ github.ref_name }}
        **Run Date:** $(date)
        
        ## 📊 Detailed Results
        
        | Component | Status | Details |
        |-----------|--------|---------|
        | Quality Checks | ${{ needs.quality-checks.outputs.quality-status == 'success' && '✅ Passed' || '❌ Failed' }} | ${{ needs.quality-checks.outputs.quality-report }} |
        | Build & Package | ${{ needs.build-validation.outputs.build-status == 'success' && '✅ Passed' || '❌ Failed' }} | ${{ needs.build-validation.outputs.build-report }} |
        | Test Suite | ${{ needs.test-validation.result == 'success' && '✅ Passed' || '❌ Failed' }} | Unit & Integration tests with coverage |
        | Security Scan | ${{ needs.security-validation.outputs.security-status == 'success' && '✅ Passed' || '❌ Failed' }} | ${{ needs.security-validation.outputs.security-report }} |
        | Documentation | ${{ needs.documentation-validation.outputs.docs-status == 'success' && '✅ Passed' || '❌ Failed' }} | ${{ needs.documentation-validation.outputs.docs-report }} |
        | Enhanced Validation | ${{ needs.enhanced-validation.outputs.enhanced-status == 'success' && '✅ Passed' || '❌ Failed' }} | ${{ needs.enhanced-validation.outputs.enhanced-report }} |
        
        ## 🎯 Coverage Requirements
        
        - **Overall Project**: ≥95% (Required for merge)
        - **Domain Layer**: ≥98% (Business logic - highest standard)
        - **Application Layer**: ≥95% (Use cases - high standard)
        - **Infrastructure Layer**: ≥90% (Adapters - moderate standard)
        - **Presentation Layer**: ≥85% (UI/API - flexible standard)
        
        **Note**: All thresholds are strictly enforced by CI/CD pipeline
        
        ## 🚀 Merge Status
        
        **Ready for merge:** $MERGE_READY
        
        EOF
        
        if [[ "$MERGE_READY" == "true" ]]; then
          cat >> validation-summary.md << 'EOF'
        ✅ **All validation checks passed!** This PR meets all quality gates:
        - 100% test pass rate
        - ≥95% code coverage
        - No security vulnerabilities
        - Clean code quality
        - Documentation builds successfully
        
        The PR is ready to be merged to the main branch.
        EOF
        else
          cat >> validation-summary.md << 'EOF'
        ❌ **Some validation checks failed.** Please address the issues above before merging.
        
        **Required for merge:**
        - All tests must pass (100% pass rate)
        - Code coverage must be ≥95%
        - No high-severity security issues
        - Code quality checks must pass
        - Documentation must build successfully
        EOF
        fi
        
        echo "merge-ready=$MERGE_READY" >> $GITHUB_OUTPUT
        echo "::endgroup::"

    - name: Comment on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          // Read the validation summary
          const summary = fs.readFileSync('validation-summary.md', 'utf8');
          
          // Post the comment
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });

    - name: Set final status
      run: |
        if [[ "${{ steps.generate-summary.outputs.merge-ready }}" == "true" ]]; then
          echo "✅ All validation checks passed - PR is ready for merge"
          exit 0
        else
          echo "❌ Some validation checks failed - PR is not ready for merge"
          exit 1
        fi

    - name: Upload validation summary
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: validation-summary
        path: validation-summary.md
        retention-days: 30
