# Advanced Web UI Testing Pipeline
# This workflow runs comprehensive UI testing including performance, accessibility, 
# load testing, memory profiling, and visual regression testing

name: ğŸ§ª Advanced Web UI Testing

on:
  push:
    branches: [main, develop]
    paths:
      - 'src/pynomaly/presentation/web/**'
      - 'tests/ui/**'
      - 'package.json'
      - 'playwright.config.ts'
  pull_request:
    branches: [main, develop]
    paths:
      - 'src/pynomaly/presentation/web/**'
      - 'tests/ui/**'
      - 'package.json'
      - 'playwright.config.ts'
  schedule:
    # Run comprehensive tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_level:
        description: 'Testing level'
        required: true
        default: 'standard'
        type: choice
        options:
          - 'quick'
          - 'standard'
          - 'comprehensive'
          - 'load_test_only'
          - 'performance_only'
      update_baselines:
        description: 'Update performance baselines'
        required: false
        default: false
        type: boolean

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'
  PLAYWRIGHT_BROWSERS_PATH: ${{ github.workspace }}/ms-playwright

jobs:
  # Pre-flight checks and setup
  setup:
    name: ğŸš€ Setup and Pre-flight Checks
    runs-on: ubuntu-latest
    outputs:
      test-level: ${{ steps.determine-level.outputs.level }}
      should-run-load-tests: ${{ steps.determine-level.outputs.load-tests }}
      should-run-visual-tests: ${{ steps.determine-level.outputs.visual-tests }}
      cache-key: ${{ steps.cache-key.outputs.key }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Determine test level
        id: determine-level
        run: |
          if [[ "${{ github.event_name }}" == "workflow_dispatch" ]]; then
            LEVEL="${{ github.event.inputs.test_level }}"
          elif [[ "${{ github.event_name }}" == "schedule" ]]; then
            LEVEL="comprehensive"
          elif [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            LEVEL="standard"
          else
            LEVEL="quick"
          fi
          
          echo "level=$LEVEL" >> $GITHUB_OUTPUT
          echo "load-tests=${{ contains(fromJSON('["standard", "comprehensive", "load_test_only"]'), env.LEVEL) }}" >> $GITHUB_OUTPUT
          echo "visual-tests=${{ contains(fromJSON('["standard", "comprehensive"]'), env.LEVEL) }}" >> $GITHUB_OUTPUT
          echo "ğŸ¯ Test level determined: $LEVEL"

      - name: Generate cache key
        id: cache-key
        run: |
          echo "key=ui-tests-${{ runner.os }}-node${{ env.NODE_VERSION }}-${{ hashFiles('package-lock.json', 'playwright.config.ts') }}" >> $GITHUB_OUTPUT

  # Core UI Testing Suite
  ui-tests:
    name: ğŸ­ Core UI Tests
    runs-on: ubuntu-latest
    needs: setup
    strategy:
      fail-fast: false
      matrix:
        browser: [chromium, firefox, webkit]
        device: [desktop, mobile]
        exclude:
          - browser: webkit
            device: mobile
          - browser: firefox  
            device: mobile
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache Playwright browsers
        uses: actions/cache@v3
        with:
          path: ${{ env.PLAYWRIGHT_BROWSERS_PATH }}
          key: ${{ needs.setup.outputs.cache-key }}-browsers
          restore-keys: |
            ui-tests-${{ runner.os }}-node${{ env.NODE_VERSION }}-browsers

      - name: Install dependencies
        run: |
          npm ci
          npx playwright install ${{ matrix.browser }}
          pip install -e .

      - name: Start application
        run: |
          python -m pynomaly.presentation.web.app &
          sleep 10
          curl -f http://localhost:8000/health || exit 1

      - name: Run UI tests
        run: |
          if [[ "${{ matrix.device }}" == "mobile" ]]; then
            npx playwright test --project="Mobile *" --browser=${{ matrix.browser }}
          else
            npx playwright test --project="Desktop ${{ matrix.browser }}" --browser=${{ matrix.browser }}
          fi
        env:
          CI: true

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: ui-test-results-${{ matrix.browser }}-${{ matrix.device }}
          path: |
            test_reports/
            test-results/
          retention-days: 7

  # Memory Leak Detection
  memory-testing:
    name: ğŸ§  Memory Leak Detection
    runs-on: ubuntu-latest
    needs: setup
    if: contains(fromJSON('["standard", "comprehensive"]'), needs.setup.outputs.test-level)
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          npm ci
          npx playwright install chromium
          pip install -e .

      - name: Start application
        run: |
          python -m pynomaly.presentation.web.app &
          sleep 10

      - name: Run memory leak tests
        run: |
          npx playwright test tests/ui/memory-leak/ --reporter=json
        env:
          CI: true

      - name: Analyze memory results
        run: |
          node -e "
          const fs = require('fs');
          const reports = fs.readdirSync('test_reports').filter(f => f.includes('memory-report'));
          reports.forEach(report => {
            const data = JSON.parse(fs.readFileSync(\`test_reports/\${report}\`));
            if (data.memoryTrend && data.memoryTrend.increasing) {
              console.log(\`âš ï¸ Memory leak detected in \${report}\`);
              process.exitCode = 1;
            }
          });
          "

      - name: Upload memory reports
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: memory-test-results
          path: test_reports/*memory-report*.json

  # Performance and Regression Testing
  performance-testing:
    name: âš¡ Performance & Regression Testing
    runs-on: ubuntu-latest
    needs: setup
    if: contains(fromJSON('["standard", "comprehensive", "performance_only"]'), needs.setup.outputs.test-level)
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          npm ci
          npx playwright install chromium
          pip install -e .

      - name: Start application
        run: |
          python -m pynomaly.presentation.web.app &
          sleep 10

      - name: Run Lighthouse CI
        run: |
          npm run lhci:collect || true
          npm run lhci:assert || true

      - name: Run performance regression tests
        run: |
          if [[ "${{ github.event.inputs.update_baselines }}" == "true" ]]; then
            export UPDATE_BASELINE=true
          fi
          npx playwright test tests/ui/performance-regression/ --reporter=json
        env:
          CI: true
          BUILD_ID: ${{ github.run_id }}
          BRANCH_NAME: ${{ github.ref_name }}
          COMMIT_SHA: ${{ github.sha }}

      - name: Check for performance regressions
        run: |
          node -e "
          const fs = require('fs');
          const path = 'test_reports/playwright-results.json';
          if (fs.existsSync(path)) {
            const results = JSON.parse(fs.readFileSync(path));
            const failed = results.stats?.failed || 0;
            if (failed > 0) {
              console.log('âŒ Performance regressions detected');
              process.exitCode = 1;
            } else {
              console.log('âœ… No performance regressions detected');
            }
          }
          "

      - name: Upload performance reports
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: performance-test-results
          path: |
            test_reports/lighthouse-*.html
            test_reports/performance-baselines/
            .lighthouseci/

  # Load Testing
  load-testing:
    name: ğŸ”¥ Load Testing
    runs-on: ubuntu-latest
    needs: setup
    if: needs.setup.outputs.should-run-load-tests == 'true'
    strategy:
      matrix:
        scenario: [light, medium, heavy]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          npm ci
          npx playwright install chromium
          pip install -e .

      - name: Start application
        run: |
          python -m pynomaly.presentation.web.app &
          sleep 10

      - name: Run load tests
        run: |
          npx playwright test tests/ui/load-testing/ --grep="${{ matrix.scenario }}" --reporter=json
        env:
          CI: true

      - name: Analyze load test results
        run: |
          node -e "
          const fs = require('fs');
          const path = 'test_reports/playwright-results.json';
          if (fs.existsSync(path)) {
            const results = JSON.parse(fs.readFileSync(path));
            console.log('Load test scenario: ${{ matrix.scenario }}');
            console.log('Tests run:', results.stats?.expected || 0);
            console.log('Tests passed:', results.stats?.passed || 0);
            console.log('Tests failed:', results.stats?.failed || 0);
          }
          "

      - name: Upload load test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: load-test-results-${{ matrix.scenario }}
          path: test_reports/

  # User Journey Testing
  user-journey-testing:
    name: ğŸš¶ User Journey Testing
    runs-on: ubuntu-latest
    needs: setup
    if: contains(fromJSON('["standard", "comprehensive"]'), needs.setup.outputs.test-level)
    strategy:
      matrix:
        persona: [data-scientist, analyst, admin]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          npm ci
          npx playwright install chromium
          pip install -e .

      - name: Start application
        run: |
          python -m pynomaly.presentation.web.app &
          sleep 10

      - name: Run user journey tests
        run: |
          npx playwright test tests/ui/user-journey/ --grep="${{ matrix.persona }}" --reporter=json
        env:
          CI: true

      - name: Upload journey results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: user-journey-${{ matrix.persona }}
          path: test_reports/

  # PWA Testing
  pwa-testing:
    name: ğŸ“± PWA Testing
    runs-on: ubuntu-latest
    needs: setup
    if: contains(fromJSON('["comprehensive"]'), needs.setup.outputs.test-level)
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          npm ci
          npx playwright install chromium
          pip install -e .

      - name: Start application
        run: |
          python -m pynomaly.presentation.web.app &
          sleep 10

      - name: Run PWA tests
        run: |
          npx playwright test tests/ui/user-journey/test_pwa_capabilities.spec.ts --reporter=json
        env:
          CI: true

      - name: Upload PWA results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: pwa-test-results
          path: test_reports/

  # Error Boundary Testing
  error-boundary-testing:
    name: ğŸ›¡ï¸ Error Boundary Testing
    runs-on: ubuntu-latest
    needs: setup
    if: contains(fromJSON('["standard", "comprehensive"]'), needs.setup.outputs.test-level)
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          npm ci
          npx playwright install chromium
          pip install -e .

      - name: Start application
        run: |
          python -m pynomaly.presentation.web.app &
          sleep 10

      - name: Run error boundary tests
        run: |
          npx playwright test tests/ui/error-boundary/ --reporter=json
        env:
          CI: true

      - name: Upload error boundary results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: error-boundary-results
          path: test_reports/

  # Generate Test Dashboard
  generate-dashboard:
    name: ğŸ“Š Generate Test Dashboard
    runs-on: ubuntu-latest
    needs: [ui-tests, memory-testing, performance-testing, load-testing, user-journey-testing, error-boundary-testing]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Download all test artifacts
        uses: actions/download-artifact@v3
        with:
          path: test_reports/

      - name: Flatten artifact structure
        run: |
          find test_reports/ -name "*.json" -exec cp {} test_reports/ \;
          find test_reports/ -name "*.html" -exec cp {} test_reports/ \;

      - name: Generate comprehensive test dashboard
        run: |
          node tests/ui/test-dashboard/generate_test_dashboard.js

      - name: Upload dashboard
        uses: actions/upload-artifact@v3
        with:
          name: test-dashboard
          path: test_reports/dashboard/

      - name: Deploy dashboard to GitHub Pages
        if: github.ref == 'refs/heads/main'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: test_reports/dashboard
          destination_dir: ui-testing-dashboard

  # Summary and Notifications
  test-summary:
    name: ğŸ“‹ Test Summary
    runs-on: ubuntu-latest
    needs: [setup, ui-tests, memory-testing, performance-testing, load-testing, user-journey-testing, error-boundary-testing, generate-dashboard]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download dashboard data
        uses: actions/download-artifact@v3
        with:
          name: test-dashboard
          path: dashboard/

      - name: Generate test summary
        run: |
          echo "# ğŸ§ª Advanced UI Testing Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Test Level:** ${{ needs.setup.outputs.test-level }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit:** \`${{ github.sha }}\`" >> $GITHUB_STEP_SUMMARY
          echo "**Branch:** \`${{ github.ref_name }}\`" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Job status summary
          echo "## ğŸ“Š Job Results" >> $GITHUB_STEP_SUMMARY
          echo "| Test Category | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|---------------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Core UI Tests | ${{ needs.ui-tests.result == 'success' && 'âœ… Passed' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Memory Testing | ${{ needs.memory-testing.result == 'success' && 'âœ… Passed' || needs.memory-testing.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Testing | ${{ needs.performance-testing.result == 'success' && 'âœ… Passed' || needs.performance-testing.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Load Testing | ${{ needs.load-testing.result == 'success' && 'âœ… Passed' || needs.load-testing.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| User Journey Testing | ${{ needs.user-journey-testing.result == 'success' && 'âœ… Passed' || needs.user-journey-testing.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Error Boundary Testing | ${{ needs.error-boundary-testing.result == 'success' && 'âœ… Passed' || needs.error-boundary-testing.result == 'skipped' && 'â­ï¸ Skipped' || 'âŒ Failed' }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          # Dashboard link
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "## ğŸ“Š Test Dashboard" >> $GITHUB_STEP_SUMMARY
            echo "[View Comprehensive Test Dashboard](https://YOUR_USERNAME.github.io/pynomaly/ui-testing-dashboard/)" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            let summary = `## ğŸ§ª Advanced UI Testing Results\n\n`;
            summary += `**Test Level:** ${{ needs.setup.outputs.test-level }}\n`;
            summary += `**Total Jobs:** ${{ strategy.job-total }}\n\n`;
            
            summary += `### Job Results\n`;
            summary += `- Core UI Tests: ${{ needs.ui-tests.result == 'success' && 'âœ…' || 'âŒ' }}\n`;
            summary += `- Memory Testing: ${{ needs.memory-testing.result == 'success' && 'âœ…' || needs.memory-testing.result == 'skipped' && 'â­ï¸' || 'âŒ' }}\n`;
            summary += `- Performance Testing: ${{ needs.performance-testing.result == 'success' && 'âœ…' || needs.performance-testing.result == 'skipped' && 'â­ï¸' || 'âŒ' }}\n`;
            summary += `- Load Testing: ${{ needs.load-testing.result == 'success' && 'âœ…' || needs.load-testing.result == 'skipped' && 'â­ï¸' || 'âŒ' }}\n`;
            summary += `- User Journey Testing: ${{ needs.user-journey-testing.result == 'success' && 'âœ…' || needs.user-journey-testing.result == 'skipped' && 'â­ï¸' || 'âŒ' }}\n`;
            summary += `- Error Boundary Testing: ${{ needs.error-boundary-testing.result == 'success' && 'âœ…' || needs.error-boundary-testing.result == 'skipped' && 'â­ï¸' || 'âŒ' }}\n\n`;
            
            summary += `### ğŸ“ Artifacts\n`;
            summary += `Test reports and detailed results are available in the workflow artifacts.\n\n`;
            summary += `---\n`;
            summary += `*Generated by Advanced UI Testing Pipeline*`;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });

      - name: Fail workflow if critical tests failed
        if: always()
        run: |
          if [[ "${{ needs.ui-tests.result }}" == "failure" ]]; then
            echo "âŒ Core UI tests failed - this is critical"
            exit 1
          fi
          
          if [[ "${{ needs.performance-testing.result }}" == "failure" && "${{ contains(fromJSON('[\"main\", \"develop\"]'), github.ref_name) }}" == "true" ]]; then
            echo "âŒ Performance tests failed on protected branch"
            exit 1
          fi
          
          echo "âœ… All critical tests passed or were skipped appropriately"
