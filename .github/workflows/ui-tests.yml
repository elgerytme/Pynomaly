name: UI Tests and Accessibility Validation

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/pynomaly/presentation/web/**'
      - 'tests/ui/**'
      - 'docker-compose.ui-testing.yml'
      - '.github/workflows/ui-tests.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'src/pynomaly/presentation/web/**'
      - 'tests/ui/**'

env:
  DOCKER_BUILDKIT: 1
  COMPOSE_DOCKER_CLI_BUILD: 1

jobs:
  ui-tests:
    name: UI Automation Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    services:
      # PostgreSQL for testing
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_USER: testuser
          POSTGRES_DB: pynomaly_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Create test directories
      run: |
        mkdir -p reports screenshots test-results visual-baselines
        chmod 755 reports screenshots test-results visual-baselines
        
    - name: Cache Docker layers
      uses: actions/cache@v3
      with:
        path: /tmp/.buildx-cache
        key: ${{ runner.os }}-buildx-ui-${{ github.sha }}
        restore-keys: |
          ${{ runner.os }}-buildx-ui-
          
    - name: Build and start application
      run: |
        # Build images with cache
        docker-compose -f docker-compose.ui-testing.yml build \
          --cache-from type=local,src=/tmp/.buildx-cache \
          --cache-to type=local,dest=/tmp/.buildx-cache-new,mode=max
        
        # Start application
        docker-compose -f docker-compose.ui-testing.yml up -d pynomaly-app
        
        # Wait for application to be ready
        timeout 120s bash -c 'until docker-compose -f docker-compose.ui-testing.yml ps pynomaly-app | grep -q healthy; do sleep 5; done'
        
    - name: Run UI tests
      run: |
        # Run comprehensive UI test suite
        docker-compose -f docker-compose.ui-testing.yml run --rm ui-tests
      continue-on-error: true
      id: ui-tests
      
    - name: Run visual regression tests
      run: |
        # Run visual regression tests
        docker-compose -f docker-compose.ui-testing.yml run --rm visual-tests
      continue-on-error: true
      id: visual-tests
      
    - name: Generate comprehensive report
      run: |
        # Generate final report with critiques
        docker-compose -f docker-compose.ui-testing.yml run --rm \
          --entrypoint="" ui-tests python tests/ui/run_ui_tests.py
      continue-on-error: true
      
    - name: Collect test artifacts
      if: always()
      run: |
        # Copy artifacts from containers
        docker cp $(docker-compose -f docker-compose.ui-testing.yml ps -q ui-tests):/app/screenshots/. ./screenshots/ 2>/dev/null || true
        docker cp $(docker-compose -f docker-compose.ui-testing.yml ps -q ui-tests):/app/reports/. ./reports/ 2>/dev/null || true
        docker cp $(docker-compose -f docker-compose.ui-testing.yml ps -q ui-tests):/app/test-results/. ./test-results/ 2>/dev/null || true
        docker cp $(docker-compose -f docker-compose.ui-testing.yml ps -q ui-tests):/app/visual-baselines/. ./visual-baselines/ 2>/dev/null || true
        
        # Set permissions
        sudo chown -R $USER:$USER screenshots reports test-results visual-baselines
        
        # Generate artifact summary
        echo "## UI Test Artifacts" > artifact-summary.md
        echo "- Screenshots: $(find screenshots -name "*.png" | wc -l) files" >> artifact-summary.md
        echo "- Reports: $(find reports -name "*.html" | wc -l) HTML reports" >> artifact-summary.md
        echo "- Test Results: $(find test-results -type f | wc -l) files" >> artifact-summary.md
        echo "- Visual Baselines: $(find visual-baselines -name "*.png" | wc -l) baselines" >> artifact-summary.md
        
    - name: Upload test reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ui-test-reports-${{ github.run_number }}
        path: |
          reports/
          artifact-summary.md
        retention-days: 30
        
    - name: Upload screenshots
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ui-screenshots-${{ github.run_number }}
        path: screenshots/
        retention-days: 14
        
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ui-test-results-${{ github.run_number }}
        path: test-results/
        retention-days: 7
        
    - name: Upload visual baselines
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: visual-baselines-${{ github.run_number }}
        path: visual-baselines/
        retention-days: 30
        
    - name: Parse test results
      if: always()
      run: |
        # Extract test results for status checks
        if [ -f "reports/ui_test_summary_*.json" ]; then
          REPORT_FILE=$(ls reports/ui_test_summary_*.json | head -1)
          
          # Extract key metrics
          OVERALL_SCORE=$(python3 -c "import json; data=json.load(open('$REPORT_FILE')); print(data.get('overall_score', 0))")
          CRITICAL_ISSUES=$(python3 -c "import json; data=json.load(open('$REPORT_FILE')); print(data.get('critical_issues_count', 0))")
          ACCESSIBILITY_SCORE=$(python3 -c "import json; data=json.load(open('$REPORT_FILE')); print(data.get('category_scores', {}).get('accessibility', 0))")
          
          # Set environment variables for next steps
          echo "OVERALL_SCORE=$OVERALL_SCORE" >> $GITHUB_ENV
          echo "CRITICAL_ISSUES=$CRITICAL_ISSUES" >> $GITHUB_ENV
          echo "ACCESSIBILITY_SCORE=$ACCESSIBILITY_SCORE" >> $GITHUB_ENV
          
          # Generate status summary
          echo "### ðŸŽ¯ UI Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Overall Score:** $OVERALL_SCORE/100" >> $GITHUB_STEP_SUMMARY
          echo "- **Critical Issues:** $CRITICAL_ISSUES" >> $GITHUB_STEP_SUMMARY
          echo "- **Accessibility Score:** $ACCESSIBILITY_SCORE/100" >> $GITHUB_STEP_SUMMARY
          
          if [ "$OVERALL_SCORE" -lt "85" ]; then
            echo "- âš ï¸ **Status:** Needs Improvement" >> $GITHUB_STEP_SUMMARY
          else
            echo "- âœ… **Status:** Good Quality" >> $GITHUB_STEP_SUMMARY
          fi
        fi
        
    - name: Comment on PR
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          // Read test summary if available
          let summary = "## ðŸ” UI Test Results\n\n";
          
          try {
            const summaryFile = fs.readdirSync('reports').find(f => f.startsWith('ui_test_summary_'));
            if (summaryFile) {
              const results = JSON.parse(fs.readFileSync(`reports/${summaryFile}`, 'utf8'));
              
              summary += `### Scores\n`;
              summary += `- **Overall:** ${results.overall_score}/100\n`;
              summary += `- **Accessibility:** ${results.category_scores?.accessibility || 'N/A'}/100\n`;
              summary += `- **Layout:** ${results.category_scores?.layout || 'N/A'}/100\n`;
              summary += `- **UX Flows:** ${results.category_scores?.ux || 'N/A'}/100\n`;
              summary += `- **Responsive:** ${results.category_scores?.responsive || 'N/A'}/100\n\n`;
              
              summary += `### Issues\n`;
              summary += `- **Critical:** ${results.critical_issues_count || 0}\n`;
              summary += `- **Warnings:** ${results.warnings_count || 0}\n\n`;
              
              if (results.critical_issues_count > 0) {
                summary += `âš ï¸ **Action Required:** Please address critical issues before merging.\n\n`;
              }
              
              summary += `ðŸ“Š [View detailed report in artifacts](${context.payload.pull_request.html_url}/checks)\n`;
            }
          } catch (error) {
            summary += "âŒ Could not parse test results. Check artifacts for details.\n";
          }
          
          // Find existing comment
          const comments = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });
          
          const existingComment = comments.data.find(comment => 
            comment.user.type === 'Bot' && comment.body.includes('ðŸ” UI Test Results')
          );
          
          if (existingComment) {
            // Update existing comment
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: existingComment.id,
              body: summary
            });
          } else {
            // Create new comment
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: summary
            });
          }
          
    - name: Quality gates
      if: github.event_name == 'pull_request'
      run: |
        # Quality gates for PR merging
        if [ "${CRITICAL_ISSUES:-0}" -gt "0" ]; then
          echo "âŒ QUALITY GATE FAILED: $CRITICAL_ISSUES critical accessibility issues found"
          echo "Please fix critical issues before merging."
          exit 1
        fi
        
        if [ "${ACCESSIBILITY_SCORE:-0}" -lt "80" ]; then
          echo "âŒ QUALITY GATE FAILED: Accessibility score $ACCESSIBILITY_SCORE is below minimum (80)"
          echo "Please improve accessibility before merging."
          exit 1
        fi
        
        if [ "${OVERALL_SCORE:-0}" -lt "75" ]; then
          echo "âŒ QUALITY GATE FAILED: Overall UI score $OVERALL_SCORE is below minimum (75)"
          echo "Please improve UI quality before merging."
          exit 1
        fi
        
        echo "âœ… All quality gates passed!"
        
    - name: Update visual baselines (main branch only)
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        # Auto-update visual baselines on main branch
        if [ -d "visual-baselines" ] && [ "$(ls -A visual-baselines)" ]; then
          echo "ðŸ“¸ Updating visual regression baselines"
          
          # Commit updated baselines
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          git add visual-baselines/
          
          if ! git diff --cached --quiet; then
            git commit -m "chore: update visual regression baselines [skip ci]"
            git push
          fi
        fi
        
    - name: Cleanup
      if: always()
      run: |
        # Cleanup Docker resources
        docker-compose -f docker-compose.ui-testing.yml down -v
        docker system prune -f
        
        # Move cache
        if [ -d "/tmp/.buildx-cache-new" ]; then
          rm -rf /tmp/.buildx-cache
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache
        fi
        
  accessibility-audit:
    name: Accessibility Compliance Check
    runs-on: ubuntu-latest
    needs: ui-tests
    if: always()
    
    steps:
    - name: Download test reports
      uses: actions/download-artifact@v4
      with:
        name: ui-test-reports-${{ github.run_number }}
        path: reports/
        
    - name: Accessibility compliance check
      run: |
        # Check if accessibility meets compliance standards
        if [ -f "reports/ui_test_summary_*.json" ]; then
          REPORT_FILE=$(ls reports/ui_test_summary_*.json | head -1)
          
          ACCESSIBILITY_SCORE=$(python3 -c "import json; data=json.load(open('$REPORT_FILE')); print(data.get('category_scores', {}).get('accessibility', 0))")
          CRITICAL_A11Y=$(python3 -c "import json; data=json.load(open('$REPORT_FILE')); print(sum(1 for issue in data.get('test_results', {}).get('accessibility', {}).get('issues', []) if issue.get('severity') == 'critical'))")
          
          echo "ðŸ” Accessibility Analysis:"
          echo "  Score: $ACCESSIBILITY_SCORE/100"
          echo "  Critical Issues: $CRITICAL_A11Y"
          
          # Generate compliance report
          cat > accessibility-compliance.md << EOF
        # ðŸ” Accessibility Compliance Report
        
        ## Summary
        - **Accessibility Score:** $ACCESSIBILITY_SCORE/100
        - **Critical Issues:** $CRITICAL_A11Y
        - **Compliance Level:** $([ "$ACCESSIBILITY_SCORE" -ge "90" ] && echo "AA" || [ "$ACCESSIBILITY_SCORE" -ge "80" ] && echo "A" || echo "Non-compliant")
        
        ## Recommendations
        $([ "$ACCESSIBILITY_SCORE" -lt "80" ] && echo "âŒ **Immediate action required** - Below minimum accessibility standards" || echo "âœ… Meets basic accessibility standards")
        
        ## Next Steps
        1. Review detailed accessibility report in artifacts
        2. Address critical WCAG violations
        3. Test with actual assistive technology users
        4. Consider accessibility expert review
        EOF
          
          echo "ðŸ“„ Accessibility compliance report generated"
        fi
