name: UI Tests and Accessibility Validation

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/pynomaly/presentation/web/**'
      - 'tests/ui/**'
      - 'docker-compose.ui-testing.yml'
      - '.github/workflows/ui-tests.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'src/pynomaly/presentation/web/**'
      - 'tests/ui/**'

env:
  DOCKER_BUILDKIT: 1
  COMPOSE_DOCKER_CLI_BUILD: 1

jobs:
  ui-tests:
    name: UI Automation Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    services:
      # PostgreSQL for testing
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: testpass
          POSTGRES_USER: testuser
          POSTGRES_DB: pynomaly_test
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
        
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Create test directories
      run: |
        mkdir -p reports screenshots test-results visual-baselines
        chmod 755 reports screenshots test-results visual-baselines
        
    - name: Cache Docker layers
      uses: actions/cache@v3
      with:
        path: /tmp/.buildx-cache
        key: ${{ runner.os }}-buildx-ui-${{ github.sha }}
        restore-keys: |
          ${{ runner.os }}-buildx-ui-
          
    - name: Build and start application
      run: |
        # Build images with cache
        docker-compose -f docker-compose.ui-testing.yml build \
          --cache-from type=local,src=/tmp/.buildx-cache \
          --cache-to type=local,dest=/tmp/.buildx-cache-new,mode=max
        
        # Start application
        docker-compose -f docker-compose.ui-testing.yml up -d pynomaly-app
        
        # Wait for application to be ready
        timeout 120s bash -c 'until docker-compose -f docker-compose.ui-testing.yml ps pynomaly-app | grep -q healthy; do sleep 5; done'
        
    - name: Run UI tests
      run: |
        # Run comprehensive UI test suite
        docker-compose -f docker-compose.ui-testing.yml run --rm ui-tests
      continue-on-error: true
      id: ui-tests
      
    - name: Run visual regression tests
      run: |
        # Run visual regression tests
        docker-compose -f docker-compose.ui-testing.yml run --rm visual-tests
      continue-on-error: true
      id: visual-tests
      
    - name: Generate comprehensive report
      run: |
        # Generate final report with critiques
        docker-compose -f docker-compose.ui-testing.yml run --rm \
          --entrypoint="" ui-tests python tests/ui/run_ui_tests.py
      continue-on-error: true
      
    - name: Collect test artifacts
      if: always()
      run: |
        # Copy artifacts from containers
        docker cp $(docker-compose -f docker-compose.ui-testing.yml ps -q ui-tests):/app/screenshots/. ./screenshots/ 2>/dev/null || true
        docker cp $(docker-compose -f docker-compose.ui-testing.yml ps -q ui-tests):/app/reports/. ./reports/ 2>/dev/null || true
        docker cp $(docker-compose -f docker-compose.ui-testing.yml ps -q ui-tests):/app/test-results/. ./test-results/ 2>/dev/null || true
        docker cp $(docker-compose -f docker-compose.ui-testing.yml ps -q ui-tests):/app/visual-baselines/. ./visual-baselines/ 2>/dev/null || true
        
        # Set permissions
        sudo chown -R $USER:$USER screenshots reports test-results visual-baselines
        
        # Generate artifact summary
        echo "## UI Test Artifacts" > artifact-summary.md
        echo "- Screenshots: $(find screenshots -name "*.png" | wc -l) files" >> artifact-summary.md
        echo "- Reports: $(find reports -name "*.html" | wc -l) HTML reports" >> artifact-summary.md
        echo "- Test Results: $(find test-results -type f | wc -l) files" >> artifact-summary.md
        echo "- Visual Baselines: $(find visual-baselines -name "*.png" | wc -l) baselines" >> artifact-summary.md
        
    - name: Upload test reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ui-test-reports-${{ github.run_number }}
        path: |
          reports/
          artifact-summary.md
        retention-days: 30
        
    - name: Upload screenshots
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ui-screenshots-${{ github.run_number }}
        path: screenshots/
        retention-days: 14
        
    - name: Upload test results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: ui-test-results-${{ github.run_number }}
        path: test-results/
        retention-days: 7
        
    - name: Upload visual baselines
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: visual-baselines-${{ github.run_number }}
        path: visual-baselines/
        retention-days: 30
        
    - name: Parse test results
      if: always()
      run: |
        # Extract test results for status checks
        if [ -f "reports/ui_test_summary_*.json" ]; then
          REPORT_FILE=$(ls reports/ui_test_summary_*.json | head -1)
          
          # Extract key metrics
          OVERALL_SCORE=$(python3 -c "import json; data=json.load(open('$REPORT_FILE')); print(data.get('overall_score', 0))")
          CRITICAL_ISSUES=$(python3 -c "import json; data=json.load(open('$REPORT_FILE')); print(data.get('critical_issues_count', 0))")
          ACCESSIBILITY_SCORE=$(python3 -c "import json; data=json.load(open('$REPORT_FILE')); print(data.get('category_scores', {}).get('accessibility', 0))")
          
          # Set environment variables for next steps
          echo "OVERALL_SCORE=$OVERALL_SCORE" >> $GITHUB_ENV
          echo "CRITICAL_ISSUES=$CRITICAL_ISSUES" >> $GITHUB_ENV
          echo "ACCESSIBILITY_SCORE=$ACCESSIBILITY_SCORE" >> $GITHUB_ENV
          
          # Generate status summary
          echo "### üéØ UI Test Results" >> $GITHUB_STEP_SUMMARY
          echo "- **Overall Score:** $OVERALL_SCORE/100" >> $GITHUB_STEP_SUMMARY
          echo "- **Critical Issues:** $CRITICAL_ISSUES" >> $GITHUB_STEP_SUMMARY
          echo "- **Accessibility Score:** $ACCESSIBILITY_SCORE/100" >> $GITHUB_STEP_SUMMARY
          
          if [ "$OVERALL_SCORE" -lt "85" ]; then
            echo "- ‚ö†Ô∏è **Status:** Needs Improvement" >> $GITHUB_STEP_SUMMARY
          else
            echo "- ‚úÖ **Status:** Good Quality" >> $GITHUB_STEP_SUMMARY
          fi
        fi
        
    - name: Comment on PR
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          
          // Read test summary if available
          let summary = "## üîç UI Test Results\n\n";
          
          try {
            const summaryFile = fs.readdirSync('reports').find(f => f.startsWith('ui_test_summary_'));
            if (summaryFile) {
              const results = JSON.parse(fs.readFileSync(`reports/${summaryFile}`, 'utf8'));
              
              summary += `### Scores\n`;
              summary += `- **Overall:** ${results.overall_score}/100\n`;
              summary += `- **Accessibility:** ${results.category_scores?.accessibility || 'N/A'}/100\n`;
              summary += `- **Layout:** ${results.category_scores?.layout || 'N/A'}/100\n`;
              summary += `- **UX Flows:** ${results.category_scores?.ux || 'N/A'}/100\n`;
              summary += `- **Responsive:** ${results.category_scores?.responsive || 'N/A'}/100\n\n`;
              
              summary += `### Issues\n`;
              summary += `- **Critical:** ${results.critical_issues_count || 0}\n`;
              summary += `- **Warnings:** ${results.warnings_count || 0}\n\n`;
              
              if (results.critical_issues_count > 0) {
                summary += `‚ö†Ô∏è **Action Required:** Please address critical issues before merging.\n\n`;
              }
              
              summary += `üìä [View detailed report in artifacts](${context.payload.pull_request.html_url}/checks)\n`;
            }
          } catch (error) {
            summary += "‚ùå Could not parse test results. Check artifacts for details.\n";
          }
          
          // Find existing comment
          const comments = await github.rest.issues.listComments({
            owner: context.repo.owner,
            repo: context.repo.repo,
            issue_number: context.issue.number,
          });
          
          const existingComment = comments.data.find(comment => 
            comment.user.type === 'Bot' && comment.body.includes('üîç UI Test Results')
          );
          
          if (existingComment) {
            // Update existing comment
            await github.rest.issues.updateComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              comment_id: existingComment.id,
              body: summary
            });
          } else {
            // Create new comment
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: summary
            });
          }
          
    - name: Quality gates
      if: github.event_name == 'pull_request'
      run: |
        # Quality gates for PR merging
        if [ "${CRITICAL_ISSUES:-0}" -gt "0" ]; then
          echo "‚ùå QUALITY GATE FAILED: $CRITICAL_ISSUES critical accessibility issues found"
          echo "Please fix critical issues before merging."
          exit 1
        fi
        
        if [ "${ACCESSIBILITY_SCORE:-0}" -lt "80" ]; then
          echo "‚ùå QUALITY GATE FAILED: Accessibility score $ACCESSIBILITY_SCORE is below minimum (80)"
          echo "Please improve accessibility before merging."
          exit 1
        fi
        
        if [ "${OVERALL_SCORE:-0}" -lt "75" ]; then
          echo "‚ùå QUALITY GATE FAILED: Overall UI score $OVERALL_SCORE is below minimum (75)"
          echo "Please improve UI quality before merging."
          exit 1
        fi
        
        echo "‚úÖ All quality gates passed!"
        
    - name: Update visual baselines (main branch only)
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        # Auto-update visual baselines on main branch
        if [ -d "visual-baselines" ] && [ "$(ls -A visual-baselines)" ]; then
          echo "üì∏ Updating visual regression baselines"
          
          # Commit updated baselines
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          
          git add visual-baselines/
          
          if ! git diff --cached --quiet; then
            git commit -m "chore: update visual regression baselines [skip ci]"
            git push
          fi
        fi
        
    - name: Cleanup
      if: always()
      run: |
        # Cleanup Docker resources
        docker-compose -f docker-compose.ui-testing.yml down -v
        docker system prune -f
        
        # Move cache
        if [ -d "/tmp/.buildx-cache-new" ]; then
          rm -rf /tmp/.buildx-cache
          mv /tmp/.buildx-cache-new /tmp/.buildx-cache
        fi
        
  accessibility-audit:
    name: Accessibility Compliance Check
    runs-on: ubuntu-latest
    needs: ui-tests
    if: always()
    
    steps:
    - name: Download test reports
      uses: actions/download-artifact@v4
      with:
        name: ui-test-reports-${{ github.run_number }}
        path: reports/
        
    - name: Accessibility compliance check
      run: |
        # Check if accessibility meets compliance standards
        if [ -f "reports/ui_test_summary_*.json" ]; then
          REPORT_FILE=$(ls reports/ui_test_summary_*.json | head -1)
          
          ACCESSIBILITY_SCORE=$(python3 -c "import json; data=json.load(open('$REPORT_FILE')); print(data.get('category_scores', {}).get('accessibility', 0))")
          CRITICAL_A11Y=$(python3 -c "import json; data=json.load(open('$REPORT_FILE')); print(sum(1 for issue in data.get('test_results', {}).get('accessibility', {}).get('issues', []) if issue.get('severity') == 'critical'))")
          
          echo "üîç Accessibility Analysis:"
          echo "  Score: $ACCESSIBILITY_SCORE/100"
          echo "  Critical Issues: $CRITICAL_A11Y"
          
          # Generate compliance report
          cat > accessibility-compliance.md << EOF
        # üîç Accessibility Compliance Report
        
        ## Summary
        - **Accessibility Score:** $ACCESSIBILITY_SCORE/100
        - **Critical Issues:** $CRITICAL_A11Y
        - **Compliance Level:** $([ "$ACCESSIBILITY_SCORE" -ge "90" ] && echo "AA" || [ "$ACCESSIBILITY_SCORE" -ge "80" ] && echo "A" || echo "Non-compliant")
        
        ## Recommendations
        $([ "$ACCESSIBILITY_SCORE" -lt "80" ] && echo "‚ùå **Immediate action required** - Below minimum accessibility standards" || echo "‚úÖ Meets basic accessibility standards")
        
        ## Next Steps
        1. Review detailed accessibility report in artifacts
        2. Address critical WCAG violations
        3. Test with actual assistive technology users
        4. Consider accessibility expert review
        EOF
          
          echo "üìÑ Accessibility compliance report generated"
        fi
