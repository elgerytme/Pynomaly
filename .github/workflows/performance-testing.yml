name: Comprehensive Integration Testing - GitHub Issue #164

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run comprehensive tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of comprehensive test'
        required: true
        default: 'comprehensive'
        type: choice
        options:
        - benchmark
        - load
        - stress
        - endurance
        - security
        - multi_tenant
        - disaster_recovery
        - api_contract
        - comprehensive
      test_intensity:
        description: 'Test intensity level'
        required: false
        default: 'medium'
        type: choice
        options:
        - light
        - medium
        - heavy
        - extreme
      baseline_update:
        description: 'Update performance baselines'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'
  PYNOMALY_ENV: testing

jobs:
  comprehensive-integration-testing:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: github.event.inputs.test_type == 'comprehensive' || github.event_name == 'schedule' || github.event_name == 'push'
    
    strategy:
      matrix:
        test-suite: [end_to_end, performance, security, multi_tenant, disaster_recovery, api_contract]
        
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test,monitoring,security]
        pip install psutil memory-profiler pytest-benchmark pytest-asyncio
    
    - name: Set test intensity parameters
      run: |
        case "${{ github.event.inputs.test_intensity || 'medium' }}" in
          light)
            echo "TEST_DURATION=30" >> $GITHUB_ENV
            echo "TEST_USERS=5" >> $GITHUB_ENV
            echo "TEST_ITERATIONS=50" >> $GITHUB_ENV
            ;;
          medium)
            echo "TEST_DURATION=60" >> $GITHUB_ENV
            echo "TEST_USERS=10" >> $GITHUB_ENV
            echo "TEST_ITERATIONS=100" >> $GITHUB_ENV
            ;;
          heavy)
            echo "TEST_DURATION=180" >> $GITHUB_ENV
            echo "TEST_USERS=25" >> $GITHUB_ENV
            echo "TEST_ITERATIONS=200" >> $GITHUB_ENV
            ;;
          extreme)
            echo "TEST_DURATION=600" >> $GITHUB_ENV
            echo "TEST_USERS=50" >> $GITHUB_ENV
            echo "TEST_ITERATIONS=500" >> $GITHUB_ENV
            ;;
        esac
    
    - name: Run comprehensive integration tests
      run: |
        export PYTHONPATH="${PYTHONPATH}:${PWD}/src"
        
        # Run specific test suite based on matrix
        case "${{ matrix.test-suite }}" in
          end_to_end)
            pytest -v -m "end_to_end" \
              tests/integration/test_integration_workflows.py::TestIntegrationWorkflows::test_comprehensive_workflow_validation \
              --tb=short --timeout=300
            ;;
          performance)
            pytest -v -m "performance" \
              tests/integration/test_integration_workflows.py::TestIntegrationWorkflows::test_performance_and_load_validation \
              tests/performance/test_performance_framework.py \
              --tb=short --timeout=600
            ;;
          security)
            pytest -v -m "security" \
              tests/integration/test_integration_workflows.py::TestIntegrationWorkflows::test_security_compliance_validation \
              --tb=short --timeout=300
            ;;
          multi_tenant)
            pytest -v -m "multi_tenant" \
              tests/integration/test_integration_workflows.py::TestIntegrationWorkflows::test_multi_tenant_isolation_validation \
              --tb=short --timeout=300
            ;;
          disaster_recovery)
            pytest -v -m "disaster_recovery" \
              tests/integration/test_integration_workflows.py::TestIntegrationWorkflows::test_disaster_recovery_validation \
              --tb=short --timeout=300
            ;;
          api_contract)
            pytest -v -m "api_contract" \
              tests/integration/test_integration_workflows.py::TestIntegrationWorkflows::test_api_contract_validation \
              --tb=short --timeout=300
            ;;
        esac
    
    - name: Generate test reports
      if: always()
      run: |
        # Create comprehensive test report
        mkdir -p reports/integration
        
        # Generate JSON report for each test suite
        echo "{" > reports/integration/${{ matrix.test-suite }}-report.json
        echo "  \"test_suite\": \"${{ matrix.test-suite }}\"," >> reports/integration/${{ matrix.test-suite }}-report.json
        echo "  \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"," >> reports/integration/${{ matrix.test-suite }}-report.json
        echo "  \"github_run_id\": \"${{ github.run_id }}\"," >> reports/integration/${{ matrix.test-suite }}-report.json
        echo "  \"commit_sha\": \"${{ github.sha }}\"," >> reports/integration/${{ matrix.test-suite }}-report.json
        echo "  \"test_intensity\": \"${{ github.event.inputs.test_intensity || 'medium' }}\"," >> reports/integration/${{ matrix.test-suite }}-report.json
        echo "  \"status\": \"completed\"" >> reports/integration/${{ matrix.test-suite }}-report.json
        echo "}" >> reports/integration/${{ matrix.test-suite }}-report.json
    
    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: integration-test-results-${{ matrix.test-suite }}
        path: |
          reports/integration/
          pytest-reports/
        retention-days: 30

  performance-benchmarks:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event.inputs.test_type == 'benchmark' || github.event.inputs.test_type == 'comprehensive'
    
    strategy:
      matrix:
        test-suite: [core, algorithms, api, infrastructure]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for performance comparison
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test,monitoring]
    
    - name: Cache performance baselines
      uses: actions/cache@v3
      with:
        path: tests/performance/baselines/
        key: performance-baselines-${{ runner.os }}-${{ github.sha }}
        restore-keys: |
          performance-baselines-${{ runner.os }}-
    
    - name: Run performance benchmarks
      run: |
        pytest -v -m "benchmark and ${{ matrix.test-suite }}" \
          --benchmark-json=benchmark-${{ matrix.test-suite }}.json \
          --benchmark-min-rounds=5 \
          --benchmark-max-time=300 \
          --benchmark-warmup=on \
          --benchmark-histogram=benchmark-${{ matrix.test-suite }}-histogram.svg \
          tests/performance/
    
    - name: Performance regression detection
      run: |
        python tests/performance/regression/performance_regression_detector.py \
          --input benchmark-${{ matrix.test-suite }}.json \
          --suite ${{ matrix.test-suite }} \
          --threshold 0.15 \
          --output regression-report-${{ matrix.test-suite }}.json
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-${{ matrix.test-suite }}
        path: |
          benchmark-${{ matrix.test-suite }}.json
          benchmark-${{ matrix.test-suite }}-histogram.svg
          regression-report-${{ matrix.test-suite }}.json
        retention-days: 30
    
    - name: Comment PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          try {
            const benchmark = JSON.parse(fs.readFileSync('benchmark-${{ matrix.test-suite }}.json', 'utf8'));
            const regression = JSON.parse(fs.readFileSync('regression-report-${{ matrix.test-suite }}.json', 'utf8'));
            
            let comment = `## Performance Test Results - ${{ matrix.test-suite }}\n\n`;
            comment += `**Tests Run:** ${benchmark.tests.length}\n`;
            comment += `**Average Performance:** ${benchmark.benchmarks[0]?.stats?.mean || 'N/A'}\n\n`;
            
            if (regression.regressions && regression.regressions.length > 0) {
              comment += `⚠️ **Performance Regressions Detected:**\n`;
              regression.regressions.forEach(reg => {
                comment += `- ${reg.test}: ${reg.change_percent}% slower\n`;
              });
            } else {
              comment += `✅ **No performance regressions detected**\n`;
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.log('Could not post performance results:', error);
          }

  load-testing:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: github.event.inputs.test_type == 'load' || github.event.inputs.test_type == 'comprehensive' || github.event_name == 'schedule'
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: pynomaly_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test,server,production]
    
    - name: Install k6
      run: |
        sudo gpg -k
        sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
        echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
        sudo apt-get update
        sudo apt-get install k6
    
    - name: Start Pynomaly API
      run: |
        export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/pynomaly_test
        export REDIS_URL=redis://localhost:6379
        uvicorn pynomaly.presentation.api.app:app --host 0.0.0.0 --port 8000 &
        sleep 10
      env:
        PYNOMALY_ENVIRONMENT: testing
    
    - name: Health check
      run: |
        curl -f http://localhost:8000/api/health/ || exit 1
    
    - name: Run k6 load test
      run: |
        k6 run --out json=k6-results.json tests/performance/load_test.js
    
    - name: Run Locust load test
      run: |
        cd tests/load
        locust -f locustfile.py --host=http://localhost:8000 \
          --users 50 --spawn-rate 5 --run-time 5m \
          --html=locust-report.html --csv=locust-results \
          --headless
    
    - name: Upload load test results
      uses: actions/upload-artifact@v3
      with:
        name: load-test-results
        path: |
          k6-results.json
          tests/load/locust-report.html
          tests/load/locust-results_*.csv
        retention-days: 30

  memory-profiling:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event.inputs.test_type == 'comprehensive' || github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies with profiling tools
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test]
        pip install memray memory-profiler pympler
    
    - name: Run memory profiling tests
      run: |
        python -m pytest -v -m "memory" tests/performance/ \
          --profile-svg --profile --profile-restrict=10
    
    - name: Generate memory reports
      run: |
        python tests/performance/memory_analysis.py
    
    - name: Upload memory analysis
      uses: actions/upload-artifact@v3
      with:
        name: memory-analysis
        path: |
          prof/
          memory_analysis_*.html
        retention-days: 30

  performance-regression:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [performance-benchmarks]
    if: always() && (needs.performance-benchmarks.result == 'success' || needs.performance-benchmarks.result == 'failure')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Download all benchmark results
      uses: actions/download-artifact@v3
      with:
        path: benchmark-artifacts/
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test]
        pip install pandas matplotlib seaborn
    
    - name: Comprehensive regression analysis
      run: |
        python tests/performance/regression/comprehensive_analysis.py \
          --input-dir benchmark-artifacts/ \
          --output comprehensive-regression-report.json \
          --generate-plots
    
    - name: Update baselines
      if: github.event.inputs.baseline_update == 'true' || (github.event_name == 'push' && github.ref == 'refs/heads/main')
      run: |
        python tests/performance/regression/update_baselines.py \
          --input-dir benchmark-artifacts/ \
          --commit-hash ${{ github.sha }}
    
    - name: Create performance summary
      run: |
        python tests/performance/generate_summary.py \
          --results-dir benchmark-artifacts/ \
          --output performance-summary.md
    
    - name: Upload final analysis
      uses: actions/upload-artifact@v3
      with:
        name: performance-analysis
        path: |
          comprehensive-regression-report.json
          performance-summary.md
          performance-plots/
        retention-days: 90
    
    - name: Performance gate check
      run: |
        python tests/performance/performance_gate.py \
          --report comprehensive-regression-report.json \
          --max-regression 20 \
          --max-failures 5

  performance-summary:
    runs-on: ubuntu-latest
    needs: [performance-benchmarks, load-testing, memory-profiling, performance-regression]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v3
      with:
        path: all-artifacts/
    
    - name: Create unified performance report
      run: |
        echo "# Performance Test Summary" > performance-report.md
        echo "- **Date**: $(date)" >> performance-report.md
        echo "- **Commit**: ${{ github.sha }}" >> performance-report.md
        echo "- **Branch**: ${{ github.ref_name }}" >> performance-report.md
        echo "" >> performance-report.md
        
        # Add results from each job
        if [ -d "all-artifacts/benchmark-results-core" ]; then
          echo "✅ Core benchmarks completed" >> performance-report.md
        fi
        if [ -d "all-artifacts/load-test-results" ]; then
          echo "✅ Load testing completed" >> performance-report.md
        fi
        if [ -d "all-artifacts/memory-analysis" ]; then
          echo "✅ Memory profiling completed" >> performance-report.md
        fi
        if [ -d "all-artifacts/performance-analysis" ]; then
          echo "✅ Regression analysis completed" >> performance-report.md
        fi
    
    - name: Upload final report
      uses: actions/upload-artifact@v3
      with:
        name: performance-final-report
        path: performance-report.md
        retention-days: 180