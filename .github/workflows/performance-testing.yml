name: Performance Testing

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run performance tests daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test'
        required: true
        default: 'benchmark'
        type: choice
        options:
        - benchmark
        - load
        - regression
        - comprehensive
      baseline_update:
        description: 'Update performance baselines'
        required: false
        default: false
        type: boolean

env:
  PYTHON_VERSION: '3.11'

jobs:
  performance-benchmarks:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    strategy:
      matrix:
        test-suite: [core, algorithms, api, infrastructure]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for performance comparison
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test,monitoring]
    
    - name: Cache performance baselines
      uses: actions/cache@v3
      with:
        path: tests/performance/baselines/
        key: performance-baselines-${{ runner.os }}-${{ github.sha }}
        restore-keys: |
          performance-baselines-${{ runner.os }}-
    
    - name: Run performance benchmarks
      run: |
        pytest -v -m "benchmark and ${{ matrix.test-suite }}" \
          --benchmark-json=benchmark-${{ matrix.test-suite }}.json \
          --benchmark-min-rounds=5 \
          --benchmark-max-time=300 \
          --benchmark-warmup=on \
          --benchmark-histogram=benchmark-${{ matrix.test-suite }}-histogram.svg \
          tests/performance/
    
    - name: Performance regression detection
      run: |
        python tests/performance/regression/performance_regression_detector.py \
          --input benchmark-${{ matrix.test-suite }}.json \
          --suite ${{ matrix.test-suite }} \
          --threshold 0.15 \
          --output regression-report-${{ matrix.test-suite }}.json
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-${{ matrix.test-suite }}
        path: |
          benchmark-${{ matrix.test-suite }}.json
          benchmark-${{ matrix.test-suite }}-histogram.svg
          regression-report-${{ matrix.test-suite }}.json
        retention-days: 30
    
    - name: Comment PR with performance results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          try {
            const benchmark = JSON.parse(fs.readFileSync('benchmark-${{ matrix.test-suite }}.json', 'utf8'));
            const regression = JSON.parse(fs.readFileSync('regression-report-${{ matrix.test-suite }}.json', 'utf8'));
            
            let comment = `## Performance Test Results - ${{ matrix.test-suite }}\n\n`;
            comment += `**Tests Run:** ${benchmark.tests.length}\n`;
            comment += `**Average Performance:** ${benchmark.benchmarks[0]?.stats?.mean || 'N/A'}\n\n`;
            
            if (regression.regressions && regression.regressions.length > 0) {
              comment += `⚠️ **Performance Regressions Detected:**\n`;
              regression.regressions.forEach(reg => {
                comment += `- ${reg.test}: ${reg.change_percent}% slower\n`;
              });
            } else {
              comment += `✅ **No performance regressions detected**\n`;
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
          } catch (error) {
            console.log('Could not post performance results:', error);
          }

  load-testing:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: github.event.inputs.test_type == 'load' || github.event.inputs.test_type == 'comprehensive' || github.event_name == 'schedule'
    
    services:
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: pynomaly_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test,server,production]
    
    - name: Install k6
      run: |
        sudo gpg -k
        sudo gpg --no-default-keyring --keyring /usr/share/keyrings/k6-archive-keyring.gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys C5AD17C747E3415A3642D57D77C6C491D6AC1D69
        echo "deb [signed-by=/usr/share/keyrings/k6-archive-keyring.gpg] https://dl.k6.io/deb stable main" | sudo tee /etc/apt/sources.list.d/k6.list
        sudo apt-get update
        sudo apt-get install k6
    
    - name: Start Pynomaly API
      run: |
        export DATABASE_URL=postgresql://postgres:postgres@localhost:5432/pynomaly_test
        export REDIS_URL=redis://localhost:6379
        uvicorn pynomaly.presentation.api.app:app --host 0.0.0.0 --port 8000 &
        sleep 10
      env:
        PYNOMALY_ENVIRONMENT: testing
    
    - name: Health check
      run: |
        curl -f http://localhost:8000/api/health/ || exit 1
    
    - name: Run k6 load test
      run: |
        k6 run --out json=k6-results.json tests/performance/load_test.js
    
    - name: Run Locust load test
      run: |
        cd tests/load
        locust -f locustfile.py --host=http://localhost:8000 \
          --users 50 --spawn-rate 5 --run-time 5m \
          --html=locust-report.html --csv=locust-results \
          --headless
    
    - name: Upload load test results
      uses: actions/upload-artifact@v3
      with:
        name: load-test-results
        path: |
          k6-results.json
          tests/load/locust-report.html
          tests/load/locust-results_*.csv
        retention-days: 30

  memory-profiling:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    if: github.event.inputs.test_type == 'comprehensive' || github.event_name == 'schedule'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies with profiling tools
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test]
        pip install memray memory-profiler pympler
    
    - name: Run memory profiling tests
      run: |
        python -m pytest -v -m "memory" tests/performance/ \
          --profile-svg --profile --profile-restrict=10
    
    - name: Generate memory reports
      run: |
        python tests/performance/memory_analysis.py
    
    - name: Upload memory analysis
      uses: actions/upload-artifact@v3
      with:
        name: memory-analysis
        path: |
          prof/
          memory_analysis_*.html
        retention-days: 30

  performance-regression:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [performance-benchmarks]
    if: always() && (needs.performance-benchmarks.result == 'success' || needs.performance-benchmarks.result == 'failure')
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Download all benchmark results
      uses: actions/download-artifact@v3
      with:
        path: benchmark-artifacts/
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .[test]
        pip install pandas matplotlib seaborn
    
    - name: Comprehensive regression analysis
      run: |
        python tests/performance/regression/comprehensive_analysis.py \
          --input-dir benchmark-artifacts/ \
          --output comprehensive-regression-report.json \
          --generate-plots
    
    - name: Update baselines
      if: github.event.inputs.baseline_update == 'true' || (github.event_name == 'push' && github.ref == 'refs/heads/main')
      run: |
        python tests/performance/regression/update_baselines.py \
          --input-dir benchmark-artifacts/ \
          --commit-hash ${{ github.sha }}
    
    - name: Create performance summary
      run: |
        python tests/performance/generate_summary.py \
          --results-dir benchmark-artifacts/ \
          --output performance-summary.md
    
    - name: Upload final analysis
      uses: actions/upload-artifact@v3
      with:
        name: performance-analysis
        path: |
          comprehensive-regression-report.json
          performance-summary.md
          performance-plots/
        retention-days: 90
    
    - name: Performance gate check
      run: |
        python tests/performance/performance_gate.py \
          --report comprehensive-regression-report.json \
          --max-regression 20 \
          --max-failures 5

  performance-summary:
    runs-on: ubuntu-latest
    needs: [performance-benchmarks, load-testing, memory-profiling, performance-regression]
    if: always()
    
    steps:
    - name: Download all artifacts
      uses: actions/download-artifact@v3
      with:
        path: all-artifacts/
    
    - name: Create unified performance report
      run: |
        echo "# Performance Test Summary" > performance-report.md
        echo "- **Date**: $(date)" >> performance-report.md
        echo "- **Commit**: ${{ github.sha }}" >> performance-report.md
        echo "- **Branch**: ${{ github.ref_name }}" >> performance-report.md
        echo "" >> performance-report.md
        
        # Add results from each job
        if [ -d "all-artifacts/benchmark-results-core" ]; then
          echo "✅ Core benchmarks completed" >> performance-report.md
        fi
        if [ -d "all-artifacts/load-test-results" ]; then
          echo "✅ Load testing completed" >> performance-report.md
        fi
        if [ -d "all-artifacts/memory-analysis" ]; then
          echo "✅ Memory profiling completed" >> performance-report.md
        fi
        if [ -d "all-artifacts/performance-analysis" ]; then
          echo "✅ Regression analysis completed" >> performance-report.md
        fi
    
    - name: Upload final report
      uses: actions/upload-artifact@v3
      with:
        name: performance-final-report
        path: performance-report.md
        retention-days: 180