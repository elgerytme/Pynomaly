name: Quality Gates

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

permissions:
  contents: read
  pull-requests: write
  checks: write

env:
  PYTHON_VERSION: "3.12"
  MIN_COVERAGE: 85
  MAX_COMPLEXITY: 10
  MAX_LINES_PER_FILE: 500

jobs:
  code-quality-metrics:
    name: Code Quality Metrics
    runs-on: ubuntu-latest
    
    outputs:
      coverage: ${{ steps.coverage.outputs.coverage }}
      complexity: ${{ steps.complexity.outputs.complexity }}
      maintainability: ${{ steps.maintainability.outputs.score }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install hatch coverage radon pytest-cov

    - name: Run tests with coverage
      id: coverage
      run: |
        hatch run test:run-cov tests/test_core_functionality.py tests/test_setup.py
        COVERAGE=$(coverage report --format=total)
        echo "coverage=$COVERAGE" >> $GITHUB_OUTPUT
        echo "Coverage: $COVERAGE%"

    - name: Calculate cyclomatic complexity
      id: complexity
      run: |
        radon cc src/ --average --show-complexity | tee complexity-report.txt
        COMPLEXITY=$(radon cc src/ --average | grep "Average complexity" | grep -oE '[0-9]+\.[0-9]+' | head -1)
        echo "complexity=$COMPLEXITY" >> $GITHUB_OUTPUT
        echo "Average complexity: $COMPLEXITY"

    - name: Calculate maintainability index
      id: maintainability
      run: |
        radon mi src/ --show | tee maintainability-report.txt
        MAINTAINABILITY=$(radon mi src/ | grep -E "^[A-Z]" | awk '{sum+=$2; count++} END {print sum/count}')
        echo "score=$MAINTAINABILITY" >> $GITHUB_OUTPUT
        echo "Maintainability index: $MAINTAINABILITY"

    - name: Upload quality reports
      uses: actions/upload-artifact@v3
      with:
        name: quality-reports
        path: |
          complexity-report.txt
          maintainability-report.txt
          htmlcov/

  test-quality-gate:
    name: Test Quality Gate
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install hatch pytest pytest-xdist pytest-timeout

    - name: Run core tests
      run: |
        hatch run test:run tests/test_core_functionality.py tests/test_setup.py -v --timeout=300

    - name: Test run scripts
      run: |
        python scripts/run/cli.py --help
        python scripts/run/run_pynomaly.py --help
        python scripts/run/run_api.py --help
        python scripts/run/run_app.py --help
        timeout 10 python scripts/run/run_web_app.py --help || true
        python scripts/run/run_web_ui.py --help

  performance-gate:
    name: Performance Quality Gate
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install hatch pytest-benchmark memory-profiler

    - name: Run performance benchmarks
      run: |
        # Create a simple performance test
        cat > test_performance.py << 'EOF'
        import time
        import sys
        sys.path.insert(0, 'src')

        def test_import_performance():
            start = time.time()
            from pynomaly.domain.entities import Dataset, Detector
            from pynomaly.application.services import DetectionService
            end = time.time()
            import_time = end - start
            print(f"Import time: {import_time:.3f}s")
            assert import_time < 2.0, f"Import time {import_time:.3f}s exceeds 2.0s threshold"

        def test_basic_functionality_performance():
            start = time.time()
            import pandas as pd
            import numpy as np
            from pynomaly.domain.entities import Dataset
            
            # Create test data
            data = pd.DataFrame({
                'x': np.random.normal(0, 1, 1000),
                'y': np.random.normal(0, 1, 1000)
            })
            dataset = Dataset(name="test", data=data, feature_names=['x', 'y'])
            end = time.time()
            
            processing_time = end - start
            print(f"Basic processing time: {processing_time:.3f}s")
            assert processing_time < 1.0, f"Processing time {processing_time:.3f}s exceeds 1.0s threshold"

        if __name__ == "__main__":
            test_import_performance()
            test_basic_functionality_performance()
            print("All performance tests passed!")
        EOF
        
        python test_performance.py

  security-gate:
    name: Security Quality Gate
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety semgrep

    - name: Run security checks
      run: |
        # Check for high-severity security issues
        bandit -r src/ -ll -f json | jq '.results | length' > bandit_count.txt
        BANDIT_ISSUES=$(cat bandit_count.txt)
        echo "Bandit high-severity issues: $BANDIT_ISSUES"
        
        # Fail if more than 0 high-severity issues
        if [ "$BANDIT_ISSUES" -gt 0 ]; then
          echo "Security gate failed: $BANDIT_ISSUES high-severity security issues found"
          bandit -r src/ -ll
          exit 1
        fi

        # Check for known vulnerabilities
        safety check --short-report || {
          echo "Security gate failed: Known vulnerabilities found"
          exit 1
        }

  quality-gate-summary:
    name: Quality Gate Summary
    runs-on: ubuntu-latest
    needs: [code-quality-metrics, test-quality-gate, performance-gate, security-gate]
    if: always()
    
    steps:
    - name: Evaluate quality gates
      run: |
        echo "## Quality Gate Results" > quality-summary.md
        echo "" >> quality-summary.md
        
        # Coverage gate
        COVERAGE="${{ needs.code-quality-metrics.outputs.coverage }}"
        if [ -n "$COVERAGE" ] && [ "$COVERAGE" -ge "$MIN_COVERAGE" ]; then
          echo "✅ Coverage Gate: PASSED ($COVERAGE% >= $MIN_COVERAGE%)" >> quality-summary.md
        else
          echo "❌ Coverage Gate: FAILED ($COVERAGE% < $MIN_COVERAGE%)" >> quality-summary.md
        fi
        
        # Complexity gate
        COMPLEXITY="${{ needs.code-quality-metrics.outputs.complexity }}"
        if [ -n "$COMPLEXITY" ]; then
          if [ "$(echo "$COMPLEXITY <= $MAX_COMPLEXITY" | bc -l)" = "1" ]; then
            echo "✅ Complexity Gate: PASSED ($COMPLEXITY <= $MAX_COMPLEXITY)" >> quality-summary.md
          else
            echo "❌ Complexity Gate: FAILED ($COMPLEXITY > $MAX_COMPLEXITY)" >> quality-summary.md
          fi
        fi
        
        # Test gate
        if [ "${{ needs.test-quality-gate.result }}" = "success" ]; then
          echo "✅ Test Gate: PASSED" >> quality-summary.md
        else
          echo "❌ Test Gate: FAILED" >> quality-summary.md
        fi
        
        # Performance gate
        if [ "${{ needs.performance-gate.result }}" = "success" ]; then
          echo "✅ Performance Gate: PASSED" >> quality-summary.md
        else
          echo "❌ Performance Gate: FAILED" >> quality-summary.md
        fi
        
        # Security gate
        if [ "${{ needs.security-gate.result }}" = "success" ]; then
          echo "✅ Security Gate: PASSED" >> quality-summary.md
        else
          echo "❌ Security Gate: FAILED" >> quality-summary.md
        fi
        
        echo "" >> quality-summary.md
        echo "### Metrics" >> quality-summary.md
        echo "- Coverage: $COVERAGE%" >> quality-summary.md
        echo "- Complexity: $COMPLEXITY" >> quality-summary.md
        echo "- Maintainability: ${{ needs.code-quality-metrics.outputs.maintainability }}" >> quality-summary.md
        
        cat quality-summary.md

    - name: Comment on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const summary = fs.readFileSync('quality-summary.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: summary
          });

    - name: Fail if any gate failed
      run: |
        # Check if any required gates failed
        FAILED_GATES=""
        
        if [ "${{ needs.test-quality-gate.result }}" != "success" ]; then
          FAILED_GATES="$FAILED_GATES test"
        fi
        
        if [ "${{ needs.security-gate.result }}" != "success" ]; then
          FAILED_GATES="$FAILED_GATES security"
        fi
        
        if [ -n "$FAILED_GATES" ]; then
          echo "Quality gates failed:$FAILED_GATES"
          exit 1
        fi
        
        echo "All required quality gates passed!"