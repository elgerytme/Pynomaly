name: Security Testing Integration

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'tests/security/**'
      - '.github/workflows/security-testing-integration.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'tests/security/**'
  schedule:
    # Run comprehensive security tests weekly
    - cron: '0 4 * * 1'
  workflow_dispatch:
    inputs:
      security_scope:
        description: 'Security testing scope'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - authentication
          - authorization
          - input_validation
          - crypto
          - network
          - dependency_scan
          - code_analysis
      severity_threshold:
        description: 'Minimum severity to fail CI'
        required: false
        default: 'medium'
        type: choice
        options:
          - low
          - medium
          - high
          - critical

env:
  PYTHON_VERSION: '3.11'
  SECURITY_SCOPE: ${{ github.event.inputs.security_scope || 'all' }}
  SEVERITY_THRESHOLD: ${{ github.event.inputs.severity_threshold || 'medium' }}

jobs:
  # Static security analysis
  static-security-analysis:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache security tools
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/pip
            ~/.cache/bandit
            ~/.cache/safety
          key: ${{ runner.os }}-security-${{ hashFiles('**/requirements*.txt') }}

      - name: Install security tools
        run: |
          python -m pip install --upgrade pip
          pip install bandit safety semgrep pytest-security
          pip install -e .[test,security]

      - name: Run Bandit security scan
        run: |
          bandit -r src/ \
            -f json \
            -o bandit-results.json \
            -ll \
            --exclude="*/tests/*" \
            || true

      - name: Run Safety dependency scan
        run: |
          safety check \
            --json \
            --output safety-results.json \
            --continue-on-error \
            || true

      - name: Run Semgrep security scan
        run: |
          # Use semgrep for additional security patterns
          python -m pip install semgrep
          semgrep \
            --config=auto \
            --json \
            --output=semgrep-results.json \
            src/ \
            || true

      - name: Analyze security scan results
        run: |
          python << 'EOF'
          import json
          from datetime import datetime
          from pathlib import Path
          
          # Load scan results
          results = {
              'timestamp': datetime.now().isoformat(),
              'commit': '${{ github.sha }}',
              'scans': {}
          }
          
          # Process Bandit results
          try:
              with open('bandit-results.json', 'r') as f:
                  bandit_data = json.load(f)
                  
              bandit_issues = bandit_data.get('results', [])
              severity_counts = {'LOW': 0, 'MEDIUM': 0, 'HIGH': 0}
              
              for issue in bandit_issues:
                  severity = issue.get('issue_severity', 'LOW')
                  severity_counts[severity] = severity_counts.get(severity, 0) + 1
              
              results['scans']['bandit'] = {
                  'total_issues': len(bandit_issues),
                  'severity_breakdown': severity_counts,
                  'high_severity_issues': [
                      {
                          'test_name': issue.get('test_name', ''),
                          'filename': issue.get('filename', ''),
                          'line_number': issue.get('line_number', 0),
                          'issue_text': issue.get('issue_text', ''),
                          'severity': issue.get('issue_severity', ''),
                          'confidence': issue.get('issue_confidence', '')
                      }
                      for issue in bandit_issues 
                      if issue.get('issue_severity') in ['HIGH', 'MEDIUM']
                  ][:10]  # Top 10
              }
          except Exception as e:
              results['scans']['bandit'] = {'error': str(e)}
          
          # Process Safety results
          try:
              with open('safety-results.json', 'r') as f:
                  safety_data = json.load(f)
              
              vulnerabilities = safety_data.get('vulnerabilities', [])
              results['scans']['safety'] = {
                  'total_vulnerabilities': len(vulnerabilities),
                  'critical_vulnerabilities': [
                      {
                          'package': vuln.get('package_name', ''),
                          'version': vuln.get('analyzed_version', ''),
                          'vulnerability_id': vuln.get('vulnerability_id', ''),
                          'advisory': vuln.get('advisory', ''),
                          'cve': vuln.get('cve', '')
                      }
                      for vuln in vulnerabilities
                  ][:10]  # Top 10
              }
          except Exception as e:
              results['scans']['safety'] = {'error': str(e)}
          
          # Process Semgrep results
          try:
              with open('semgrep-results.json', 'r') as f:
                  semgrep_data = json.load(f)
              
              semgrep_results = semgrep_data.get('results', [])
              results['scans']['semgrep'] = {
                  'total_findings': len(semgrep_results),
                  'security_findings': [
                      {
                          'rule_id': finding.get('check_id', ''),
                          'message': finding.get('extra', {}).get('message', ''),
                          'path': finding.get('path', ''),
                          'severity': finding.get('extra', {}).get('severity', 'INFO')
                      }
                      for finding in semgrep_results
                      if 'security' in finding.get('check_id', '').lower()
                  ][:10]  # Top 10 security findings
              }
          except Exception as e:
              results['scans']['semgrep'] = {'error': str(e)}
          
          # Calculate overall security score
          total_issues = 0
          critical_issues = 0
          
          for scan_name, scan_data in results['scans'].items():
              if 'error' in scan_data:
                  continue
                  
              if scan_name == 'bandit':
                  total_issues += scan_data.get('total_issues', 0)
                  critical_issues += scan_data.get('severity_breakdown', {}).get('HIGH', 0)
              elif scan_name == 'safety':
                  total_issues += scan_data.get('total_vulnerabilities', 0)
                  critical_issues += len(scan_data.get('critical_vulnerabilities', []))
              elif scan_name == 'semgrep':
                  total_issues += scan_data.get('total_findings', 0)
          
          # Security score (0-100, higher is better)
          if total_issues == 0:
              security_score = 100
          else:
              security_score = max(0, 100 - (critical_issues * 20 + total_issues * 2))
          
          results['security_score'] = security_score
          results['total_issues'] = total_issues
          results['critical_issues'] = critical_issues
          
          # Save results
          with open('security-analysis.json', 'w') as f:
              json.dump(results, f, indent=2)
          
          # Generate markdown report
          with open('security-analysis.md', 'w') as f:
              f.write("# Security Analysis Report\n\n")
              f.write(f"**Generated**: {results['timestamp']}\n")
              f.write(f"**Commit**: {results['commit']}\n")
              f.write(f"**Security Score**: {security_score}/100\n\n")
              
              # Overall summary
              status_emoji = "ðŸŸ¢" if security_score >= 80 else "ðŸŸ¡" if security_score >= 60 else "ðŸ”´"
              f.write(f"## Overall Status: {status_emoji}\n\n")
              f.write(f"- **Total Issues**: {total_issues}\n")
              f.write(f"- **Critical Issues**: {critical_issues}\n")
              f.write(f"- **Security Score**: {security_score}/100\n\n")
              
              # Scan results
              for scan_name, scan_data in results['scans'].items():
                  if 'error' in scan_data:
                      f.write(f"## {scan_name.title()} Scan\n\n")
                      f.write(f"âŒ **Error**: {scan_data['error']}\n\n")
                      continue
                  
                  f.write(f"## {scan_name.title()} Scan Results\n\n")
                  
                  if scan_name == 'bandit':
                      f.write(f"- **Total Issues**: {scan_data['total_issues']}\n")
                      f.write(f"- **High Severity**: {scan_data['severity_breakdown'].get('HIGH', 0)}\n")
                      f.write(f"- **Medium Severity**: {scan_data['severity_breakdown'].get('MEDIUM', 0)}\n")
                      f.write(f"- **Low Severity**: {scan_data['severity_breakdown'].get('LOW', 0)}\n\n")
                      
                      if scan_data['high_severity_issues']:
                          f.write("### High/Medium Severity Issues\n\n")
                          f.write("| File | Line | Test | Severity | Issue |\n")
                          f.write("|------|------|------|----------|-------|\n")
                          for issue in scan_data['high_severity_issues']:
                              f.write(f"| {issue['filename']} | {issue['line_number']} | {issue['test_name']} | {issue['severity']} | {issue['issue_text'][:50]}... |\n")
                          f.write("\n")
                  
                  elif scan_name == 'safety':
                      f.write(f"- **Total Vulnerabilities**: {scan_data['total_vulnerabilities']}\n\n")
                      
                      if scan_data['critical_vulnerabilities']:
                          f.write("### Critical Vulnerabilities\n\n")
                          f.write("| Package | Version | CVE | Advisory |\n")
                          f.write("|---------|---------|-----|----------|\n")
                          for vuln in scan_data['critical_vulnerabilities']:
                              f.write(f"| {vuln['package']} | {vuln['version']} | {vuln['cve']} | {vuln['advisory'][:50]}... |\n")
                          f.write("\n")
                  
                  elif scan_name == 'semgrep':
                      f.write(f"- **Total Findings**: {scan_data['total_findings']}\n\n")
                      
                      if scan_data['security_findings']:
                          f.write("### Security Findings\n\n")
                          f.write("| Rule | Path | Severity | Message |\n")
                          f.write("|------|------|----------|----------|\n")
                          for finding in scan_data['security_findings']:
                              f.write(f"| {finding['rule_id']} | {finding['path']} | {finding['severity']} | {finding['message'][:50]}... |\n")
                          f.write("\n")
          
          print(f"Security analysis completed")
          print(f"Security score: {security_score}/100")
          print(f"Total issues: {total_issues}")
          print(f"Critical issues: {critical_issues}")
          EOF

      - name: Upload security analysis
        uses: actions/upload-artifact@v3
        with:
          name: static-security-analysis
          path: |
            security-analysis.json
            security-analysis.md
            bandit-results.json
            safety-results.json
            semgrep-results.json
          retention-days: 90

  # Dynamic security testing
  dynamic-security-testing:
    runs-on: ubuntu-latest
    needs: [static-security-analysis]
    
    services:
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: pynomaly_test
          POSTGRES_USER: pynomaly
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-asyncio requests
          pip install -e .[test,security]

      - name: Set up test environment
        run: |
          cp .env.example .env
          echo "DATABASE_URL=postgresql://pynomaly:test_password@localhost:5432/pynomaly_test" >> .env
          echo "ENVIRONMENT=test" >> .env
          echo "AUTH_ENABLED=true" >> .env

      - name: Start API server
        run: |
          python -m pynomaly.presentation.api.app &
          API_PID=$!
          echo $API_PID > api.pid
          sleep 15  # Wait for server to start

      - name: Run authentication security tests
        if: env.SECURITY_SCOPE == 'all' || env.SECURITY_SCOPE == 'authentication'
        run: |
          pytest tests/security/test_authentication_security.py \
            -v \
            --junitxml=security-auth-results.xml

      - name: Run authorization security tests
        if: env.SECURITY_SCOPE == 'all' || env.SECURITY_SCOPE == 'authorization'
        run: |
          pytest tests/security/test_authorization_security.py \
            -v \
            --junitxml=security-authz-results.xml

      - name: Run input validation security tests
        if: env.SECURITY_SCOPE == 'all' || env.SECURITY_SCOPE == 'input_validation'
        run: |
          pytest tests/security/test_input_validation_security.py \
            -v \
            --junitxml=security-input-results.xml

      - name: Run cryptography security tests
        if: env.SECURITY_SCOPE == 'all' || env.SECURITY_SCOPE == 'crypto'
        run: |
          pytest tests/security/test_cryptography_security.py \
            -v \
            --junitxml=security-crypto-results.xml

      - name: Run network security tests
        if: env.SECURITY_SCOPE == 'all' || env.SECURITY_SCOPE == 'network'
        run: |
          pytest tests/security/test_network_security.py \
            -v \
            --junitxml=security-network-results.xml

      - name: Stop API server
        if: always()
        run: |
          if [ -f api.pid ]; then
            kill $(cat api.pid) || true
            rm api.pid
          fi

      - name: Generate dynamic security report
        run: |
          python << 'EOF'
          import xml.etree.ElementTree as ET
          import json
          import glob
          from datetime import datetime
          from pathlib import Path
          
          # Parse test results
          test_files = glob.glob('security-*-results.xml')
          
          report = {
              'timestamp': datetime.now().isoformat(),
              'commit': '${{ github.sha }}',
              'test_results': {},
              'summary': {
                  'total_tests': 0,
                  'passed': 0,
                  'failed': 0,
                  'errors': 0,
                  'skipped': 0
              }
          }
          
          for test_file in test_files:
              try:
                  tree = ET.parse(test_file)
                  root = tree.getroot()
                  
                  test_category = test_file.replace('security-', '').replace('-results.xml', '')
                  
                  testsuite = root.find('testsuite')
                  if testsuite is not None:
                      tests = int(testsuite.get('tests', 0))
                      failures = int(testsuite.get('failures', 0))
                      errors = int(testsuite.get('errors', 0))
                      skipped = int(testsuite.get('skipped', 0))
                      
                      passed = tests - failures - errors - skipped
                      
                      report['test_results'][test_category] = {
                          'total': tests,
                          'passed': passed,
                          'failed': failures,
                          'errors': errors,
                          'skipped': skipped,
                          'success_rate': (passed / max(tests, 1)) * 100
                      }
                      
                      # Update summary
                      report['summary']['total_tests'] += tests
                      report['summary']['passed'] += passed
                      report['summary']['failed'] += failures
                      report['summary']['errors'] += errors
                      report['summary']['skipped'] += skipped
                      
                      # Extract failed test details
                      failed_tests = []
                      for testcase in root.findall('.//testcase'):
                          failure = testcase.find('failure')
                          error = testcase.find('error')
                          
                          if failure is not None or error is not None:
                              failed_tests.append({
                                  'name': testcase.get('name', ''),
                                  'classname': testcase.get('classname', ''),
                                  'time': float(testcase.get('time', 0)),
                                  'failure_message': failure.get('message', '') if failure is not None else '',
                                  'error_message': error.get('message', '') if error is not None else ''
                              })
                      
                      report['test_results'][test_category]['failed_tests'] = failed_tests
                      
              except Exception as e:
                  print(f"Error parsing {test_file}: {e}")
          
          # Calculate overall security test score
          total_tests = report['summary']['total_tests']
          passed_tests = report['summary']['passed']
          
          if total_tests > 0:
              security_test_score = (passed_tests / total_tests) * 100
          else:
              security_test_score = 0
          
          report['security_test_score'] = security_test_score
          
          # Save report
          with open('dynamic-security-report.json', 'w') as f:
              json.dump(report, f, indent=2)
          
          # Generate markdown report
          with open('dynamic-security-report.md', 'w') as f:
              f.write("# Dynamic Security Testing Report\n\n")
              f.write(f"**Generated**: {report['timestamp']}\n")
              f.write(f"**Commit**: {report['commit']}\n")
              f.write(f"**Security Test Score**: {security_test_score:.1f}%\n\n")
              
              # Summary
              summary = report['summary']
              status_emoji = "ðŸŸ¢" if security_test_score >= 95 else "ðŸŸ¡" if security_test_score >= 80 else "ðŸ”´"
              f.write(f"## Overall Status: {status_emoji}\n\n")
              f.write(f"- **Total Tests**: {summary['total_tests']}\n")
              f.write(f"- **Passed**: {summary['passed']}\n")
              f.write(f"- **Failed**: {summary['failed']}\n")
              f.write(f"- **Errors**: {summary['errors']}\n")
              f.write(f"- **Skipped**: {summary['skipped']}\n")
              f.write(f"- **Success Rate**: {security_test_score:.1f}%\n\n")
              
              # Results by category
              f.write("## Results by Security Category\n\n")
              f.write("| Category | Tests | Passed | Failed | Errors | Success Rate |\n")
              f.write("|----------|-------|---------|---------|---------|---------------|\n")
              
              for category, results in report['test_results'].items():
                  f.write(f"| {category.title()} | {results['total']} | {results['passed']} | {results['failed']} | {results['errors']} | {results['success_rate']:.1f}% |\n")
              
              # Failed tests details
              failed_tests_found = False
              for category, results in report['test_results'].items():
                  if results['failed_tests']:
                      if not failed_tests_found:
                          f.write(f"\n## Failed Security Tests\n\n")
                          failed_tests_found = True
                      
                      f.write(f"### {category.title()} Failures\n\n")
                      for test in results['failed_tests']:
                          f.write(f"- **{test['name']}**\n")
                          if test['failure_message']:
                              f.write(f"  - Failure: {test['failure_message']}\n")
                          if test['error_message']:
                              f.write(f"  - Error: {test['error_message']}\n")
                          f.write(f"  - Duration: {test['time']:.3f}s\n\n")
          
          print(f"Dynamic security testing completed")
          print(f"Security test score: {security_test_score:.1f}%")
          print(f"Tests: {total_tests}, Passed: {passed_tests}, Failed: {summary['failed']}")
          EOF

      - name: Upload dynamic security results
        uses: actions/upload-artifact@v3
        with:
          name: dynamic-security-testing
          path: |
            dynamic-security-report.json
            dynamic-security-report.md
            security-*-results.xml
          retention-days: 90

  # Security test aggregation and reporting
  security-aggregation:
    runs-on: ubuntu-latest
    needs: [static-security-analysis, dynamic-security-testing]
    if: always()
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all security results
        uses: actions/download-artifact@v3
        with:
          path: security-results/

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Aggregate security results
        run: |
          python << 'EOF'
          import json
          import glob
          from datetime import datetime
          from pathlib import Path
          
          # Load all security reports
          static_report = None
          dynamic_report = None
          
          # Load static analysis report
          static_file = Path('security-results/static-security-analysis/security-analysis.json')
          if static_file.exists():
              with open(static_file, 'r') as f:
                  static_report = json.load(f)
          
          # Load dynamic testing report
          dynamic_file = Path('security-results/dynamic-security-testing/dynamic-security-report.json')
          if dynamic_file.exists():
              with open(dynamic_file, 'r') as f:
                  dynamic_report = json.load(f)
          
          # Create comprehensive security report
          comprehensive_report = {
              'timestamp': datetime.now().isoformat(),
              'commit': '${{ github.sha }}',
              'static_analysis': static_report,
              'dynamic_testing': dynamic_report,
              'overall_assessment': {}
          }
          
          # Calculate overall security score
          static_score = static_report.get('security_score', 0) if static_report else 0
          dynamic_score = dynamic_report.get('security_test_score', 0) if dynamic_report else 0
          
          # Weighted average (70% static, 30% dynamic)
          overall_score = (static_score * 0.7) + (dynamic_score * 0.3)
          
          # Security grade
          if overall_score >= 90:
              grade = 'A'
              status = 'ðŸŸ¢ Excellent'
          elif overall_score >= 80:
              grade = 'B'
              status = 'ðŸŸ¡ Good'
          elif overall_score >= 70:
              grade = 'C'
              status = 'ðŸŸ  Fair'
          elif overall_score >= 60:
              grade = 'D'
              status = 'ðŸ”´ Poor'
          else:
              grade = 'F'
              status = 'â›” Critical'
          
          comprehensive_report['overall_assessment'] = {
              'score': overall_score,
              'grade': grade,
              'status': status,
              'static_score': static_score,
              'dynamic_score': dynamic_score
          }
          
          # Recommendations
          recommendations = []
          
          if static_report:
              critical_issues = static_report.get('critical_issues', 0)
              if critical_issues > 0:
                  recommendations.append(f"ðŸ”´ Address {critical_issues} critical security issues found in static analysis")
          
          if dynamic_report:
              failed_tests = dynamic_report.get('summary', {}).get('failed', 0)
              if failed_tests > 0:
                  recommendations.append(f"ðŸ”´ Fix {failed_tests} failing security tests")
          
          if overall_score < 80:
              recommendations.append("ðŸ“‹ Consider implementing additional security controls")
              recommendations.append("ðŸ” Schedule a comprehensive security review")
          
          if not recommendations:
              recommendations.append("âœ… Security posture is good - maintain current practices")
          
          comprehensive_report['recommendations'] = recommendations
          
          # Save comprehensive report
          with open('comprehensive-security-report.json', 'w') as f:
              json.dump(comprehensive_report, f, indent=2)
          
          # Generate executive summary
          with open('security-executive-summary.md', 'w') as f:
              f.write("# Security Assessment Executive Summary\n\n")
              f.write(f"**Assessment Date**: {comprehensive_report['timestamp']}\n")
              f.write(f"**Commit**: {comprehensive_report['commit']}\n\n")
              
              f.write(f"## Overall Security Score: {overall_score:.1f}/100 (Grade: {grade})\n\n")
              f.write(f"**Status**: {status}\n\n")
              
              f.write("## Score Breakdown\n\n")
              f.write(f"- **Static Analysis**: {static_score:.1f}/100 (70% weight)\n")
              f.write(f"- **Dynamic Testing**: {dynamic_score:.1f}/100 (30% weight)\n\n")
              
              if static_report:
                  f.write("## Static Analysis Summary\n\n")
                  f.write(f"- **Total Issues**: {static_report.get('total_issues', 0)}\n")
                  f.write(f"- **Critical Issues**: {static_report.get('critical_issues', 0)}\n")
                  
                  for scan_name, scan_data in static_report.get('scans', {}).items():
                      if 'error' not in scan_data:
                          if scan_name == 'bandit':
                              f.write(f"- **Bandit Issues**: {scan_data.get('total_issues', 0)}\n")
                          elif scan_name == 'safety':
                              f.write(f"- **Vulnerabilities**: {scan_data.get('total_vulnerabilities', 0)}\n")
                  f.write("\n")
              
              if dynamic_report:
                  f.write("## Dynamic Testing Summary\n\n")
                  summary = dynamic_report.get('summary', {})
                  f.write(f"- **Total Security Tests**: {summary.get('total_tests', 0)}\n")
                  f.write(f"- **Passed**: {summary.get('passed', 0)}\n")
                  f.write(f"- **Failed**: {summary.get('failed', 0)}\n")
                  f.write(f"- **Success Rate**: {dynamic_score:.1f}%\n\n")
              
              f.write("## Recommendations\n\n")
              for rec in recommendations:
                  f.write(f"- {rec}\n")
              f.write("\n")
              
              # Security trends (if available)
              f.write("## Security Posture\n\n")
              if overall_score >= 90:
                  f.write("âœ… **Excellent**: Security posture is very strong with minimal issues.\n")
              elif overall_score >= 80:
                  f.write("ðŸŸ¡ **Good**: Security posture is solid with minor improvements needed.\n")
              elif overall_score >= 70:
                  f.write("ðŸŸ  **Fair**: Security posture needs attention and improvements.\n")
              elif overall_score >= 60:
                  f.write("ðŸ”´ **Poor**: Security posture has significant issues that need immediate attention.\n")
              else:
                  f.write("â›” **Critical**: Security posture has critical issues requiring urgent remediation.\n")
          
          print(f"Comprehensive security assessment completed")
          print(f"Overall score: {overall_score:.1f}/100 (Grade: {grade})")
          print(f"Status: {status}")
          print(f"Recommendations: {len(recommendations)}")
          EOF

      - name: Upload comprehensive security report
        uses: actions/upload-artifact@v3
        with:
          name: comprehensive-security-report
          path: |
            comprehensive-security-report.json
            security-executive-summary.md
          retention-days: 90

      - name: Add security summary to job summary
        run: |
          if [ -f security-executive-summary.md ]; then
            cat security-executive-summary.md >> $GITHUB_STEP_SUMMARY
          fi

      - name: Comment PR with security assessment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            if (fs.existsSync('security-executive-summary.md')) {
              const summary = fs.readFileSync('security-executive-summary.md', 'utf8');
              
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## ðŸ”’ Security Assessment\n\n${summary}`
              });
            }

      - name: Evaluate security threshold
        run: |
          python << 'EOF'
          import json
          
          # Load comprehensive report
          with open('comprehensive-security-report.json', 'r') as f:
              report = json.load(f)
          
          overall_score = report['overall_assessment']['score']
          grade = report['overall_assessment']['grade']
          severity_threshold = '${{ env.SEVERITY_THRESHOLD }}'
          
          # Define failure thresholds
          thresholds = {
              'low': 50,      # Grade F
              'medium': 70,   # Grade C
              'high': 80,     # Grade B
              'critical': 90  # Grade A
          }
          
          min_score = thresholds.get(severity_threshold, 70)
          
          if overall_score < min_score:
              print(f"âŒ Security assessment failed")
              print(f"Score: {overall_score:.1f}/100 (Grade: {grade})")
              print(f"Required: {min_score}/100 for {severity_threshold} threshold")
              exit(1)
          else:
              print(f"âœ… Security assessment passed")
              print(f"Score: {overall_score:.1f}/100 (Grade: {grade})")
              print(f"Required: {min_score}/100 for {severity_threshold} threshold")
          EOF
