name: Enhanced Parallel Testing Pipeline

on:
  push:
    branches: [ main, develop, feature/* ]
    paths:
      - 'src/**'
      - 'tests/**'
      - '.github/workflows/enhanced-parallel-testing.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'src/**'
      - 'tests/**'
  workflow_dispatch:
    inputs:
      test_scope:
        description: 'Test scope to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - infrastructure
          - cli
          - api
          - performance
      parallel_workers:
        description: 'Number of parallel workers'
        required: false
        default: '4'
        type: string

env:
  PYTHON_VERSION: '3.11'
  PARALLEL_WORKERS: ${{ github.event.inputs.parallel_workers || '4' }}
  TEST_SCOPE: ${{ github.event.inputs.test_scope || 'all' }}

jobs:
  # Pre-flight checks and setup
  setup:
    runs-on: ubuntu-latest
    outputs:
      test-matrix: ${{ steps.test-discovery.outputs.matrix }}
      cache-key: ${{ steps.cache-setup.outputs.cache-key }}
      should-run-tests: ${{ steps.changes.outputs.should-run }}
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect changes
        id: changes
        uses: dorny/paths-filter@v2
        with:
          filters: |
            src:
              - 'src/**'
            tests:
              - 'tests/**'
            config:
              - 'pyproject.toml'
              - 'requirements*.txt'
              - '.github/workflows/**'

      - name: Set up Python
        if: steps.changes.outputs.src == 'true' || steps.changes.outputs.tests == 'true' || steps.changes.outputs.config == 'true'
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up cache
        id: cache-setup
        if: steps.changes.outputs.src == 'true' || steps.changes.outputs.tests == 'true' || steps.changes.outputs.config == 'true'
        run: |
          CACHE_KEY="${{ runner.os }}-py${{ env.PYTHON_VERSION }}-${{ hashFiles('**/pyproject.toml', '**/requirements*.txt') }}"
          echo "cache-key=$CACHE_KEY" >> $GITHUB_OUTPUT

      - name: Cache dependencies
        if: steps.changes.outputs.src == 'true' || steps.changes.outputs.tests == 'true' || steps.changes.outputs.config == 'true'
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/pip
            ~/.cache/pytest_cache
          key: ${{ steps.cache-setup.outputs.cache-key }}
          restore-keys: |
            ${{ runner.os }}-py${{ env.PYTHON_VERSION }}-

      - name: Install dependencies
        if: steps.changes.outputs.src == 'true' || steps.changes.outputs.tests == 'true' || steps.changes.outputs.config == 'true'
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-xdist pytest-cov
          pip install -e .[test]

      - name: Discover test matrix
        id: test-discovery
        if: steps.changes.outputs.src == 'true' || steps.changes.outputs.tests == 'true' || steps.changes.outputs.config == 'true'
        run: |
          python << 'EOF'
          import json
          import os
          from pathlib import Path
          
          test_scope = os.environ.get('TEST_SCOPE', 'all')
          
          # Define test categories with their paths and characteristics
          test_categories = {
              'unit-domain': {
                  'path': 'tests/unit/domain/',
                  'timeout': 10,
                  'workers': 4,
                  'markers': 'not slow',
                  'coverage': True
              },
              'unit-application': {
                  'path': 'tests/application/',
                  'timeout': 15,
                  'workers': 4,
                  'markers': 'not slow',
                  'coverage': True
              },
              'unit-infrastructure': {
                  'path': 'tests/unit/infrastructure/',
                  'timeout': 20,
                  'workers': 2,
                  'markers': 'not integration',
                  'coverage': True
              },
              'integration-application': {
                  'path': 'tests/integration/',
                  'timeout': 30,
                  'workers': 2,
                  'markers': 'integration',
                  'coverage': True,
                  'requires': ['postgres', 'redis']
              },
              'infrastructure-auth': {
                  'path': 'tests/infrastructure/auth/',
                  'timeout': 25,
                  'workers': 2,
                  'markers': 'infrastructure',
                  'coverage': True
              },
              'infrastructure-cache': {
                  'path': 'tests/infrastructure/cache/',
                  'timeout': 20,
                  'workers': 2,
                  'markers': 'infrastructure',
                  'coverage': True,
                  'requires': ['redis']
              },
              'infrastructure-resilience': {
                  'path': 'tests/infrastructure/resilience/',
                  'timeout': 30,
                  'workers': 2,
                  'markers': 'infrastructure',
                  'coverage': True
              },
              'presentation-api': {
                  'path': 'tests/presentation/api/',
                  'timeout': 25,
                  'workers': 3,
                  'markers': 'api',
                  'coverage': True,
                  'requires': ['postgres']
              },
              'presentation-cli': {
                  'path': 'tests/cli/',
                  'timeout': 20,
                  'workers': 3,
                  'markers': 'cli',
                  'coverage': True
              },
              'presentation-web': {
                  'path': 'tests/presentation/web/',
                  'timeout': 25,
                  'workers': 2,
                  'markers': 'web',
                  'coverage': True
              },
              'performance': {
                  'path': 'tests/performance/',
                  'timeout': 60,
                  'workers': 1,
                  'markers': 'performance',
                  'coverage': False,
                  'requires': ['postgres', 'redis']
              },
              'security': {
                  'path': 'tests/security/',
                  'timeout': 30,
                  'workers': 2,
                  'markers': 'security',
                  'coverage': False
              }
          }
          
          # Filter based on test scope
          if test_scope != 'all':
              if test_scope == 'unit':
                  categories = {k: v for k, v in test_categories.items() if k.startswith('unit-')}
              elif test_scope == 'integration':
                  categories = {k: v for k, v in test_categories.items() if k.startswith('integration-')}
              elif test_scope == 'infrastructure':
                  categories = {k: v for k, v in test_categories.items() if k.startswith('infrastructure-')}
              elif test_scope == 'cli':
                  categories = {k: v for k, v in test_categories.items() if 'cli' in k}
              elif test_scope == 'api':
                  categories = {k: v for k, v in test_categories.items() if 'api' in k}
              elif test_scope == 'performance':
                  categories = {k: v for k, v in test_categories.items() if k == 'performance'}
              else:
                  categories = test_categories
          else:
              categories = test_categories
          
          # Only include categories where test path exists
          matrix = []
          for category, config in categories.items():
              test_path = Path(config['path'])
              if test_path.exists() and any(test_path.rglob('test_*.py')):
                  matrix.append({
                      'category': category,
                      'path': config['path'],
                      'timeout': config['timeout'],
                      'workers': config['workers'],
                      'markers': config.get('markers', ''),
                      'coverage': config.get('coverage', False),
                      'requires': config.get('requires', [])
                  })
          
          print(f"::set-output name=matrix::{json.dumps(matrix)}")
          print(f"Discovered {len(matrix)} test categories to run")
          for item in matrix:
              print(f"  - {item['category']}: {item['path']}")
          EOF

      - name: Set should run tests
        id: should-run
        run: |
          if [ "${{ steps.changes.outputs.src }}" == "true" ] || [ "${{ steps.changes.outputs.tests }}" == "true" ] || [ "${{ steps.changes.outputs.config }}" == "true" ]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
          else
            echo "should-run=false" >> $GITHUB_OUTPUT
          fi

  # Parallel test execution
  test-parallel:
    runs-on: ubuntu-latest
    needs: setup
    if: needs.setup.outputs.should-run-tests == 'true'
    
    strategy:
      fail-fast: false
      matrix:
        include: ${{ fromJson(needs.setup.outputs.test-matrix) }}
    
    services:
      postgres:
        image: postgres:15-alpine
        if: contains(matrix.requires, 'postgres')
        env:
          POSTGRES_PASSWORD: test_password
          POSTGRES_DB: anomaly_detection_test
          POSTGRES_USER: anomaly_detection
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
          
      redis:
        image: redis:7-alpine
        if: contains(matrix.requires, 'redis')
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Restore cache
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/pip
            ~/.cache/pytest_cache
          key: ${{ needs.setup.outputs.cache-key }}
          restore-keys: |
            ${{ runner.os }}-py${{ env.PYTHON_VERSION }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-xdist pytest-cov pytest-timeout pytest-mock
          pip install -e .[test]

      - name: Set up test environment
        run: |
          cp .env.example .env
          if [ "${{ contains(matrix.requires, 'postgres') }}" == "true" ]; then
            echo "DATABASE_URL=postgresql://anomaly_detection:test_password@localhost:5432/anomaly_detection_test" >> .env
          fi
          if [ "${{ contains(matrix.requires, 'redis') }}" == "true" ]; then
            echo "REDIS_URL=redis://localhost:6379" >> .env
          fi
          echo "ENVIRONMENT=test" >> .env
          echo "ANOMALY_DETECTION_USE_LAZY_CLI=false" >> .env

      - name: Run database migrations
        if: contains(matrix.requires, 'postgres')
        run: |
          timeout 30 alembic upgrade head || echo "Migration skipped"

      - name: Run tests with coverage
        if: matrix.coverage == true
        timeout-minutes: ${{ matrix.timeout }}
        run: |
          pytest ${{ matrix.path }} \
            -v \
            -n ${{ matrix.workers }} \
            --dist=worksteal \
            --timeout=300 \
            --cov=src/anomaly_detection \
            --cov-report=xml:coverage-${{ matrix.category }}.xml \
            --cov-report=html:htmlcov-${{ matrix.category }} \
            --cov-context=test \
            --junitxml=test-results-${{ matrix.category }}.xml \
            ${{ matrix.markers && format('-m "{0}"', matrix.markers) || '' }}

      - name: Run tests without coverage
        if: matrix.coverage == false
        timeout-minutes: ${{ matrix.timeout }}
        run: |
          pytest ${{ matrix.path }} \
            -v \
            -n ${{ matrix.workers }} \
            --dist=worksteal \
            --timeout=600 \
            --junitxml=test-results-${{ matrix.category }}.xml \
            ${{ matrix.markers && format('-m "{0}"', matrix.markers) || '' }}

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-results-${{ matrix.category }}
          path: |
            test-results-${{ matrix.category }}.xml
            coverage-${{ matrix.category }}.xml
            htmlcov-${{ matrix.category }}/
          retention-days: 30

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
        if: matrix.coverage == true && always()
        with:
          file: coverage-${{ matrix.category }}.xml
          flags: ${{ matrix.category }}
          name: ${{ matrix.category }}-coverage
          fail_ci_if_error: false

  # Aggregate results and reporting
  test-results:
    runs-on: ubuntu-latest
    needs: [setup, test-parallel]
    if: always() && needs.setup.outputs.should-run-tests == 'true'
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all test results
        uses: actions/download-artifact@v3
        with:
          path: test-results/

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pytest pytest-html junitparser coverage

      - name: Merge coverage reports
        run: |
          python << 'EOF'
          import glob
          import subprocess
          from pathlib import Path
          
          coverage_files = list(Path('test-results').rglob('coverage-*.xml'))
          
          if coverage_files:
              print(f"Found {len(coverage_files)} coverage files")
              
              # Combine coverage files
              subprocess.run(['coverage', 'combine'] + [str(f) for f in coverage_files], check=False)
              subprocess.run(['coverage', 'xml', '-o', 'combined-coverage.xml'], check=False)
              subprocess.run(['coverage', 'html', '-d', 'combined-htmlcov'], check=False)
              subprocess.run(['coverage', 'report', '--format=markdown'], 
                           stdout=open('coverage-report.md', 'w'), check=False)
              
              print("Coverage reports merged successfully")
          else:
              print("No coverage files found")
          EOF

      - name: Generate test report
        run: |
          python << 'EOF'
          import glob
          import json
          from pathlib import Path
          from datetime import datetime
          from junitparser import JUnitXml
          
          # Collect all test result files
          result_files = list(Path('test-results').rglob('test-results-*.xml'))
          
          total_tests = 0
          total_failures = 0
          total_errors = 0
          total_skipped = 0
          total_time = 0.0
          
          test_categories = {}
          
          for result_file in result_files:
              try:
                  xml = JUnitXml.fromfile(str(result_file))
                  category = result_file.stem.replace('test-results-', '')
                  
                  category_tests = xml.tests
                  category_failures = xml.failures
                  category_errors = xml.errors
                  category_skipped = xml.skipped
                  category_time = xml.time
                  
                  total_tests += category_tests
                  total_failures += category_failures
                  total_errors += category_errors
                  total_skipped += category_skipped
                  total_time += category_time
                  
                  test_categories[category] = {
                      'tests': category_tests,
                      'failures': category_failures,
                      'errors': category_errors,
                      'skipped': category_skipped,
                      'time': category_time,
                      'success_rate': round((category_tests - category_failures - category_errors) / max(category_tests, 1) * 100, 2)
                  }
                  
              except Exception as e:
                  print(f"Error processing {result_file}: {e}")
          
          # Generate summary report
          overall_success_rate = round((total_tests - total_failures - total_errors) / max(total_tests, 1) * 100, 2)
          
          report = {
              'timestamp': datetime.now().isoformat(),
              'summary': {
                  'total_tests': total_tests,
                  'failures': total_failures,
                  'errors': total_errors,
                  'skipped': total_skipped,
                  'success_rate': overall_success_rate,
                  'total_time': round(total_time, 2)
              },
              'categories': test_categories,
              'status': 'PASSED' if total_failures == 0 and total_errors == 0 else 'FAILED'
          }
          
          with open('test-summary.json', 'w') as f:
              json.dump(report, f, indent=2)
          
          # Generate markdown summary
          with open('test-summary.md', 'w') as f:
              f.write(f"# Test Results Summary\n\n")
              f.write(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}\n\n")
              f.write(f"## Overall Results\n\n")
              f.write(f"| Metric | Value |\n")
              f.write(f"|--------|-------|\n")
              f.write(f"| Total Tests | {total_tests} |\n")
              f.write(f"| Passed | {total_tests - total_failures - total_errors} |\n")
              f.write(f"| Failed | {total_failures} |\n")
              f.write(f"| Errors | {total_errors} |\n")
              f.write(f"| Skipped | {total_skipped} |\n")
              f.write(f"| Success Rate | {overall_success_rate}% |\n")
              f.write(f"| Total Time | {round(total_time, 2)}s |\n")
              f.write(f"| Status | {'✅ PASSED' if report['status'] == 'PASSED' else '❌ FAILED'} |\n\n")
              
              if test_categories:
                  f.write(f"## Results by Category\n\n")
                  f.write(f"| Category | Tests | Pass Rate | Time | Status |\n")
                  f.write(f"|----------|-------|-----------|------|--------|\n")
                  
                  for category, stats in test_categories.items():
                      status = "✅" if stats['failures'] == 0 and stats['errors'] == 0 else "❌"
                      f.write(f"| {category} | {stats['tests']} | {stats['success_rate']}% | {stats['time']:.1f}s | {status} |\n")
          
          print(f"Test summary generated: {total_tests} tests, {overall_success_rate}% success rate")
          EOF

      - name: Upload combined results
        uses: actions/upload-artifact@v3
        with:
          name: combined-test-results
          path: |
            test-summary.json
            test-summary.md
            combined-coverage.xml
            combined-htmlcov/
            coverage-report.md
          retention-days: 90

      - name: Add test results to job summary
        run: |
          if [ -f test-summary.md ]; then
            cat test-summary.md >> $GITHUB_STEP_SUMMARY
          fi
          
          if [ -f coverage-report.md ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "## Coverage Report" >> $GITHUB_STEP_SUMMARY
            cat coverage-report.md >> $GITHUB_STEP_SUMMARY
          fi

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            if (fs.existsSync('test-summary.md')) {
              const summary = fs.readFileSync('test-summary.md', 'utf8');
              
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: summary
              });
            }

      - name: Fail if tests failed
        run: |
          if [ -f test-summary.json ]; then
            STATUS=$(jq -r '.status' test-summary.json)
            if [ "$STATUS" != "PASSED" ]; then
              echo "❌ Tests failed"
              exit 1
            else
              echo "✅ All tests passed"
            fi
          else
            echo "⚠️ No test summary found"
            exit 1
          fi
