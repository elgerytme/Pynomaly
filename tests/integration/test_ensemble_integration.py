#!/usr/bin/env python3
"""Integration test for ensemble detection functionality."""

import asyncio
import os
import sys
from unittest.mock import AsyncMock, Mock

import numpy as np
import pandas as pd

# Add the src directory to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "src"))


async def test_ensemble_integration():
    """Test ensemble detection integration."""
    print("üîç Testing Pynomaly Ensemble Detection Integration")
    print("=" * 55)

    try:
        # Test imports
        from pynomaly.application.dto.ensemble_dto import (
            EnsembleDetectionRequestDTO,
        )
        from pynomaly.application.use_cases.ensemble_detection_use_case import (
            EnsembleDetectionRequest,
            EnsembleDetectionUseCase,
            EnsembleOptimizationObjective,
            EnsembleOptimizationRequest,
            VotingStrategy,
        )

        print("‚úÖ All ensemble imports successful")

        # Test DTO validation
        print("\nüìã Testing DTO Validation")
        print("-" * 25)

        # Test valid request DTO
        valid_request = EnsembleDetectionRequestDTO(
            detector_ids=["detector_1", "detector_2", "detector_3"],
            data=[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]],
            voting_strategy="weighted_average",
            enable_dynamic_weighting=True,
            enable_explanation=True,
        )
        print(
            f"‚úÖ Valid request DTO created: {len(valid_request.detector_ids)} detectors"
        )

        # Test validation errors
        try:
            invalid_request = EnsembleDetectionRequestDTO(
                detector_ids=["only_one"], data=[[1.0, 2.0]]  # Too few detectors
            )
            print("‚ùå Should have failed validation for too few detectors")
        except ValueError as e:
            print(f"‚úÖ Validation correctly caught error: {str(e)[:50]}...")

        # Test ensemble use case creation
        print("\nüè≠ Testing Ensemble Use Case")
        print("-" * 30)

        # Create mock dependencies
        detector_repo = Mock()
        dataset_repo = Mock()
        adapter_registry = Mock()

        # Create use case
        ensemble_use_case = EnsembleDetectionUseCase(
            detector_repository=detector_repo,
            dataset_repository=dataset_repo,
            adapter_registry=adapter_registry,
            enable_performance_tracking=True,
            enable_auto_optimization=True,
        )

        print("‚úÖ Ensemble use case created successfully")
        print(
            f"   Performance tracking: {ensemble_use_case.enable_performance_tracking}"
        )
        print(f"   Auto optimization: {ensemble_use_case.enable_auto_optimization}")
        print(f"   Cache size: {ensemble_use_case.cache_size}")

        # Test voting strategies
        print("\n‚ö° Testing Voting Strategies")
        print("-" * 30)

        strategies = list(VotingStrategy)
        print(f"üìä Available strategies: {len(strategies)}")
        for strategy in strategies[:6]:  # Show first 6
            print(f"   ‚Ä¢ {strategy.value}")
        print(f"   ... and {len(strategies) - 6} more")

        # Test optimization objectives
        objectives = list(EnsembleOptimizationObjective)
        print(f"üìà Available objectives: {len(objectives)}")
        for obj in objectives[:4]:  # Show first 4
            print(f"   ‚Ä¢ {obj.value}")
        print(f"   ... and {len(objectives) - 4} more")

        # Test basic ensemble detection workflow
        print("\nüéØ Testing Ensemble Detection Workflow")
        print("-" * 40)

        # Mock detectors
        mock_detectors = []
        for i in range(3):
            detector = Mock()
            detector.id = f"detector_{i}"
            detector.algorithm = "IsolationForest"
            detector.is_fitted = True
            detector.model = Mock()
            mock_detectors.append(detector)

        # Setup mock responses
        detector_repo.get = AsyncMock(
            side_effect=lambda detector_id: next(
                (d for d in mock_detectors if d.id == detector_id), None
            )
        )

        # Mock adapter
        adapter = Mock()
        adapter.predict.return_value = (
            np.random.choice([0, 1], size=5),  # predictions
            np.random.rand(5),  # scores
        )
        adapter_registry.get_adapter.return_value = adapter

        # Create ensemble request
        test_data = np.random.randn(5, 3)
        request = EnsembleDetectionRequest(
            detector_ids=[d.id for d in mock_detectors],
            data=test_data,
            voting_strategy=VotingStrategy.WEIGHTED_AVERAGE,
            enable_dynamic_weighting=True,
            enable_explanation=True,
        )

        print(f"üìä Created test request with {len(request.detector_ids)} detectors")
        print(f"   Voting strategy: {request.voting_strategy.value}")
        print(f"   Data shape: {test_data.shape}")

        # Execute ensemble detection
        response = await ensemble_use_case.detect_anomalies_ensemble(request)

        print("‚úÖ Ensemble detection completed")
        print(f"   Success: {response.success}")
        print(f"   Processing time: {response.processing_time:.3f}s")

        if response.success:
            print(f"   Predictions: {len(response.predictions)} items")
            print(f"   Anomaly scores: {len(response.anomaly_scores)} items")
            print(f"   Detector weights: {len(response.detector_weights)} weights")
            print(f"   Strategy used: {response.voting_strategy_used}")

            if response.explanations:
                print(f"   Explanations: {len(response.explanations)} explanations")
                explanation = response.explanations[0]
                print(
                    f"     Sample explanation keys: {list(explanation.keys())[:4]}..."
                )

        # Test ensemble optimization workflow
        print("\nüîß Testing Ensemble Optimization Workflow")
        print("-" * 42)

        # Mock dataset for optimization
        mock_dataset = Mock()
        mock_dataset.id = "validation_dataset"
        mock_dataset.data = pd.DataFrame(np.random.randn(100, 3))

        dataset_repo.get = AsyncMock(return_value=mock_dataset)

        # Create optimization request
        opt_request = EnsembleOptimizationRequest(
            detector_ids=[d.id for d in mock_detectors],
            validation_dataset_id="validation_dataset",
            optimization_objective=EnsembleOptimizationObjective.F1_SCORE,
            target_voting_strategies=[
                VotingStrategy.WEIGHTED_AVERAGE,
                VotingStrategy.DYNAMIC_SELECTION,
            ],
            max_ensemble_size=3,
        )

        print("üéØ Created optimization request")
        print(f"   Objective: {opt_request.optimization_objective.value}")
        print(
            f"   Target strategies: {[s.value for s in opt_request.target_voting_strategies]}"
        )
        print(f"   Max ensemble size: {opt_request.max_ensemble_size}")

        # Execute optimization
        opt_response = await ensemble_use_case.optimize_ensemble(opt_request)

        print("‚úÖ Ensemble optimization completed")
        print(f"   Success: {opt_response.success}")
        print(f"   Optimization time: {opt_response.optimization_time:.3f}s")

        if opt_response.success:
            print(f"   Optimized detectors: {len(opt_response.optimized_detector_ids)}")
            print(
                f"   Optimal strategy: {opt_response.optimal_voting_strategy.value if opt_response.optimal_voting_strategy else 'None'}"
            )
            print(f"   Recommendations: {len(opt_response.recommendations)}")

        # Test API endpoint imports
        print("\nüåê Testing API Endpoints")
        print("-" * 25)

        try:
            from pynomaly.presentation.api.endpoints.ensemble import router

            print(f"‚úÖ API router imported: {len(router.routes)} routes")

            # List available routes
            for route in router.routes[:3]:  # Show first 3
                print(f"   ‚Ä¢ {route.methods} {route.path}")
            if len(router.routes) > 3:
                print(f"   ... and {len(router.routes) - 3} more routes")

        except Exception as e:
            print(f"‚ö†Ô∏è  API endpoints import failed: {e}")

        # Test performance tracking
        print("\nüìä Testing Performance Tracking")
        print("-" * 32)

        print(f"Performance trackers: {len(ensemble_use_case._performance_tracker)}")
        print(f"Ensemble cache: {len(ensemble_use_case._ensemble_cache)}")
        print(f"Optimization history: {len(ensemble_use_case._optimization_history)}")

        # Show performance tracker details
        for detector_id, metrics in list(
            ensemble_use_case._performance_tracker.items()
        )[:2]:
            print(
                f"  {detector_id}: F1={metrics.f1_score:.3f}, Stability={metrics.stability_score:.3f}"
            )

        print("\nüéâ Ensemble Detection Integration Summary")
        print("=" * 45)
        print("‚úÖ Domain layer: VotingStrategy, EnsembleOptimizationObjective")
        print("‚úÖ Application layer: EnsembleDetectionUseCase, DTOs")
        print("‚úÖ Use cases: Ensemble detection, optimization workflows")
        print("‚úÖ API layer: RESTful endpoints with comprehensive validation")
        print("‚úÖ Advanced features:")
        print("   ‚Ä¢ 12 voting strategies with sophisticated algorithms")
        print("   ‚Ä¢ Dynamic weighting based on performance metrics")
        print("   ‚Ä¢ Uncertainty estimation and confidence scoring")
        print("   ‚Ä¢ Explanation generation for ensemble decisions")
        print("   ‚Ä¢ Performance tracking and optimization history")
        print("   ‚Ä¢ Caching system for improved performance")
        print("   ‚Ä¢ Comprehensive error handling and validation")

        print("\nüìà Key Capabilities:")
        print("   ‚Ä¢ 2-20 detectors per ensemble with automatic validation")
        print("   ‚Ä¢ 12 voting strategies from simple average to cascaded voting")
        print("   ‚Ä¢ 9 optimization objectives for different use cases")
        print("   ‚Ä¢ Real-time performance tracking and metrics")
        print("   ‚Ä¢ Ensemble optimization with cross-validation")
        print("   ‚Ä¢ RESTful API with comprehensive documentation")

        return True

    except Exception as e:
        print(f"‚ùå Error testing ensemble integration: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = asyncio.run(test_ensemble_integration())
    print(f"\n{'‚úÖ SUCCESS' if success else '‚ùå FAILED'}")
    sys.exit(0 if success else 1)
