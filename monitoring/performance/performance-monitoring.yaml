# Performance Monitoring Configuration
# Comprehensive performance monitoring setup for the anomaly detection platform

apiVersion: v1
kind: ConfigMap
metadata:
  name: performance-monitoring-config
  namespace: anomaly-detection
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      - "performance-rules.yml"
      - "alerting-rules.yml"
    
    alertmanager_configs:
      - static_configs:
          - targets:
            - alertmanager:9093
    
    scrape_configs:
      # Application metrics
      - job_name: 'anomaly-detection-api'
        static_configs:
          - targets: ['api:8000']
        scrape_interval: 5s
        metrics_path: /metrics
        
      - job_name: 'anomaly-detection-worker'
        static_configs:
          - targets: ['worker:8001']
        scrape_interval: 10s
        
      # Infrastructure metrics
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
            
      - job_name: 'kubernetes-nodes'
        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - action: labelmap
            regex: __meta_kubernetes_node_label_(.+)
            
      # Database performance
      - job_name: 'postgresql'
        static_configs:
          - targets: ['postgres-exporter:9187']
        scrape_interval: 30s
        
      - job_name: 'redis'
        static_configs:
          - targets: ['redis-exporter:9121']
        scrape_interval: 30s

  performance-rules.yml: |
    groups:
      - name: performance.rules
        interval: 30s
        rules:
          # API Performance Metrics
          - record: api:request_duration_seconds:rate5m
            expr: rate(http_request_duration_seconds_sum[5m]) / rate(http_request_duration_seconds_count[5m])
            
          - record: api:request_rate:rate5m
            expr: rate(http_requests_total[5m])
            
          - record: api:error_rate:rate5m
            expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])
            
          # System Performance Metrics
          - record: system:cpu_usage:rate5m
            expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
            
          - record: system:memory_usage:percentage
            expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100
            
          - record: system:disk_usage:percentage
            expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100
            
          # Database Performance
          - record: database:connections:active
            expr: pg_stat_database_numbackends
            
          - record: database:query_time:avg5m
            expr: rate(pg_stat_database_blk_read_time[5m]) / rate(pg_stat_database_blks_read[5m])
            
          # Application-specific Performance
          - record: anomaly_detection:processing_time:avg5m
            expr: rate(anomaly_processing_duration_seconds_sum[5m]) / rate(anomaly_processing_duration_seconds_count[5m])
            
          - record: anomaly_detection:throughput:rate5m
            expr: rate(anomaly_detections_total[5m])

  alerting-rules.yml: |
    groups:
      - name: performance.alerts
        rules:
          # High-priority performance alerts
          - alert: HighAPIResponseTime
            expr: api:request_duration_seconds:rate5m > 2.0
            for: 2m
            labels:
              severity: critical
              team: platform
            annotations:
              summary: "High API response time detected"
              description: "API response time is {{ $value }}s, which is above the 2s threshold"
              runbook_url: "https://docs.company.com/runbooks/high-api-response-time"
              
          - alert: HighErrorRate
            expr: api:error_rate:rate5m > 0.05
            for: 1m
            labels:
              severity: critical
              team: platform
            annotations:
              summary: "High error rate detected"
              description: "Error rate is {{ $value | humanizePercentage }}, which is above 5%"
              
          - alert: HighCPUUsage
            expr: system:cpu_usage:rate5m > 80
            for: 5m
            labels:
              severity: warning
              team: infrastructure
            annotations:
              summary: "High CPU usage detected"
              description: "CPU usage is {{ $value }}%, which is above 80%"
              
          - alert: HighMemoryUsage
            expr: system:memory_usage:percentage > 85
            for: 5m
            labels:
              severity: warning
              team: infrastructure
            annotations:
              summary: "High memory usage detected"
              description: "Memory usage is {{ $value }}%, which is above 85%"
              
          - alert: DatabaseConnectionPoolExhaustion
            expr: database:connections:active > 80
            for: 2m
            labels:
              severity: critical
              team: database
            annotations:
              summary: "Database connection pool near exhaustion"
              description: "Active connections: {{ $value }}, approaching connection limit"
              
          - alert: SlowAnomalyProcessing
            expr: anomaly_detection:processing_time:avg5m > 10.0
            for: 3m
            labels:
              severity: warning
              team: ml
            annotations:
              summary: "Slow anomaly processing detected"
              description: "Average processing time is {{ $value }}s, which is above 10s threshold"
              
          # Service availability alerts
          - alert: ServiceDown
            expr: up == 0
            for: 1m
            labels:
              severity: critical
              team: platform
            annotations:
              summary: "Service {{ $labels.job }} is down"
              description: "Service {{ $labels.job }} on {{ $labels.instance }} has been down for more than 1 minute"
              
          - alert: LowThroughput
            expr: anomaly_detection:throughput:rate5m < 1
            for: 10m
            labels:
              severity: warning
              team: ml
            annotations:
              summary: "Low anomaly detection throughput"
              description: "Processing only {{ $value }} detections per second, which is below expected threshold"

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: anomaly-detection
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'localhost:587'
      smtp_from: 'alerts@anomaly-detection.io'
      
    route:
      group_by: ['alertname', 'cluster', 'service']
      group_wait: 10s
      group_interval: 10s
      repeat_interval: 12h
      receiver: 'default'
      routes:
        - match:
            severity: critical
          receiver: 'critical-alerts'
          group_wait: 5s
          
        - match:
            team: infrastructure
          receiver: 'infrastructure-team'
          
        - match:
            team: database
          receiver: 'database-team'
          
        - match:
            team: ml
          receiver: 'ml-team'
    
    receivers:
      - name: 'default'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#alerts'
            title: 'Anomaly Detection Alert'
            text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
            
      - name: 'critical-alerts'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#critical-alerts'
            title: 'ðŸš¨ CRITICAL: Anomaly Detection'
            text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}{{ end }}'
        email_configs:
          - to: 'oncall@company.com'
            subject: 'ðŸš¨ CRITICAL: {{ .GroupLabels.alertname }}'
            body: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
            
      - name: 'infrastructure-team'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#infrastructure-alerts'
            title: 'Infrastructure Alert'
            text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
            
      - name: 'database-team'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#database-alerts'
            title: 'Database Alert'
            text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
            
      - name: 'ml-team'
        slack_configs:
          - api_url: '${SLACK_WEBHOOK_URL}'
            channel: '#ml-alerts'
            title: 'ML Pipeline Alert'
            text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
    
    inhibit_rules:
      - source_match:
          severity: 'critical'
        target_match:
          severity: 'warning'
        equal: ['alertname', 'cluster', 'service']