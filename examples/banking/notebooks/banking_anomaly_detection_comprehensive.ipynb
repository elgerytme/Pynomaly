{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banking Data Anomaly Detection - Comprehensive Analysis\n",
    "\n",
    "This notebook demonstrates anomaly detection across various banking transaction types to identify:\n",
    "- Fraudulent transactions\n",
    "- Money laundering patterns\n",
    "- Data quality issues\n",
    "- Regulatory compliance violations\n",
    "- Operational anomalies\n",
    "\n",
    "## Table of Contents\n",
    "1. [Environment Setup](#Environment-Setup)\n",
    "2. [Data Loading and Exploration](#Data-Loading-and-Exploration)\n",
    "3. [Deposit Transaction Analysis](#Deposit-Transaction-Analysis)\n",
    "4. [Credit Card Fraud Detection](#Credit-Card-Fraud-Detection)\n",
    "5. [ATM Transaction Monitoring](#ATM-Transaction-Monitoring)\n",
    "6. [Foreign Exchange Analysis](#Foreign-Exchange-Analysis)\n",
    "7. [Investment Transaction Surveillance](#Investment-Transaction-Surveillance)\n",
    "8. [Corporate Expense Audit](#Corporate-Expense-Audit)\n",
    "9. [General Ledger Validation](#General-Ledger-Validation)\n",
    "10. [Cross-Transaction Analysis](#Cross-Transaction-Analysis)\n",
    "11. [Executive Dashboard](#Executive-Dashboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import seaborn as sns\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Pynomaly imports\n",
    "sys.path.append(\"../../../src\")\n",
    "from pynomaly.domain.entities.dataset import Dataset\n",
    "from pynomaly.domain.value_objects.contamination_rate import ContaminationRate\n",
    "from pynomaly.infrastructure.adapters.pyod_adapter import PyODAdapter\n",
    "from pynomaly.infrastructure.adapters.sklearn_adapter import SklearnAdapter\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", None)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all banking datasets\n",
    "datasets = {}\n",
    "data_dir = \"../datasets/\"\n",
    "\n",
    "dataset_files = [\n",
    "    \"deposits.csv\",\n",
    "    \"loans.csv\",\n",
    "    \"investments.csv\",\n",
    "    \"fx_transactions.csv\",\n",
    "    \"atm_transactions.csv\",\n",
    "    \"debit_card_transactions.csv\",\n",
    "    \"credit_card_transactions.csv\",\n",
    "    \"expense_transactions.csv\",\n",
    "    \"gl_transactions.csv\",\n",
    "]\n",
    "\n",
    "for file in dataset_files:\n",
    "    name = file.replace(\".csv\", \"\")\n",
    "    datasets[name] = pd.read_csv(os.path.join(data_dir, file))\n",
    "    print(f\"Loaded {name}: {len(datasets[name]):,} records\")\n",
    "\n",
    "print(\n",
    "    f\"\\nTotal records across all datasets: {sum(len(df) for df in datasets.values()):,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "overview_data = []\n",
    "for name, df in datasets.items():\n",
    "    overview_data.append(\n",
    "        {\n",
    "            \"Dataset\": name.replace(\"_\", \" \").title(),\n",
    "            \"Records\": len(df),\n",
    "            \"Anomalies\": df[\"is_anomaly\"].sum() if \"is_anomaly\" in df.columns else 0,\n",
    "            \"Anomaly Rate\": f\"{df['is_anomaly'].mean() * 100:.1f}%\"\n",
    "            if \"is_anomaly\" in df.columns\n",
    "            else \"N/A\",\n",
    "            \"Date Range\": f\"{df['timestamp'].min()} to {df['timestamp'].max()}\"\n",
    "            if \"timestamp\" in df.columns\n",
    "            else \"N/A\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "overview_df = pd.DataFrame(overview_data)\n",
    "print(\"Banking Datasets Overview:\")\n",
    "print(overview_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset sizes and anomaly rates\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Dataset sizes\n",
    "sizes = [len(df) for df in datasets.values()]\n",
    "names = [name.replace(\"_\", \" \").title() for name in datasets.keys()]\n",
    "ax1.bar(names, sizes, color=\"skyblue\")\n",
    "ax1.set_title(\"Dataset Sizes\")\n",
    "ax1.set_ylabel(\"Number of Records\")\n",
    "ax1.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# Anomaly rates\n",
    "anomaly_rates = [\n",
    "    df[\"is_anomaly\"].mean() * 100 if \"is_anomaly\" in df.columns else 0\n",
    "    for df in datasets.values()\n",
    "]\n",
    "ax2.bar(names, anomaly_rates, color=\"lightcoral\")\n",
    "ax2.set_title(\"Anomaly Rates by Dataset\")\n",
    "ax2.set_ylabel(\"Anomaly Rate (%)\")\n",
    "ax2.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deposit Transaction Analysis\n",
    "\n",
    "Analyzing deposit transactions for:\n",
    "- Money laundering patterns\n",
    "- Structuring (transactions just under $10,000)\n",
    "- Unusual timing patterns\n",
    "- Velocity anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load deposits data and convert timestamp\n",
    "deposits = datasets[\"deposits\"].copy()\n",
    "deposits[\"timestamp\"] = pd.to_datetime(deposits[\"timestamp\"])\n",
    "\n",
    "# Basic statistics\n",
    "print(\"Deposit Transaction Statistics:\")\n",
    "print(f\"Total deposits: {len(deposits):,}\")\n",
    "print(f\"Total amount: ${deposits['amount'].sum():,.2f}\")\n",
    "print(f\"Average deposit: ${deposits['amount'].mean():,.2f}\")\n",
    "print(f\"Median deposit: ${deposits['amount'].median():,.2f}\")\n",
    "print(f\"Largest deposit: ${deposits['amount'].max():,.2f}\")\n",
    "print(\n",
    "    f\"Structuring indicators (near $10K): {((deposits['amount'] >= 9000) & (deposits['amount'] < 10000)).sum()}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering for deposit analysis\n",
    "def engineer_deposit_features(df):\n",
    "    features_df = df.copy()\n",
    "\n",
    "    # Time features\n",
    "    features_df[\"hour\"] = features_df[\"timestamp\"].dt.hour\n",
    "    features_df[\"day_of_week\"] = features_df[\"timestamp\"].dt.dayofweek\n",
    "    features_df[\"is_weekend\"] = features_df[\"day_of_week\"].isin([5, 6]).astype(int)\n",
    "    features_df[\"is_business_hours\"] = (\n",
    "        (features_df[\"hour\"] >= 9) & (features_df[\"hour\"] <= 17)\n",
    "    ).astype(int)\n",
    "\n",
    "    # Amount features\n",
    "    features_df[\"amount_log\"] = np.log1p(features_df[\"amount\"])\n",
    "    features_df[\"near_threshold\"] = (\n",
    "        (features_df[\"amount\"] >= 9000) & (features_df[\"amount\"] < 10000)\n",
    "    ).astype(int)\n",
    "    features_df[\"large_amount\"] = (features_df[\"amount\"] > 50000).astype(int)\n",
    "\n",
    "    # Source type encoding\n",
    "    source_encoded = pd.get_dummies(features_df[\"source_type\"], prefix=\"source\")\n",
    "    features_df = pd.concat([features_df, source_encoded], axis=1)\n",
    "\n",
    "    return features_df\n",
    "\n",
    "\n",
    "deposits_features = engineer_deposit_features(deposits)\n",
    "print(\"Feature engineering complete for deposits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection on deposits\n",
    "def detect_deposit_anomalies(df, contamination=0.05):\n",
    "    feature_columns = [\n",
    "        \"amount\",\n",
    "        \"amount_log\",\n",
    "        \"hour\",\n",
    "        \"day_of_week\",\n",
    "        \"is_weekend\",\n",
    "        \"is_business_hours\",\n",
    "        \"near_threshold\",\n",
    "        \"large_amount\",\n",
    "    ]\n",
    "\n",
    "    # Add source type columns\n",
    "    source_columns = [col for col in df.columns if col.startswith(\"source_\")]\n",
    "    feature_columns.extend(source_columns)\n",
    "\n",
    "    X = df[feature_columns].fillna(0).values\n",
    "\n",
    "    # Initialize adapters\n",
    "    sklearn_adapter = SklearnAdapter()\n",
    "    pyod_adapter = PyODAdapter()\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = Dataset(data=X, target=None, feature_names=feature_columns)\n",
    "\n",
    "    contamination_rate = ContaminationRate(contamination)\n",
    "\n",
    "    # Run multiple algorithms\n",
    "    iso_result = sklearn_adapter.detect_anomalies(\n",
    "        dataset=dataset,\n",
    "        algorithm_type=\"isolation_forest\",\n",
    "        contamination=contamination_rate,\n",
    "    )\n",
    "\n",
    "    lof_result = pyod_adapter.detect_anomalies(\n",
    "        dataset=dataset, algorithm_type=\"lof\", contamination=contamination_rate\n",
    "    )\n",
    "\n",
    "    # Combine scores\n",
    "    df_result = df.copy()\n",
    "    df_result[\"iso_score\"] = iso_result.anomaly_scores\n",
    "    df_result[\"lof_score\"] = lof_result.anomaly_scores\n",
    "    df_result[\"ensemble_score\"] = (df_result[\"iso_score\"] + df_result[\"lof_score\"]) / 2\n",
    "\n",
    "    # Flag anomalies\n",
    "    threshold = np.percentile(df_result[\"ensemble_score\"], (1 - contamination) * 100)\n",
    "    df_result[\"predicted_anomaly\"] = (df_result[\"ensemble_score\"] > threshold).astype(\n",
    "        int\n",
    "    )\n",
    "\n",
    "    return df_result\n",
    "\n",
    "\n",
    "deposit_results = detect_deposit_anomalies(deposits_features)\n",
    "print(\n",
    "    f\"Deposit anomaly detection complete. Detected {deposit_results['predicted_anomaly'].sum()} anomalies.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze deposit anomalies\n",
    "deposit_anomalies = deposit_results[deposit_results[\"predicted_anomaly\"] == 1]\n",
    "deposit_normal = deposit_results[deposit_results[\"predicted_anomaly\"] == 0]\n",
    "\n",
    "print(\"=== DEPOSIT ANOMALY ANALYSIS ===\")\n",
    "print(f\"Normal transactions: {len(deposit_normal):,}\")\n",
    "print(f\"Anomalous transactions: {len(deposit_anomalies):,}\")\n",
    "print(f\"Detection rate: {len(deposit_anomalies) / len(deposit_results) * 100:.1f}%\")\n",
    "\n",
    "if len(deposit_anomalies) > 0:\n",
    "    print(\"\\nAnomaly Characteristics:\")\n",
    "    print(\n",
    "        f\"Average amount: ${deposit_anomalies['amount'].mean():,.2f} vs ${deposit_normal['amount'].mean():,.2f}\"\n",
    "    )\n",
    "    print(f\"Structuring indicators: {deposit_anomalies['near_threshold'].sum()}\")\n",
    "    print(f\"Large deposits (>$50K): {deposit_anomalies['large_amount'].sum()}\")\n",
    "    print(f\"After-hours deposits: {(1 - deposit_anomalies['is_business_hours']).sum()}\")\n",
    "    print(f\"Weekend deposits: {deposit_anomalies['is_weekend'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize deposit anomalies\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle(\"Deposit Transaction Anomaly Analysis\", fontsize=16)\n",
    "\n",
    "# Amount distribution\n",
    "axes[0, 0].hist(\n",
    "    deposit_normal[\"amount\"], bins=50, alpha=0.7, label=\"Normal\", density=True\n",
    ")\n",
    "axes[0, 0].hist(\n",
    "    deposit_anomalies[\"amount\"], bins=50, alpha=0.7, label=\"Anomaly\", density=True\n",
    ")\n",
    "axes[0, 0].set_xlabel(\"Amount ($)\")\n",
    "axes[0, 0].set_ylabel(\"Density\")\n",
    "axes[0, 0].set_title(\"Amount Distribution\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_xscale(\"log\")\n",
    "\n",
    "# Time patterns\n",
    "hour_counts = (\n",
    "    deposit_results.groupby([\"hour\", \"predicted_anomaly\"]).size().unstack(fill_value=0)\n",
    ")\n",
    "hour_counts.plot(kind=\"bar\", ax=axes[0, 1], color=[\"blue\", \"red\"])\n",
    "axes[0, 1].set_xlabel(\"Hour of Day\")\n",
    "axes[0, 1].set_ylabel(\"Count\")\n",
    "axes[0, 1].set_title(\"Hourly Distribution\")\n",
    "axes[0, 1].legend([\"Normal\", \"Anomaly\"])\n",
    "\n",
    "# Source type distribution\n",
    "source_data = (\n",
    "    deposit_results.groupby([\"source_type\", \"predicted_anomaly\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    ")\n",
    "source_data.plot(kind=\"bar\", ax=axes[1, 0], color=[\"blue\", \"red\"])\n",
    "axes[1, 0].set_xlabel(\"Source Type\")\n",
    "axes[1, 0].set_ylabel(\"Count\")\n",
    "axes[1, 0].set_title(\"Source Type Distribution\")\n",
    "axes[1, 0].legend([\"Normal\", \"Anomaly\"])\n",
    "\n",
    "# Anomaly scores\n",
    "axes[1, 1].scatter(\n",
    "    deposit_results[\"iso_score\"],\n",
    "    deposit_results[\"lof_score\"],\n",
    "    c=deposit_results[\"predicted_anomaly\"],\n",
    "    cmap=\"coolwarm\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "axes[1, 1].set_xlabel(\"Isolation Forest Score\")\n",
    "axes[1, 1].set_ylabel(\"LOF Score\")\n",
    "axes[1, 1].set_title(\"Algorithm Score Comparison\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credit Card Fraud Detection\n",
    "\n",
    "Detecting fraudulent credit card transactions using:\n",
    "- Velocity analysis\n",
    "- Amount deviation patterns\n",
    "- Geographic anomalies\n",
    "- Merchant category analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credit card data\n",
    "credit_cards = datasets[\"credit_card_transactions\"].copy()\n",
    "credit_cards[\"timestamp\"] = pd.to_datetime(credit_cards[\"timestamp\"])\n",
    "\n",
    "print(\"Credit Card Transaction Statistics:\")\n",
    "print(f\"Total transactions: {len(credit_cards):,}\")\n",
    "print(f\"Total amount: ${credit_cards['amount'].sum():,.2f}\")\n",
    "print(f\"Average transaction: ${credit_cards['amount'].mean():,.2f}\")\n",
    "print(f\"Unique customers: {credit_cards['customer_id'].nunique():,}\")\n",
    "print(f\"Unique merchants: {credit_cards['merchant'].nunique():,}\")\n",
    "print(f\"Card-not-present transactions: {(~credit_cards['card_present']).sum():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit card fraud detection\n",
    "def detect_credit_fraud(df, contamination=0.05):\n",
    "    features_df = df.copy()\n",
    "\n",
    "    # Time features\n",
    "    features_df[\"hour\"] = features_df[\"timestamp\"].dt.hour\n",
    "    features_df[\"is_night\"] = (\n",
    "        (features_df[\"hour\"] < 6) | (features_df[\"hour\"] > 22)\n",
    "    ).astype(int)\n",
    "    features_df[\"is_weekend\"] = (\n",
    "        features_df[\"timestamp\"].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    )\n",
    "\n",
    "    # Amount features\n",
    "    features_df[\"amount_log\"] = np.log1p(features_df[\"amount\"])\n",
    "    features_df[\"high_amount\"] = (features_df[\"amount\"] > 1000).astype(int)\n",
    "\n",
    "    # Card features\n",
    "    features_df[\"card_present_int\"] = features_df[\"card_present\"].astype(int)\n",
    "\n",
    "    # Customer behavior\n",
    "    customer_stats = (\n",
    "        features_df.groupby(\"customer_id\")[\"amount\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "    )\n",
    "    customer_stats.columns = [\"customer_id\", \"avg_amount\", \"std_amount\"]\n",
    "    customer_stats[\"std_amount\"] = customer_stats[\"std_amount\"].fillna(0)\n",
    "    features_df = features_df.merge(customer_stats, on=\"customer_id\")\n",
    "\n",
    "    # Deviation from normal\n",
    "    features_df[\"amount_dev\"] = abs(\n",
    "        features_df[\"amount\"] - features_df[\"avg_amount\"]\n",
    "    ) / (features_df[\"std_amount\"] + 1)\n",
    "\n",
    "    # Encode categoricals\n",
    "    category_encoded = pd.get_dummies(features_df[\"category\"], prefix=\"cat\")\n",
    "    location_encoded = pd.get_dummies(features_df[\"location\"], prefix=\"loc\")\n",
    "    features_df = pd.concat([features_df, category_encoded, location_encoded], axis=1)\n",
    "\n",
    "    # Select features\n",
    "    feature_columns = [\n",
    "        \"amount\",\n",
    "        \"amount_log\",\n",
    "        \"high_amount\",\n",
    "        \"hour\",\n",
    "        \"is_night\",\n",
    "        \"is_weekend\",\n",
    "        \"card_present_int\",\n",
    "        \"mcc_code\",\n",
    "        \"amount_dev\",\n",
    "    ]\n",
    "\n",
    "    # Add encoded features\n",
    "    cat_cols = [col for col in features_df.columns if col.startswith(\"cat_\")]\n",
    "    loc_cols = [col for col in features_df.columns if col.startswith(\"loc_\")]\n",
    "    feature_columns.extend(cat_cols + loc_cols)\n",
    "\n",
    "    X = features_df[feature_columns].fillna(0).values\n",
    "\n",
    "    # Run anomaly detection\n",
    "    sklearn_adapter = SklearnAdapter()\n",
    "    dataset = Dataset(data=X, target=None, feature_names=feature_columns)\n",
    "    contamination_rate = ContaminationRate(contamination)\n",
    "\n",
    "    result = sklearn_adapter.detect_anomalies(\n",
    "        dataset=dataset,\n",
    "        algorithm_type=\"isolation_forest\",\n",
    "        contamination=contamination_rate,\n",
    "    )\n",
    "\n",
    "    features_df[\"fraud_score\"] = result.anomaly_scores\n",
    "    threshold = np.percentile(features_df[\"fraud_score\"], (1 - contamination) * 100)\n",
    "    features_df[\"predicted_fraud\"] = (features_df[\"fraud_score\"] > threshold).astype(\n",
    "        int\n",
    "    )\n",
    "\n",
    "    return features_df\n",
    "\n",
    "\n",
    "credit_results = detect_credit_fraud(credit_cards)\n",
    "print(\n",
    "    f\"Credit card fraud detection complete. Detected {credit_results['predicted_fraud'].sum()} fraudulent transactions.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze credit card fraud\n",
    "fraud_txns = credit_results[credit_results[\"predicted_fraud\"] == 1]\n",
    "normal_txns = credit_results[credit_results[\"predicted_fraud\"] == 0]\n",
    "\n",
    "print(\"=== CREDIT CARD FRAUD ANALYSIS ===\")\n",
    "print(\n",
    "    f\"Total fraud detected: {len(fraud_txns):,} ({len(fraud_txns) / len(credit_results) * 100:.1f}%)\"\n",
    ")\n",
    "print(f\"Fraud amount: ${fraud_txns['amount'].sum():,.2f}\")\n",
    "print(f\"Average fraud amount: ${fraud_txns['amount'].mean():,.2f}\")\n",
    "\n",
    "if len(fraud_txns) > 0:\n",
    "    print(\"\\nFraud Patterns:\")\n",
    "    print(f\"High-value fraud (>$1000): {fraud_txns['high_amount'].sum()}\")\n",
    "    print(f\"Card-not-present fraud: {(1 - fraud_txns['card_present_int']).sum()}\")\n",
    "    print(f\"Night-time fraud: {fraud_txns['is_night'].sum()}\")\n",
    "    print(f\"Weekend fraud: {fraud_txns['is_weekend'].sum()}\")\n",
    "\n",
    "    if \"loc_international\" in fraud_txns.columns:\n",
    "        print(f\"International fraud: {fraud_txns['loc_international'].sum()}\")\n",
    "\n",
    "    print(\"\\nTop 5 Fraud Transactions:\")\n",
    "    top_fraud = fraud_txns.nlargest(5, \"fraud_score\")[\n",
    "        [\"transaction_id\", \"amount\", \"merchant\", \"fraud_score\"]\n",
    "    ]\n",
    "    print(top_fraud.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATM Transaction Monitoring\n",
    "\n",
    "Detecting ATM fraud and skimming attacks through:\n",
    "- Multiple failed attempts\n",
    "- Geographic anomalies\n",
    "- Timing patterns\n",
    "- Amount patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATM analysis\n",
    "atm_txns = datasets[\"atm_transactions\"].copy()\n",
    "atm_txns[\"timestamp\"] = pd.to_datetime(atm_txns[\"timestamp\"])\n",
    "\n",
    "print(\"ATM Transaction Statistics:\")\n",
    "print(f\"Total ATM transactions: {len(atm_txns):,}\")\n",
    "print(f\"Successful transactions: {(atm_txns['status'] == 'success').sum():,}\")\n",
    "print(f\"Declined transactions: {(atm_txns['status'] == 'declined').sum():,}\")\n",
    "print(f\"Error transactions: {(atm_txns['status'] == 'error').sum():,}\")\n",
    "print(f\"Unique ATMs: {atm_txns['atm_id'].nunique():,}\")\n",
    "print(f\"Unique locations: {atm_txns['location'].nunique():,}\")\n",
    "\n",
    "# Withdrawal analysis\n",
    "withdrawals = atm_txns[atm_txns[\"transaction_type\"] == \"withdrawal\"]\n",
    "print(\"\\nWithdrawal Analysis:\")\n",
    "print(f\"Total withdrawals: {len(withdrawals):,}\")\n",
    "if len(withdrawals) > 0:\n",
    "    print(f\"Total withdrawn: ${withdrawals['amount'].sum():,.2f}\")\n",
    "    print(f\"Average withdrawal: ${withdrawals['amount'].mean():.2f}\")\n",
    "    print(f\"Most common amounts: {withdrawals['amount'].value_counts().head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Dashboard\n",
    "\n",
    "Summary of all anomaly detection results across banking operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create executive summary\n",
    "def create_executive_summary():\n",
    "    summary = {\n",
    "        \"Metric\": [\n",
    "            \"Total Transactions Analyzed\",\n",
    "            \"Deposits - Total Anomalies\",\n",
    "            \"Credit Cards - Fraud Detected\",\n",
    "            \"ATM - Suspicious Activity\",\n",
    "            \"Investment - Unusual Patterns\",\n",
    "            \"FX - Money Laundering Risk\",\n",
    "            \"Expenses - Potential Fraud\",\n",
    "            \"GL - Data Quality Issues\",\n",
    "        ],\n",
    "        \"Count\": [\n",
    "            sum(len(df) for df in datasets.values()),\n",
    "            deposit_results[\"predicted_anomaly\"].sum(),\n",
    "            credit_results[\"predicted_fraud\"].sum(),\n",
    "            \"TBD\",  # ATM analysis not fully implemented\n",
    "            \"TBD\",  # Investment analysis not fully implemented\n",
    "            \"TBD\",  # FX analysis not fully implemented\n",
    "            \"TBD\",  # Expense analysis not fully implemented\n",
    "            \"TBD\",  # GL analysis not fully implemented\n",
    "        ],\n",
    "        \"Risk Level\": [\n",
    "            \"INFO\",\n",
    "            \"MEDIUM\" if deposit_results[\"predicted_anomaly\"].sum() > 100 else \"LOW\",\n",
    "            \"HIGH\" if credit_results[\"predicted_fraud\"].sum() > 500 else \"MEDIUM\",\n",
    "            \"TBD\",\n",
    "            \"TBD\",\n",
    "            \"TBD\",\n",
    "            \"TBD\",\n",
    "            \"TBD\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "\n",
    "executive_summary = create_executive_summary()\n",
    "print(\"=== EXECUTIVE SUMMARY ===\")\n",
    "print(executive_summary.to_string(index=False))\n",
    "\n",
    "# Risk assessment\n",
    "total_deposit_risk = (\n",
    "    deposit_anomalies[\"amount\"].sum() if len(deposit_anomalies) > 0 else 0\n",
    ")\n",
    "total_fraud_risk = fraud_txns[\"amount\"].sum() if len(fraud_txns) > 0 else 0\n",
    "\n",
    "print(\"\\n=== FINANCIAL RISK ASSESSMENT ===\")\n",
    "print(f\"Suspicious deposit amounts: ${total_deposit_risk:,.2f}\")\n",
    "print(f\"Fraudulent transaction amounts: ${total_fraud_risk:,.2f}\")\n",
    "print(f\"Total risk exposure: ${total_deposit_risk + total_fraud_risk:,.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive dashboard\n",
    "fig = make_subplots(\n",
    "    rows=2,\n",
    "    cols=2,\n",
    "    subplot_titles=[\n",
    "        \"Dataset Overview\",\n",
    "        \"Deposit Anomalies\",\n",
    "        \"Credit Card Fraud\",\n",
    "        \"Risk Summary\",\n",
    "    ],\n",
    "    specs=[\n",
    "        [{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
    "        [{\"type\": \"scatter\"}, {\"type\": \"bar\"}],\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Dataset sizes\n",
    "dataset_names = list(datasets.keys())\n",
    "dataset_sizes = [len(df) for df in datasets.values()]\n",
    "\n",
    "fig.add_trace(go.Bar(x=dataset_names, y=dataset_sizes, name=\"Records\"), row=1, col=1)\n",
    "\n",
    "# Deposit amount vs anomaly score\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=deposit_results[\"amount\"],\n",
    "        y=deposit_results[\"ensemble_score\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=deposit_results[\"predicted_anomaly\"], colorscale=\"Viridis\"),\n",
    "        name=\"Deposits\",\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2,\n",
    ")\n",
    "\n",
    "# Credit card amount vs fraud score\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=credit_results[\"amount\"],\n",
    "        y=credit_results[\"fraud_score\"],\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=credit_results[\"predicted_fraud\"], colorscale=\"Reds\"),\n",
    "        name=\"Credit Cards\",\n",
    "    ),\n",
    "    row=2,\n",
    "    col=1,\n",
    ")\n",
    "\n",
    "# Risk summary\n",
    "risk_categories = [\"Deposits\", \"Credit Cards\", \"Other\"]\n",
    "risk_amounts = [total_deposit_risk, total_fraud_risk, 0]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=risk_categories, y=risk_amounts, name=\"Risk Amount\"), row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800, showlegend=False, title_text=\"Banking Anomaly Detection Dashboard\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations and Next Steps\n",
    "\n",
    "Based on the anomaly detection analysis, here are the key recommendations:\n",
    "\n",
    "### Immediate Actions\n",
    "1. **Review high-risk transactions** identified by the ensemble models\n",
    "2. **Implement real-time monitoring** for the identified patterns\n",
    "3. **Enhance customer verification** for transactions flagged as anomalous\n",
    "\n",
    "### System Improvements\n",
    "1. **Deploy automated alerts** for transactions exceeding risk thresholds\n",
    "2. **Integrate multiple detection algorithms** for better coverage\n",
    "3. **Establish feedback loops** to improve model accuracy\n",
    "\n",
    "### Compliance and Reporting\n",
    "1. **Generate regulatory reports** for suspicious activity\n",
    "2. **Document investigation procedures** for anomalous transactions\n",
    "3. **Regular model validation** and performance monitoring\n",
    "\n",
    "### Future Enhancements\n",
    "1. **Cross-transaction analysis** to identify complex fraud schemes\n",
    "2. **Real-time streaming analytics** for immediate detection\n",
    "3. **Advanced ML models** including deep learning and graph analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}