# Kubernetes ConfigMaps for Pynomaly application configuration
# Application settings, monitoring configuration, and service configuration

apiVersion: v1
kind: ConfigMap
metadata:
  name: pynomaly-config
  namespace: pynomaly
  labels:
    app.kubernetes.io/name: pynomaly
    app.kubernetes.io/component: config
    app.kubernetes.io/instance: production
data:
  # Application configuration
  PYNOMALY_ENVIRONMENT: "production"
  PYNOMALY_DEBUG: "false"
  PYNOMALY_API_TITLE: "Pynomaly Production API"
  PYNOMALY_API_VERSION: "1.0.0"
  PYNOMALY_CORS_ORIGINS: '["https://pynomaly.local","https://grafana.pynomaly.local"]'
  PYNOMALY_ALLOWED_HOSTS: '["pynomaly.local","localhost"]'
  PYNOMALY_TRUSTED_PROXIES: '["127.0.0.1","10.0.0.0/8","172.16.0.0/12","192.168.0.0/16"]'
  
  # Feature flags
  FEATURE_WEB_UI: "true"
  FEATURE_STREAMING: "true"
  FEATURE_EXPLAINABILITY: "true"
  FEATURE_AUTOML: "true"
  FEATURE_ENSEMBLE: "true"
  FEATURE_DRIFT_DETECTION: "true"
  FEATURE_MONITORING: "true"
  
  # Performance configuration
  API_WORKERS: "4"
  API_WORKER_CLASS: "uvicorn.workers.UvicornWorker"
  API_MAX_CONNECTIONS: "1000"
  API_BACKLOG: "2048"
  
  # Rate limiting
  RATE_LIMIT_ENABLED: "true"
  RATE_LIMIT_REQUESTS_PER_MINUTE: "60"
  RATE_LIMIT_BURST: "20"
  
  # Machine learning configuration
  ML_MODEL_CACHE_SIZE: "100"
  ML_TRAINING_TIMEOUT: "3600"
  ML_PREDICTION_TIMEOUT: "30"
  ML_ENSEMBLE_SIZE: "5"
  
  # AutoML configuration
  AUTOML_ENABLED: "true"
  AUTOML_MAX_TRIALS: "50"
  AUTOML_TIMEOUT: "7200"
  
  # Drift detection configuration
  DRIFT_DETECTION_ENABLED: "true"
  DRIFT_CHECK_INTERVAL: "3600"
  DRIFT_ALERT_THRESHOLD: "0.1"
  
  # Logging configuration
  LOG_LEVEL: "INFO"
  LOG_FORMAT: "json"
  LOG_FILE_MAX_SIZE: "10MB"
  LOG_FILE_BACKUP_COUNT: "5"
  
  # Cache configuration
  REDIS_CACHE_TTL: "3600"
  REDIS_MAX_CONNECTIONS: "100"
  
  # File upload limits
  PYNOMALY_MAX_FILE_SIZE: "100MB"
  PYNOMALY_MAX_DATASET_SIZE: "1GB"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: pynomaly-worker-config
  namespace: pynomaly
  labels:
    app.kubernetes.io/name: pynomaly
    app.kubernetes.io/component: worker-config
    app.kubernetes.io/instance: production
data:
  # Celery configuration
  CELERY_TASK_SERIALIZER: "json"
  CELERY_RESULT_SERIALIZER: "json"
  CELERY_ACCEPT_CONTENT: '["json"]'
  CELERY_TIMEZONE: "UTC"
  CELERY_ENABLE_UTC: "true"
  
  # Worker-specific configuration
  TRAINING_WORKER_CONCURRENCY: "4"
  DRIFT_WORKER_CONCURRENCY: "2"
  GENERAL_WORKER_CONCURRENCY: "2"
  
  # Task timeouts
  TASK_SOFT_TIME_LIMIT: "3600"
  TASK_TIME_LIMIT: "7200"
  TASK_REJECT_ON_WORKER_LOST: "true"
  
  # Performance configuration
  WORKER_PREFETCH_MULTIPLIER: "4"
  WORKER_MAX_TASKS_PER_CHILD: "1000"
  WORKER_DISABLE_RATE_LIMITS: "false"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: pynomaly-prometheus-config
  namespace: pynomaly
  labels:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/component: config
    app.kubernetes.io/instance: production
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
      external_labels:
        cluster: 'pynomaly-production'
        environment: 'production'

    rule_files:
      - "alert_rules.yml"

    alerting:
      alertmanagers:
        - static_configs:
            - targets:
              - pynomaly-alertmanager-service:80

    scrape_configs:
      # Prometheus self-monitoring
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']
        metrics_path: /metrics
        scrape_interval: 15s

      # Pynomaly API application metrics
      - job_name: 'pynomaly-api'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - pynomaly
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: pynomaly-api-service
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: metrics
          - source_labels: [__meta_kubernetes_pod_name]
            target_label: instance
          - source_labels: [__meta_kubernetes_namespace]
            target_label: namespace

      # Kubernetes pods with prometheus annotations
      - job_name: 'kubernetes-pods'
        kubernetes_sd_configs:
          - role: pod
            namespaces:
              names:
                - pynomaly
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - action: labelmap
            regex: __meta_kubernetes_pod_label_(.+)
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

      # PostgreSQL metrics
      - job_name: 'postgres'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - pynomaly
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: pynomaly-postgres-service
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: metrics

      # Redis metrics
      - job_name: 'redis'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - pynomaly
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: pynomaly-redis-service
          - source_labels: [__meta_kubernetes_endpoint_port_name]
            action: keep
            regex: metrics

  alert_rules.yml: |
    groups:
      - name: pynomaly.alerts
        interval: 30s
        rules:
          - alert: PynomalyAPIDown
            expr: up{job="pynomaly-api"} == 0
            for: 1m
            labels:
              severity: critical
              service: api
            annotations:
              summary: "Pynomaly API is down"
              description: "Pynomaly API has been down for more than 1 minute"

          - alert: PynomalyAPIHighLatency
            expr: histogram_quantile(0.95, rate(pynomaly_http_request_duration_seconds_bucket[5m])) > 2
            for: 3m
            labels:
              severity: warning
              service: api
            annotations:
              summary: "High API latency"
              description: "95th percentile latency is above 2 seconds"

          - alert: PynomalyAPIHighErrorRate
            expr: rate(pynomaly_http_requests_total{status=~"5.."}[5m]) / rate(pynomaly_http_requests_total[5m]) > 0.1
            for: 2m
            labels:
              severity: critical
              service: api
            annotations:
              summary: "High API error rate"
              description: "API error rate is above 10%"

          - alert: PostgreSQLDown
            expr: up{job="postgres"} == 0
            for: 1m
            labels:
              severity: critical
              service: database
            annotations:
              summary: "PostgreSQL is down"
              description: "PostgreSQL database has been down for more than 1 minute"

          - alert: RedisDown
            expr: up{job="redis"} == 0
            for: 1m
            labels:
              severity: critical
              service: cache
            annotations:
              summary: "Redis is down"
              description: "Redis cache has been down for more than 1 minute"
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: pynomaly-grafana-config
  namespace: pynomaly
  labels:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/component: config
    app.kubernetes.io/instance: production
data:
  grafana.ini: |
    [analytics]
    check_for_updates = false
    reporting_enabled = false

    [server]
    root_url = http://grafana.pynomaly.local/
    serve_from_sub_path = false

    [security]
    disable_gravatar = true

    [users]
    allow_sign_up = false
    auto_assign_org = true
    auto_assign_org_role = Viewer

    [auth.anonymous]
    enabled = false

    [log]
    mode = console
    level = info

    [paths]
    data = /var/lib/grafana
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning

  datasources.yml: |
    apiVersion: 1
    datasources:
    - name: Prometheus
      type: prometheus
      access: proxy
      url: http://pynomaly-prometheus-service:80
      isDefault: true
      editable: false

  dashboards.yml: |
    apiVersion: 1
    providers:
    - name: 'default'
      orgId: 1
      folder: ''
      type: file
      disableDeletion: false
      editable: true
      options:
        path: /var/lib/grafana/dashboards
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: pynomaly-otel-config
  namespace: pynomaly
  labels:
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/component: config
    app.kubernetes.io/instance: production
data:
  config.yaml: |
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
      prometheus:
        config:
          scrape_configs:
            - job_name: 'otel-collector'
              scrape_interval: 30s
              static_configs:
                - targets: ['localhost:8889']

    processors:
      batch:
        timeout: 1s
        send_batch_size: 1024
      memory_limiter:
        limit_mib: 512
        spike_limit_mib: 128

    exporters:
      prometheus:
        endpoint: "0.0.0.0:8889"
      logging:
        loglevel: info

    extensions:
      health_check:
        endpoint: 0.0.0.0:13133
      pprof:
        endpoint: 0.0.0.0:1777

    service:
      extensions: [health_check, pprof]
      pipelines:
        traces:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [logging]
        metrics:
          receivers: [otlp, prometheus]
          processors: [memory_limiter, batch]
          exporters: [prometheus, logging]
        logs:
          receivers: [otlp]
          processors: [memory_limiter, batch]
          exporters: [logging]
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: pynomaly-postgres-config
  namespace: pynomaly
  labels:
    app.kubernetes.io/name: postgres
    app.kubernetes.io/component: config
    app.kubernetes.io/instance: production
data:
  postgresql.conf: |
    # PostgreSQL configuration for Pynomaly production
    max_connections = 100
    shared_buffers = 256MB
    effective_cache_size = 1GB
    maintenance_work_mem = 64MB
    checkpoint_completion_target = 0.9
    wal_buffers = 16MB
    default_statistics_target = 100
    random_page_cost = 1.1
    effective_io_concurrency = 200
    work_mem = 4MB
    min_wal_size = 1GB
    max_wal_size = 4GB
    max_worker_processes = 8
    max_parallel_workers_per_gather = 2
    max_parallel_workers = 8
    max_parallel_maintenance_workers = 2
    
    # Logging
    log_destination = 'stderr'
    logging_collector = on
    log_directory = 'log'
    log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
    log_statement = 'all'
    log_duration = on
    log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: pynomaly-postgres-init
  namespace: pynomaly
  labels:
    app.kubernetes.io/name: postgres
    app.kubernetes.io/component: init
    app.kubernetes.io/instance: production
data:
  01-init-database.sql: |
    -- Initialize Pynomaly database schema
    CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
    CREATE EXTENSION IF NOT EXISTS "pg_stat_statements";
    
    -- Create application schemas
    CREATE SCHEMA IF NOT EXISTS pynomaly_core;
    CREATE SCHEMA IF NOT EXISTS pynomaly_models;
    CREATE SCHEMA IF NOT EXISTS pynomaly_monitoring;
    
    -- Set default search path
    ALTER ROLE pynomaly SET search_path = pynomaly_core, pynomaly_models, pynomaly_monitoring, public;
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: pynomaly-redis-config
  namespace: pynomaly
  labels:
    app.kubernetes.io/name: redis
    app.kubernetes.io/component: config
    app.kubernetes.io/instance: production
data:
  redis.conf: |
    # Redis configuration for Pynomaly production
    bind 0.0.0.0
    port 6379
    
    # Memory management
    maxmemory 1gb
    maxmemory-policy allkeys-lru
    
    # Persistence
    save 900 1
    save 300 10
    save 60 10000
    
    # Append only file
    appendonly yes
    appendfsync everysec
    no-appendfsync-on-rewrite no
    auto-aof-rewrite-percentage 100
    auto-aof-rewrite-min-size 64mb
    
    # Security
    requirepass ${REDIS_PASSWORD}
    
    # Performance
    tcp-keepalive 300
    timeout 0
    
    # Logging
    loglevel notice
    logfile ""
