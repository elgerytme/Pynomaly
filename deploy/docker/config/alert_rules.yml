# Prometheus alerting rules for Pynomaly monitoring
# Comprehensive alerts for system health, performance, and business metrics

groups:
  - name: pynomaly.system
    interval: 30s
    rules:
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          service: system
        annotations:
          summary: "High CPU usage detected"
          description: "CPU usage is above 80% for more than 5 minutes on {{ $labels.instance }}"

      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          service: system
        annotations:
          summary: "High memory usage detected"
          description: "Memory usage is above 85% for more than 5 minutes on {{ $labels.instance }}"

      - alert: DiskSpaceRunningOut
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 90
        for: 2m
        labels:
          severity: critical
          service: system
        annotations:
          summary: "Disk space running out"
          description: "Disk usage is above 90% on {{ $labels.instance }} at {{ $labels.mountpoint }}"

  - name: pynomaly.application
    interval: 15s
    rules:
      - alert: APIHighErrorRate
        expr: rate(pynomaly_http_requests_total{status=~"5.."}[5m]) / rate(pynomaly_http_requests_total[5m]) > 0.1
        for: 2m
        labels:
          severity: critical
          service: api
        annotations:
          summary: "High API error rate"
          description: "API error rate is above 10% for the last 5 minutes"

      - alert: APIHighLatency
        expr: histogram_quantile(0.95, rate(pynomaly_http_request_duration_seconds_bucket[5m])) > 2
        for: 3m
        labels:
          severity: warning
          service: api
        annotations:
          summary: "High API latency"
          description: "95th percentile latency is above 2 seconds for the last 5 minutes"

      - alert: ServiceDown
        expr: up{job=~"pynomaly-.*"} == 0
        for: 1m
        labels:
          severity: critical
          service: application
        annotations:
          summary: "Pynomaly service is down"
          description: "{{ $labels.job }} service has been down for more than 1 minute"

      - alert: HealthCheckFailing
        expr: pynomaly_health_check_status != 1
        for: 2m
        labels:
          severity: critical
          service: application
        annotations:
          summary: "Health check failing"
          description: "Health check for {{ $labels.service }} is failing"

  - name: pynomaly.anomaly_detection
    interval: 30s
    rules:
      - alert: HighAnomalyDetectionLatency
        expr: histogram_quantile(0.95, rate(pynomaly_detection_duration_seconds_bucket[10m])) > 10
        for: 5m
        labels:
          severity: warning
          service: detection
        annotations:
          summary: "High anomaly detection latency"
          description: "95th percentile detection latency is above 10 seconds"

      - alert: AnomalyDetectionErrors
        expr: rate(pynomaly_detection_errors_total[5m]) > 0.1
        for: 3m
        labels:
          severity: warning
          service: detection
        annotations:
          summary: "High anomaly detection error rate"
          description: "Anomaly detection error rate is above 0.1 errors/second"

      - alert: ModelTrainingFailed
        expr: increase(pynomaly_training_failures_total[30m]) > 3
        for: 1m
        labels:
          severity: warning
          service: training
        annotations:
          summary: "Multiple model training failures"
          description: "More than 3 model training failures in the last 30 minutes"

      - alert: ModelPerformanceDegradation
        expr: pynomaly_model_accuracy < 0.8
        for: 5m
        labels:
          severity: warning
          service: models
        annotations:
          summary: "Model performance degradation"
          description: "Model {{ $labels.model_id }} accuracy has dropped below 80%"

  - name: pynomaly.drift_detection
    interval: 60s
    rules:
      - alert: DataDriftDetected
        expr: pynomaly_drift_detected == 1
        for: 1m
        labels:
          severity: warning
          service: drift_detection
        annotations:
          summary: "Data drift detected"
          description: "Data drift detected for detector {{ $labels.detector_id }} with severity {{ $labels.severity }}"

      - alert: HighDriftDetectionRate
        expr: rate(pynomaly_drift_detections_total[1h]) > 0.1
        for: 5m
        labels:
          severity: warning
          service: drift_detection
        annotations:
          summary: "High drift detection rate"
          description: "Drift detection rate is above 0.1 detections per hour"

      - alert: DriftMonitoringDown
        expr: up{job="pynomaly-worker-drift"} == 0
        for: 2m
        labels:
          severity: critical
          service: drift_monitoring
        annotations:
          summary: "Drift monitoring service is down"
          description: "Drift monitoring worker has been down for more than 2 minutes"

      - alert: ModelHealthScoreLow
        expr: pynomaly_model_health_score < 0.7
        for: 10m
        labels:
          severity: warning
          service: models
        annotations:
          summary: "Low model health score"
          description: "Model {{ $labels.detector_id }} health score is below 0.7"

  - name: pynomaly.infrastructure
    interval: 30s
    rules:
      - alert: PostgreSQLDown
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
          service: database
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database has been down for more than 1 minute"

      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 1m
        labels:
          severity: critical
          service: cache
        annotations:
          summary: "Redis is down"
          description: "Redis cache has been down for more than 1 minute"

      - alert: HighDatabaseConnections
        expr: pg_stat_activity_count > 80
        for: 5m
        labels:
          severity: warning
          service: database
        annotations:
          summary: "High database connection count"
          description: "PostgreSQL connection count is above 80"

      - alert: RedisHighMemoryUsage
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 5m
        labels:
          severity: warning
          service: cache
        annotations:
          summary: "Redis high memory usage"
          description: "Redis memory usage is above 90%"

  - name: pynomaly.workers
    interval: 30s
    rules:
      - alert: CeleryWorkerDown
        expr: up{job="pynomaly-workers"} == 0
        for: 2m
        labels:
          severity: critical
          service: workers
        annotations:
          summary: "Celery worker is down"
          description: "Celery worker {{ $labels.instance }} has been down for more than 2 minutes"

      - alert: HighTaskQueueLength
        expr: celery_tasks_by_state{state="PENDING"} > 100
        for: 5m
        labels:
          severity: warning
          service: workers
        annotations:
          summary: "High task queue length"
          description: "Celery queue has more than 100 pending tasks"

      - alert: TaskFailureRate
        expr: rate(celery_tasks_by_state{state="FAILURE"}[10m]) > 0.1
        for: 3m
        labels:
          severity: warning
          service: workers
        annotations:
          summary: "High task failure rate"
          description: "Celery task failure rate is above 0.1 failures/second"

  - name: pynomaly.business
    interval: 60s
    rules:
      - alert: LowDatasetProcessingRate
        expr: rate(pynomaly_datasets_processed_total[1h]) < 0.01
        for: 10m
        labels:
          severity: warning
          service: business
        annotations:
          summary: "Low dataset processing rate"
          description: "Dataset processing rate is below 0.01 datasets per hour"

      - alert: HighAnomalyRate
        expr: rate(pynomaly_anomalies_detected_total[6h]) / rate(pynomaly_samples_processed_total[6h]) > 0.5
        for: 15m
        labels:
          severity: warning
          service: business
        annotations:
          summary: "High anomaly detection rate"
          description: "Anomaly rate is above 50% for the last 6 hours - possible system issue"

      - alert: NoRecentTraining
        expr: time() - pynomaly_last_training_timestamp > 86400
        for: 1m
        labels:
          severity: warning
          service: training
        annotations:
          summary: "No recent model training"
          description: "No model training has occurred in the last 24 hours"

  - name: pynomaly.monitoring
    interval: 30s
    rules:
      - alert: PrometheusConfigReloadFailed
        expr: prometheus_config_last_reload_successful == 0
        for: 1m
        labels:
          severity: critical
          service: monitoring
        annotations:
          summary: "Prometheus config reload failed"
          description: "Prometheus configuration reload has failed"

      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 2m
        labels:
          severity: critical
          service: monitoring
        annotations:
          summary: "Alertmanager is down"
          description: "Alertmanager has been down for more than 2 minutes"

      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 2m
        labels:
          severity: warning
          service: monitoring
        annotations:
          summary: "Grafana is down"
          description: "Grafana has been down for more than 2 minutes"
